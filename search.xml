<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深入理解 Sentinel 中的限流算法]]></title>
    <url>%2Fpost%2F690c857.html</url>
    <content type="text"><![CDATA[最近在学习 Sentinel，深入学习了源码之后分享一下心得 Sentinel 版本1.8.0 固定窗口算法先介绍一下最简单的限流算法 每个窗口都有一个计数器（counter）用于统计流量，如果 counter + 本次申请的请求数 &gt; 预设的 QPS，则拒绝请求。 固定窗口很简单，但是也有很大的问题 假设我们规定 QPS 不能超过 100，如上图所示 r1 和 r2 两个时间点分别来了 60 个请求， QPS 已经大于 100 了。此时应该触发限流了，但是固定窗口算法傻傻的只关注自己窗口的流量，感知不到 QPS 已经超了 滑动窗口算法 该算法将单位时间切成了多个窗口，每次计算 QPS 时，计算 当前窗口 + 过去几个窗口 的流量总和，这样就避免了固定窗口的问题（具体使用几个窗口，取决于窗口大小和单位时间大小。例如上图，每个窗口大小为 500 ms，以 1 s 为单位时间做限流，每次使用 current + last 即可） 算法实现细节思考理解算法思路之后，接下来要思考如何实现这个算法了 首先我们需要有一个上图中的时间轴，来记录时间窗口，可以通过数组来实现这个时间轴。 时间轴有了，我们再来考虑一下时间窗口。 每个时间窗口肯定要有一个线程安全的计数器以及当前窗口对应的时间 12345678910// 时间轴List&lt;Window&gt; timeline = new ArrayList&lt;&gt;();// 每个窗口的大小int windowTime;// 时间窗口class Window &#123; Timestamp startTime; AtomicInteger counter;&#125; 但是如果仔细一想，还是存在一些问题的 由于时间是会一直增长的，那我们的数组怎么办？也要跟着时间无限的增大吗？ 旧的时间窗口（例如几秒之前的）在之后的计算不会再用到了，如何清理这些无用的窗口？ Sentinel 中滑动窗口算法如何实现的带着上述的问题与思考来看下 Sentinel 中是如何实现的 LeapArraySentinel 中滑动窗口算法的核心类，首先来了解一下他的核心成员变量 123456789101112131415161718192021222324public abstract class LeapArray&lt;T&gt; &#123; // 要统计的单位时间大小，例如计算QPS时，为1000 protected int intervalInMs; // 样本数量 protected int sampleCount; // 窗口大小 该值 = intervalInMs / sampleCount protected int windowLengthInMs; // 存储时间窗口的数组 protected final AtomicReferenceArray&lt;WindowWrap&lt;T&gt;&gt; array; public LeapArray(int sampleCount, int intervalInMs) &#123; AssertUtil.isTrue(sampleCount &gt; 0, "bucket count is invalid: " + sampleCount); AssertUtil.isTrue(intervalInMs &gt; 0, "total time interval of the sliding window should be positive"); AssertUtil.isTrue(intervalInMs % sampleCount == 0, "time span needs to be evenly divided"); this.windowLengthInMs = intervalInMs / sampleCount; this.intervalInMs = intervalInMs; this.sampleCount = sampleCount; this.array = new AtomicReferenceArray&lt;&gt;(sampleCount); &#125; &#125; 单机限流在统计 QPS 时，默认 sampleCount = 2，intervalInMs = 1000，windowLengthInMs = 500 LeapArray#calculateTimeIdx大体思路相同，同样是利用一个数组实现时间轴，每个元素代表一个时间窗口 Sentinel 中 数组长度是固定的，通过方法 LeapArray#calculateTimeIdx 来 确定时间戳在数组 中的位置 （找到时间戳对应的窗口位置） 怎么理解这个方法呢？ 我们把数据带入进去，假设 windowLengthInMs = 500 ms （每个时间窗口大小是 500 ms） 如果 timestamp 从 0 开始的话，每个时间窗口为 [0,500) [500,1000) [1000,1500) … 这时候先不考虑 timeId % array.length() ，也不考虑数组长度。假设当前 timeMillis = 601，将数值代入到 timeMillis / windowLengthInMs 其实就可以确定出当前的 timestamp 对应的时间窗口在数组中的位置了 由于数组长度是固定的，所以再加上求余数取模来确定时间窗在数组中的位置 LeapArray#currentWindow先来看一下 Sentinel 中 Window 的结构，基本和我们上面想的一致，计数器使用了泛型，可以更灵活12345678910111213141516171819public class WindowWrap&lt;T&gt; &#123; /** * Time length of a single window bucket in milliseconds. */ private final long windowLengthInMs; /** * Start timestamp of the window in milliseconds. */ private long windowStart; /** * Statistic data. */ private T value; // 省略。。。&#125; 继续说 currentWindow，该方法根据传入的 timestamp 找到 或者 创建 这个时间戳对应的 Window 这个方法源码中注释很多，我删除了部分注释1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public WindowWrap&lt;T&gt; currentWindow(long timeMillis) &#123; if (timeMillis &lt; 0) &#123; return null; &#125; int idx = calculateTimeIdx(timeMillis); // Calculate current bucket start time. long windowStart = calculateWindowStart(timeMillis); /* * Get bucket item at given time from the array. * * (1) Bucket is absent, then just create a new bucket and CAS update to circular array. * (2) Bucket is up-to-date, then just return the bucket. * (3) Bucket is deprecated, then reset current bucket and clean all deprecated buckets. */ while (true) &#123; WindowWrap&lt;T&gt; old = array.get(idx); if (old == null) &#123; WindowWrap&lt;T&gt; window = new WindowWrap&lt;T&gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis)); if (array.compareAndSet(idx, null, window)) &#123; // Successfully updated, return the created bucket. return window; &#125; else &#123; // Contention failed, the thread will yield its time slice to wait for bucket available. Thread.yield(); &#125; &#125; else if (windowStart == old.windowStart()) &#123; return old; &#125; else if (windowStart &gt; old.windowStart()) &#123; if (updateLock.tryLock()) &#123; try &#123; // Successfully get the update lock, now we reset the bucket. return resetWindowTo(old, windowStart); &#125; finally &#123; updateLock.unlock(); &#125; &#125; else &#123; // Contention failed, the thread will yield its time slice to wait for bucket available. Thread.yield(); &#125; &#125; else if (windowStart &lt; old.windowStart()) &#123; // Should not go through here, as the provided time is already behind. return new WindowWrap&lt;T&gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis)); &#125; &#125;&#125; 方法逻辑分析如下： 首先要做的两件事 计算 timestamp 在数组中的位置，就是我们上文说的 calculateTimeIdx 计算 timestamp 的 windowStart（窗口开始时间），通过 timeMillis - timeMillis % windowLengthInMs，这个值在后边会用到 然后进入一个 while(true) 循环， 通过 WindowWrap&lt;T&gt; old = array.get(idx) 找出对应的窗口，接下来就是三种情况了 old == null 这个时候代表数组中还没有这个 window，创建这个 window 加入到数组中（由于此时可能会有多个线程同时添加数组元素，所以一定要保证线程安全，所以这里使用的数组为 AtomicReferenceArray），添加成功后返回新建的 window windowStart == old.windowStart() window 已经存在了，直接返回即可 windowStart &gt; old.windowStart() 代表数组中的元素已经至少是 25s 之前的了，重置当前窗口的 windowStart 和 计数器，这个操作同样也是一个多线程操作，所以使用了 updateLock.tryLock()。 仔细看了代码后，我提出了一个问题。我觉得这个地方并不能一定保证能锁住。会不会出现两个线程同时判断需要更新，由于一个线程很快执行成功并释放了锁，第二个线程也成功获取到 Lock，会执行多次 resetWindow。我认为需要再 tryLock 之后再判断一下执行条件，目前已经给 Sentinel 提交了 Issue windowStart &lt; old.windowStart() 通常情况下不会走到这个逻辑分支，上面源码的注释也是这样解释的 LeapArray#values上文中提到过，计算流量时具体使用几个窗口，取决于窗口大小和单位时间大小 该方法的作用通过传入一个时间戳，找出本次计算所需的所有时间窗口 123456789101112131415161718192021 public List&lt;T&gt; values(long timeMillis) &#123; if (timeMillis &lt; 0) &#123; return new ArrayList&lt;T&gt;(); &#125; int size = array.length(); List&lt;T&gt; result = new ArrayList&lt;T&gt;(size); for (int i = 0; i &lt; size; i++) &#123; WindowWrap&lt;T&gt; windowWrap = array.get(i); if (windowWrap == null || isWindowDeprecated(timeMillis, windowWrap)) &#123; continue; &#125; result.add(windowWrap.value()); &#125; return result; &#125; public boolean isWindowDeprecated(long time, WindowWrap&lt;T&gt; windowWrap) &#123;// intervalInMs 在单机限流计算QPS时默认为 1000(ms) return time - windowWrap.windowStart() &gt; intervalInMs; &#125; values 的逻辑没什么可说的，遍历数组将时间符合的窗口加入到 List 中 重点看一下 isWindowDeprecated 这个方法 还是像上面那样把数值带进去。每个窗口大小为 500 ms，例如 timestamp 为 1601，这个 timestamp 对应的 windowStart 为 1500，此时 (1601 - 1500 &gt; 1000) = false 即这个窗口是有效的，再往前推算，上一个窗口 windowStart 为 1000 也是有效的。再往前推算，或者向后推算都是无效的窗口。 intervalInMs 我是这样理解的，以多长的时间段作为单位时间来限流。即可以以 1s 为一个时间段来做限流，也可以以 60s 为一个时间段来限流。 Sentinel 限流思路在理解了 LeapArray#currentWindow 和 LeapArray#values 方法的细节之后，其实我们就可以琢磨出限流的实现思路了 首先根据当前时间戳，找到对应的几个 window，根据 所有 window 中的流量总和 + 当前申请的流量数 决定能否通过 如果不能通过，抛出异常 如果能通过，则对应的窗口加上本次通过的流量数 Sentinel 限流实现Sentinel 基本也是这个思路，只不过逻辑复杂一些，这里贴出几处代码，感兴趣的同学可以自己 debug 一下 Sentinel 限流检查根据 Sentinel 文档中的解释，我们可以知道负责限流的类为 FlowSlot，FlowSlot 会使用 FlowRuleChecker 来检查当前资源是否需要限流 FlowSlot#entry FlowRuleChecker#checkFlow 根据 FlowRule 的设定来做限流检查，这中间我省略了几段代码，默认情况没有设置 ControlBehavior 会使用 DefaultController#canPass 做限流检查。如下图，通过判断 当前流量数 + 申请的数量 是否大于预设的数量，来决定是否限流 注：当使用 SphU.entry 时 prioritized = false，使用 SphU.entryWithPriority 时 prioritized = true。node.tryOccupyNext 的含义：如果想占用未来的时间窗口令牌，需要等待多久（上图中的waitInMs）。如果小于规定的超时时间，则记录正在等待的请求数，然后执行 sleep(waitInMs)，外层捕获到 PriorityWaitException 会自己处理掉，然后执行用户逻辑，用户完全无感知。 通过上图 avgUsedTokens 可以看到，当 Rule 的 grade 为 FLOW_GRADE_QPS 时，会调用 node.pass()。这里调用的具体实现为 StatisticNode#passQps，如下图 rollingCounterInSecond.getWindowIntervalInSec() 计算 QPS 时为 1 秒 rollingCounterInSecond.pass() 计算 QPS 时，最多返回两个窗口的通过请求数（currentWindow + lastWindow） rollingCounterInSecond#pass 首先先尝试是否需要创建当前的时间窗口，然后找到相关的窗口，计算流量总和。 Sentinel 请求记录代码位置 StatisticSlot#entry，fireEntry 会根据我们配置的规则进行检查（例如上述的限流）。 如果检查没有抛出异常，则记录线程数和申请的请求数（限流检查依赖的数据就是这里记录的）。 集群限流集群限流有什么用在没有集群限流之前，如果想把整个服务的 QPS 限制在某个值。举个例子现在某 Server 有十个实例，我们希望总 QPS 不超过 100，这时我们的做法是把每个实例的 QPS 设置为 10。 在理想情况下，这样做可以将 QPS 控制在 100。但是如果每台 Server 分配到的流量不均匀。这可能会导致总量在没达到 100 的时候，某些 Server 就开始限流了。 这种情况就需要 Sentinel 的集群限流出场了。 集群限流原理由于篇幅限制，我们这里不讨论如何搭建集群限流，只是来说说 Sentinel 如何在这一基础上做的集群限流。 思路很简单，选出一个 Token Server。在开启集群限流后，所有的 Client 在需要限流时，询问 Token Server，Server 决定当前请求是否限流。具体的实现细节与单机限流略有不同，但是核心的算法还是使用的 LeapArray 这里也是给出几处源码位置，感兴趣的同学自行阅读一下 Client 端根据 Rule 决定本次使用本地限流还是集群限流，FlowRuleChecker#canPassCheck Server 端，DefaultTokenService#requestToken 并发下限流的问题在完整的阅读完单机和集群的限流代码之后，发现了一个问题，限流流程可以简化为如下1234567891011121314// 伪代码// 最大QPSint maxCount;// 当前申请的流量数int aquireCount;int passQps = getPassQPS();if (passQps + aquireCount &lt;= maxCount) &#123; addPass(aquireCount);&#125; else &#123; // 限流处理&#125; 由于没有并发控制，并发场景下会出现，多个线程同时满足 passQps + aquireCount &lt;= maxCount，然后增加流量统计，这样的话，没法保证一定将 QPS 控制在 maxCount，并发的情况下会出现实际流量超出预设 QPS 的情况。 这肯定不是个Bug。这里没有并发控制可能是出于性能考虑，在性能和准确度可以接受的情况下做了一个折中 所以在使用时，如果实际 QPS 高于预设值，可能是并发导致的 demo 单机限流： https://github.com/TavenYin/taven-springcloud-learning/blob/master/sentinel-example/src/main/java/com/github/taven/limit/SentinelExample.java 集群限流：https://github.com/TavenYin/taven-springcloud-learning/blob/master/sentinel-example/src/main/java/com/github/taven/limit/SentinelClusterEmbedded.java]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>源码</tag>
        <tag>Sentinel</tag>
        <tag>限流</tag>
        <tag>微服务</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊 Java GC 算法]]></title>
    <url>%2Fpost%2F1e4a5faa.html</url>
    <content type="text"><![CDATA[Java 和 C++ 之间有一堵由内存动态分配和垃圾回收技术所围成的高墙，墙外面的人想进去，墙里面的人却想出来 今天来聊聊 Java GC（Garbage Collection，垃圾回收）中的常见算法 引用与GC的关系正题开始前，先来了解一下 Java 中的引用。对象使用不同的引用类型，决定 GC 发生时是否会回收它 引用类型 特点 强引用（Strong Reference） Java中的默认引用类型。例如Object obj = new Object()，只要强引用存在，对象永远不会被回收 软引用（Soft Reference） 内存足够时，软引用不会被回收。只有当系统要发生内存溢出时，才会被回收。适合用于缓存场景 弱引用（Weak Reference） 只要发生垃圾回收，弱引用的对象就会被回收 虚引用（Phantom Reference） 一个对象有虚引用的存在不会对生存时间都构成影响，也无法通过虚引用来获取对一个对象的真实引用。唯一的用处：能在对象被GC时收到系统通知 如何判断对象是否可以被回收？在GC开始之前，首先要做的事就是确定哪些对象『活着』，哪些对象『已死』 常见的两种算法用于判断该对象是否可以被回收 引用计数算法：每个对象中添加引用计数器。每当对象被引用，引用计数器就会加 1；每当引用失效，计数器就会减 1。当对象的引用计数器的值为 0 时，就说明该对象不再被引用，可以被回收了。强调一点，虽然引用计数算法的实现简单，判断效率也很高，但它存在着对象之间相互循环引用的问题（所以在后来的JVM版本中已经不采用这种算法了）。 可达性分析算法：GC Roots 是该算法的基础，GC Roots 是所有对象的根对象。这些对象作为正常对象的起始点，在垃圾回收时，会从这些 GC Roots 开始向下搜索，当一个对象到 GC Roots 没有任何引用链相连时，就证明此对象不再被引用。目前 HotSpot 虚拟机采用的就是这种算法。 可以作为 GC Roots 的对象： 虚拟机栈中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中 JNI（Native 方法）引用的对象 Java 虚拟机中内部的引用 synchronized 持有的对象 JMXBean、JVMTI 中注册的回调、本地代码缓存 除此之外，不同的垃圾回收器可能还会加入一些『临时的』对象共同构建 GC Roots 基础的 GC 算法标记-清除算法 （mark-sweep）如下图所示，该算法分为『标记』和『清除』两个阶段 从 GC Roots 出发标记出存活的对象，然后遍历堆清除未被标记的对象 最基础的收集算法，标记-清除的效率中等，缺点也比较明显：会产生内存碎片 复制算法（copying）将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块内存用完，需要进行垃圾收集时，就将存活者的对象复制到另一块上面，然后将第一块内存全部清除 相比较『标记-清除』不会有内存碎片的问题，但每次只使用一半的内存，内存利用率很低。当内存中存活的对象较多时，会进行大量的复制操作，效率会较低（对象的复制也是有成本的，需要复制的对象越多、越大，复制带来的代价也就越大）。适用于存活率低的情况 标记-整理算法（mark-compact）和『标记-清除』算法有点类似，从 GC Roots 出发标记出存活的对象，然后是整理阶段，将存活移动到一端。整理阶段结束后我们可以知道一个临界点，另一端的内存空间就可以被重新分配了 该算法的优点：不像复制算法那样会浪费内存空间，也不会产生内存碎片。 写到这时，我很想知道『标记-整理』的效率如何。一番搜索后，发现该算法是这三种算法中效率最低的，因为『整理』这个过程会遍历整个堆三次，具体实现思路如下 『标记-整理』以 lisp2 算法为例，实现思路如下 标记阶段：首先从 GC Roots 出发，标记出所有存活的对象 设置 forwarding 指针：每个对象头中都有一个forwarding指针，指向该对象整理后的位置。遍历整个堆计算存活对象的 forwarding 更新指针：遍历整个堆，根据 forwarding 更新 GC Roots 指针以及所有存活对象的子对象的指针，将指针指向新的位置 移动对象：遍历整个堆，根据 forwarding 移动对象并且清除对象中的标记 聊聊我个人的理解，为什么2,3,4一定要遍历整个堆？都是针对存活对象的操作，直接遍历 GC Roots 不行吗？ 设置 forwarding 阶段：一定要顺序遍历整个堆。如上图所示，如果仅是遍历 GC Roots 的话，你没法知道A这个区域是否可以覆盖 更新指针阶段：我觉得这个阶段仅遍历 GC Roots 的话，确实可行 移动对象阶段：由于 GC Roots 指针已经全部指向新的位置了，只能遍历整个堆 写到这，我又想为什么一定要三个步骤搞这么复杂，直接移动对象不行吗？ 根据上面的理解，一定要遍历整个堆来确定存活的对象该移动到哪。就现有的结构来看，如果直接移动，子对象的指针还好说可以处理，GC Roots 中的指针就没法更新了 关于lisp2 实现的更多细节可以参考下这篇Blog 『gc-标记整理算法的两种实现』 小结总结下上述三种基础GC算法的优缺点 维度 标记-清除 标记-整理 复制算法 速度 中等 最慢 最快 时间开销 mark阶段与存活对象的数量成正比，sweep阶段与整堆大小成正比 mark阶段与存活对象的数量成正比，compact阶段与整堆大小成正比，与存活对象的大小成正比 与存活对象大小成正比 空间开销 少（但会堆积碎片） 少（不堆积碎片） 通常需要存活对象的2倍大小（不堆积碎片） 移动对象 否 是 是 分代回收理论垃圾回收器都不会只选择一种算法，JVM根据对象存活周期的不同，将内存划分为几块。一般是把堆分为新生代和老年代，根据年代的特点来选择最佳的收集算法。 HotSpot 中大部分垃圾回收器都采用分代回收的思想 新生代：复制算法 老年代：标记-清除 / 标记-整理 / 或者两者同时使用 堆大小=新生代+老年代（默认分别占堆空间为1/3、2/3），新生代又被分为Eden、from survivor (S0)、to survivor (S1)，默认分配比例为 8:1:1 对象的分配对象的分配通常在 Eden 中（需要大量连续内存空间的 Java 对象，如很长的字符串或数据可以直接进入老年代，由 -XX:PretenureSizeThreshold 决定） 新生代的回收当 Eden 区满后，会触发 Young GC（新生代回收），复制 Eden 区和 S0 区中存活的对象到 S1 或者老年代（其中到达年龄的会被放入老年代，未到达年龄的放入 S1 区） 每经历一次 Young GC，survivor 区中对象年龄 +1 然后清空 Eden 区和 S0 区，交换 S0 与 S1 的名字 若存活对象大于 S1 区容量，则会被直接放入老年代。若打开了自适应（-XX:+AdaptiveSizePolicy），GC会自动重新调整新生代大小 老年代的回收在发生 Full GC 或者 Old GC 时，会根据不同的垃圾回收器或者情况选择使用 标记-清除 / 标记-整理 来进行回收 除了 Young GC 之外，常见的还有 Full GC（新生代、老生代、元空间或永久代的回收） Old GC（只有 CMS 有这个模式） Mixed GC（只有 G1 有这个模式） 通常情况下 Full GC 的触发条件，当准备要触发一次 Young GC时，如果发现统计数据说之前 Young GC 的平均晋升大小比目前老年代剩余的空间大，则不会触发 Young GC 而是转为触发 Full GC 小结由于新生代的特点是大多数对象都是「朝生夕死」的，存活率低，所以非常适合复制算法。而 survivor 区存在的意义是为了确保「朝生夕死」的对象不会轻易进入老年代，当对象的年龄满足（经历了多次 Young GC）才会进入老年代。 又到了提问环节，为什么分代回收中需要有两个 survivor 区，一个不行吗？ 答案是不行。假设只有一个 survivor，Eden 回收后存活的对象进入了 survivor。那么 survivor 区可以被回收的对象该怎么处理？难道要用标记清除和标记整理？那可太没有必要了，所以划分出两个 survivor 区，将新生代的复制算法贯彻到底 参考『深入理解Java虚拟机』 『极客时间 - Java性能调优实战』 『Java-GC 垃圾收集算法』 『gc-标记整理算法的两种实现』 『Mark-compact algorithm - Wikipedia』 『为什么新生代内存需要有两个Survivor区』 『Major GC和Full GC的区别是什么？触发条件呢？ - RednaxelaFX的回答 - 知乎』 『关于JVM垃圾搜集算法（标记-整理算法与复制算法）的效率？ - RednaxelaFX的回答 - 知乎』]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle LogMiner 数据迁移实战]]></title>
    <url>%2Fpost%2F82cb4508.html</url>
    <content type="text"><![CDATA[LogMiner 是什么LogMiner 是Oracle官方提供的工具，可以解析 Redo log 和 Archived Redo log LogMiner 可以做什么？官方文档中列举了很多，大家可以自己去看下。 我们目前的项目在使用基于LogMiner 的 Debezium Oracle Connector 做数据迁移 Oracle LogMiner 数据迁移的原理是什么？首先需要了解几个概念，这里简单介绍下 Redo log：Redo中记录了所有对数据块的更改，Oralce 要求至少有两个以上的Redo Log Group Archived Redo log：当一个Redo Log 写满之后，会发生日志切换，数据的更改会记录到下一个Redo Log中（所以一定要有两个以上的Redo）。如果开启了归档模式，Oracle 会将写满的Redo Log 归档。 SCN (System Change Number)：Oracle 内部逻辑时间戳 Flashback：通过闪回查询 SELECT ... AS OF SCN 可以查询Oracle某个时间点的全量数据 思路如下： 首先查询出一下当前的SCN 根据SCN 查询出这一时刻的全量数据 通过Logminer 指定Start_SCN，获取增量数据 安装与配置想尝试却不太熟悉Oracle的同学，可以参考一下我整理的文档 Oralce Install (docker)： https://github.com/TavenYin/database-cdc/blob/master/doc/oracle/oracle-install.md Logminer：https://github.com/TavenYin/database-cdc/blob/master/doc/oracle/oracle12c-logminer.md 小试牛刀在准备好了环境之后，我们来开箱体验一下Logminer logminer 用户登录 conn c##logminer/password 1. 构建数据字典LogMiner使用数据字典将内部对象标识符和数据类型转换为正常字段和数据格式 12345# 这是一条常规的SQL INSERT INTO HR.JOBS(JOB_ID, JOB_TITLE, MIN_SALARY, MAX_SALARY) VALUES(&apos;IT_WT&apos;,&apos;Technical Writer&apos;, 4000, 11000);# 如果没有数据字典，数据会是这样的insert into &quot;UNKNOWN&quot;.&quot;OBJ# 45522&quot;(&quot;COL 1&quot;,&quot;COL 2&quot;,&quot;COL 3&quot;,&quot;COL 4&quot;) values (HEXTORAW(&apos;45465f4748&apos;),HEXTORAW(&apos;546563686e6963616c20577269746572&apos;),HEXTORAW(&apos;c229&apos;),HEXTORAW(&apos;c3020b&apos;)); 官方文档中提到三种方式： 在线数据字典：当你可以访问创建Redo的源数据库并且表结构不会发生任何变动时。可以考虑使用在线数据字典。这是最简单有效的，也是Oracle的推荐选项由于在线数据字典永远存储的是最新的结构。如果发生了表结构变动，Logminer 捕获到旧版本的数据，SQL将会如上述代码块中那样 提取数据字典到redo中：需要执行命令BEGIN DBMS_LOGMNR_D.BUILD (options =&gt; DBMS_LOGMNR_D.STORE_IN_REDO_LOGS); END;该操作会占用一定数据库资源 提取数据字典到Flat File：Oracle维护该选项是为了兼容历史版本，本文并没有使用到该方式，不多做介绍 LogMiner 在启动时会通过指定的数据字典选项维护一个内部数据字典，当启动LogMiner时指定 DBMS_LOGMNR.DDL_DICT_TRACKING，LogMiner会自动捕获DDL来更新内部字典，这样即使发生了表结构变动时，也可以正确的解析DDL。注意：该选项不能和在线数据字典同时使用更多解释参考Oracle文档：https://docs.oracle.com/en/database/oracle/oracle-database/12.2/sutil/oracle-logminer-utility.html#GUID-56743517-A0C0-4CCD-9D20-2883AFB5683B 这一步我选择在线数据字典，什么都不用做，直接进入下一步 2. 添加日志文件12345678910111213141516171819# 查询目前的redol ogSQL&gt; select member from v$logfile;MEMBER--------------------------------------------------------------------------------/opt/oracle/oradata/ORCLCDB/redo03.log/opt/oracle/oradata/ORCLCDB/redo02.log/opt/oracle/oradata/ORCLCDB/redo01.log# 添加redo logSQL&gt; EXECUTE DBMS_LOGMNR.ADD_LOGFILE( - LOGFILENAME =&gt; &apos;/opt/oracle/oradata/ORCLCDB/redo03.log&apos;, - OPTIONS =&gt; DBMS_LOGMNR.NEW);EXECUTE DBMS_LOGMNR.ADD_LOGFILE( - LOGFILENAME =&gt; &apos;/opt/oracle/oradata/ORCLCDB/redo02.log&apos;, - OPTIONS =&gt; DBMS_LOGMNR.ADDFILE);EXECUTE DBMS_LOGMNR.ADD_LOGFILE( - LOGFILENAME =&gt; &apos;/opt/oracle/oradata/ORCLCDB/redo01.log&apos;, - OPTIONS =&gt; DBMS_LOGMNR.ADDFILE); 3. START_LOGMNR123# 使用在线数据字典进行log解析SQL&gt; EXECUTE DBMS_LOGMNR.START_LOGMNR(-OPTIONS =&gt; DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG); 然后执行一条INSERT 4. 查询结果通过查询V$LOGMNR_CONTENTS 获取LogMiner捕获的结果。当执行该视图查询时，LogMiner会按照顺序解析Redo和Archived Log，所有执行时间会有一点慢123SELECT OPERATION, SQL_REDO, SQL_UNDOFROM V$LOGMNR_CONTENTSWHERE table_name='TEST_TAB'; 结果如下，可以看到我们刚刚INSERT的SQL 实战我们已经知道了迁移的思路和Logminer如何使用，现在可以动手搞一个demo了。 由于篇幅问题，这里我只讨论思路和我的一些想法。 完整代码参考👉 https://github.com/TavenYin/database-cdc/tree/master/oracle-logminer 1. 整体思路相关实现思路参考自Debezium 需要解释一下第四步为什么，发生Redo发生切换时，需要重启Logminer流程，两点原因 Redo Log 切换后，会生成新的归档，我们需要Add新的归档日志 长时间开启LogMiner会话，会导致PGA使用量一直上升无法释放，End LogMiner 可以解决这个问题。所以代码逻辑中需要找一个时机去重启LogMiner，而Redo 切换这个时间点确实也挺合适的。 写到这的时候，我突然有了一个疑问 我们刚刚已经说过了，只有在查询 V$LOGMNR_CONTENTS 时，LogMiner才会去解析Redo Log，然后动态的生成视图。 参考上图。如果在第四步和第六步之间，程序检查到没有RedoLog切换准备继续执行。突然插入了大量数据导致Current Redo Log 被覆盖（注意必须是已经被覆盖而不是切换）了，此时是不是我们再查询 V$LOGMNR_CONTENTS 岂不是会丢失一部分数据？ 由于start_logminer时会指定，起始和结束SCN，所以即使下次执行时添加了新的Archived Log，由于SCN已经被跨过去了，所以一定不会读这部分数据 在我做了测试之后发现，如果情况真的如此极端，确实会这样。 那么Debezium为什么没有考虑这个问题呢？ 个人理解，在生产环境通常Redo Log 不会频繁切换，并且一定会有多个Redo Group。这么短时间内被覆盖的情况几乎不可能发生。 2. 处理 V$LOGMNR_CONTENTS 结果集最开始在看Debezium源码的时候，没仔细注意这个地方，在自己动手搞一遍之后，发现这个地方的逻辑有点麻烦 V$LOGMNR_CONTENTS 每一行可能是事务的提交、回滚，DDL，DML 上面提到了一个 TransactionalBuffer是什么？ 我们在读取 V$LOGMNR_CONTENTS 会发生如下图的情况，因为每次只从startScn 读取到 当前Scn。而这中间可能发生的情况是，事务并没有Commit，但是我们拿到了其中一部分的DML，我们并不能确定这些DML是不是要Commit，所以需要将这些“一半”的事务暂时缓存在内存中 其实在调用 DBMS_LOGMNR.START_LOGMNR 时，可以指定一个选项 COMMITTED_DATA_ONLY，仅读出已提交的事务。这样就不必要这么麻烦的处理结果集了。但是为什么不选择 COMMITTED_DATA_ONLY？使用该策略会一直等待事务提交才会响应客户端，这很容易造成 “Out of Memory”，所以这个策略不适合我们的程序。 3. 迁移进程宕机处理数据迁移必定是一个漫长的过程，如果在执行中遇到什么意外，导致Java进程挂了，那么一切都要从头开始吗？ 如果我们能确定某个SCN之前的所有记录都已经被处理了，那么下次重启时从这个SCN开始处理即可 两处可以确定之前SCN已经被全部处理的地方，代码如下： a. 当前TransactionalBuffer中没有数据，代表END_SCN之前所有的事务都已经被提交了 b. 提交事务时，如果当前要提交的事务的Start_SCN 早于TransactionalBuffer中的所有事务 4. SQL解析如果你想将Oracle的数据同步到其他数据库（包含NoSQL）的话，最好的办法是将SQL解析成结构化的对象，让下游服务去消费这些对象。 Debezium的做法，我还没抽出空研究。目前的解决方法是用com.alibaba.druid.sql.SQLUtils，这个类可以将SQL解析成结构化对象，我们再对这些对象进行一些处理，即可让下游服务消费了。 DEMO运行效果如下 GitHub 👉 https://github.com/TavenYin/database-cdc/tree/master/oracle-logminer 参考 Oracle Redo : https://docs.oracle.com/cd/B28359_01/server.111/b28310/onlineredo001.htm Oracle Archived : https://docs.oracle.com/cd/B28359_01/server.111/b28310/archredo001.htm Oracle Flashback : https://docs.oracle.com/cd/E11882_01/appdev.112/e41502/adfns_flashback.htm LogMiner : https://docs.oracle.com/en/database/oracle/oracle-database/12.2/sutil/oracle-logminer-utility.html Debezium Oracle Connector : https://debezium.io/documentation/reference/connectors/oracle.html]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
        <tag>LogMiner</tag>
        <tag>数据库迁移技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka Connect-入门]]></title>
    <url>%2Fpost%2F7ec53704.html</url>
    <content type="text"><![CDATA[前提首先你需要了解MQ / Kafka相关的知识 本文目标了解 Kafka Connect 基本概念与功能 什么是Kafka Connect Kafka Connect 是一款可扩展并且可靠地在 Apache Kafka 和其他系统之间进行数据传输的工具。 可以很简单的定义 connectors（连接器） 将大量数据迁入、迁出Kafka。 例如我现在想要把数据从MySQL迁移到ElasticSearch，为了保证高效和数据不会丢失，我们选择MQ作为中间件保存数据。这时候我们需要一个生产者线程，不断的从MySQL中读取数据并发送到MQ，还需要一个消费者线程消费MQ的数据写到ElasticSearch，这件事情似乎很简单，不需要任何框架。 但是如果我们想要保证生产者和消费者服务的高可用性，例如重启后生产者恢复到之前读取的位置，分布式部署并且节点宕机后将任务转移到其他节点。如果要加上这些的话，这件事就变得复杂起来了，而Kafka Connect 已经为我们造好这些轮子。 Kafka Connect 如何工作？ Kafka Connect 特性如下： Kafka 连接器的通用框架：Kafka Connect 标准化了其他数据系统与Kafka的集成，从而简化了连接器的开发，部署和管理 支持分布式模式和单机模式部署 Rest API：通过简单的Rest API管理连接器 偏移量管理：针对Source和Sink都有相应的偏移量（Offset）管理方案，程序员无须关心Offset 的提交 分布式模式可扩展的，支持故障转移 Kafka Connect Concepts这里简单介绍下Kafka Connect 的概念与组成更多细节请参考 👉 https://docs.confluent.io/platform/current/connect/concepts.html Connectors连接器，分为两种 Source（从源数据库拉取数据写入Kafka），Sink（从Kafka消费数据写入目标数据） 连接器其实并不参与实际的数据copy，连接器负责管理Task。连接器中定义了对应Task的类型，对外提供配置选项（用户创建连接器时需要提供对应的配置信息）。并且连接器还可以决定启动多少个Task线程。 用户可以通过Rest API 启停连接器，查看连接器状态 Confluent 已经提供了许多成熟的连接器，传送门👉 https://www.confluent.io/product/connectors/ Task实际进行数据传输的单元，和连接器一样同样分为 Source和Sink Task的配置和状态存储在Kafka的Topic中，config.storage.topic和status.storage.topic。我们可以随时启动，停止任务，以提供弹性、可扩展的数据管道 Worker刚刚我们讲的Connectors 和Task 属于逻辑单元，而Worker 是实际运行逻辑单元的进程，Worker 分为两种模式，单机模式和分布式模式 单机模式：比较简单，但是功能也受限，只有一些特殊的场景会使用到，例如收集主机的日志，通常来说更多的是使用分布式模式 分布式模式：为Kafka Connect提供了可扩展和故障转移。相同group.id的Worker，会自动组成集群。当新增Worker，或者有Worker挂掉时，集群会自动协调分配所有的Connector 和 Task（这个过程称为Rebalance） 当使用Worker集群时，创建连接器，或者连接器Task数量变动时，都会触发Rebalance 以保证集群各个Worker节点负载均衡。但是当Task 进入Fail状态的时候并不会触发 Rebalance，只能通过Rest Api 对Task进行重启 ConvertersKafka Connect 通过 Converter 将数据在Kafka（字节数组）与Task（Object）之间进行转换 默认支持以下Converter AvroConverter io.confluent.connect.avro.AvroConverter: 需要使用 Schema Registry ProtobufConverter io.confluent.connect.protobuf.ProtobufConverter: 需要使用 Schema Registry JsonSchemaConverter io.confluent.connect.json.JsonSchemaConverter: 需要使用 Schema Registry JsonConverter org.apache.kafka.connect.json.JsonConverter (无需 Schema Registry): 转换为json结构 StringConverter org.apache.kafka.connect.storage.StringConverter: 简单的字符串格式 ByteArrayConverter org.apache.kafka.connect.converters.ByteArrayConverter: 不做任何转换 Converters 与 Connector 是解耦的，下图展示了在Kafka Connect中，Converter 在何时进行数据转换 Transforms连接器可以通过配置Transform 实现对单个消息（对应代码中的Record）的转换和修改，可以配置多个Transform 组成一个链。例如让所有消息的topic加一个前缀、sink无法消费source 写入的数据格式，这些场景都可以使用Transform 解决 Transform 如果配置在Source 则在Task之后执行，如果配置在Sink 则在Task之前执行 Dead Letter Queue与其他MQ不同，Kafka 并没有死信队列这个功能。但是Kafka Connect提供了这一功能。 当Sink Task遇到无法处理的消息，会根据errors.tolerance配置项决定如何处理，默认情况下(errors.tolerance=none) Sink 遇到无法处理的记录会直接抛出异常，Task进入Fail 状态。开发人员需要根据Worker的错误日志解决问题，然后重启Task，才能继续消费数据 设置 errors.tolerance=all，Sink Task 会忽略所有的错误，继续处理。Worker中不会有任何错误日志。可以通过配置errors.deadletterqueue.topic.name = &lt;dead-letter-topic-name&gt; 让无法处理的消息路由到 Dead Letter Topic 快速上手下面我来实战一下，如何使用Kafka Connect，我们先定一个小目标 将MySQL中的全量数据同步到Redis 新建文件 docker-compose.yaml123456789101112131415161718192021version: &apos;3.7&apos;services: zookeeper: image: wurstmeister/zookeeper container_name: zk ports: - 2182:2181 kafka: image: wurstmeister/kafka:2.13-2.7.0 container_name: kafka ports: - 9092:9092 environment: KAFKA_BROKER_ID: 0 # 宿主机ip KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://192.168.3.21:9092 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092 depends_on: - zookeeper 在终端上执行 docker-compose -f docker-compose.yaml up -d 启动docker容器 准备连接器，这里我是自己写了一个简单的连接器😄。下载地址：https://github.com/TavenYin/kafka-connect-example/blob/master/release/kafka-connector-example-bin.jar 12# 将连接器上传到kafka 容器中docker cp kafka-connector-example-bin.jar kafka:/opt/connectors 修改配置并启动Worker 12345#在配置文件末尾追加 plugin.path=/opt/connectorsvi /opt/kafka/config/connect-distributed.properties# 启动Workerbin/connect-distributed.sh -daemon config/connect-distributed.properties 准备MySQL 由于我宿主机里已经安装了MySQL，我就直接使用了，使用如下Sql创建表。创建之后随便造几条数据12345CREATE TABLE `test_user` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`)) ; 创建连接器 新建 source.json1234567891011&#123; &quot;name&quot; : &quot;example-source&quot;, &quot;config&quot; : &#123; &quot;connector.class&quot; : &quot;com.github.taven.source.ExampleSourceConnector&quot;, &quot;tasks.max&quot; : &quot;1&quot;, &quot;database.url&quot; : &quot;jdbc:mysql://192.168.3.21:3306/test?useUnicode=true&amp;characterEncoding=UTF-8&amp;autoReconnect=true&amp;useSSL=false&amp;zeroDateTimeBehavior=convertToNull&amp;serverTimezone=UTC&amp;rewriteBatchedStatements=true&quot;, &quot;database.username&quot; : &quot;root&quot;, &quot;database.password&quot; : &quot;root&quot;, &quot;database.tables&quot; : &quot;test_user&quot; &#125;&#125; 向Worker 发送请求，创建连接器curl -i -X POST -H &quot;Accept:application/json&quot; -H &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @source.json source.json 中，有一些属性是Kafka Connect 提供的，例如上述文件中 name, connector.class, tasks.max，剩下的属性可以在开发Connector 时自定义。关于Kafka Connect Configuration 相关请阅读这里 👉 https://docs.confluent.io/platform/current/installation/configuration/connect/index.html 确认数据是否写入Kafka 首先查看一下Worker中的运行状态，如果Task的state = RUNNING，代表Task没有抛出任何异常，平稳运行123bash-4.4# curl -X GET localhost:8083/connectors/example-source/status&#123;&quot;name&quot;:&quot;example-source&quot;,&quot;connector&quot;:&#123;&quot;state&quot;:&quot;RUNNING&quot;,&quot;worker_id&quot;:&quot;172.21.0.3:8083&quot;&#125;,&quot;tasks&quot;:[&#123;&quot;id&quot;:0,&quot;state&quot;:&quot;RUNNING&quot;,&quot;worker_id&quot;:&quot;172.21.0.3:8083&quot;&#125;],&quot;type&quot;:&quot;source&quot;&#125; 查看kafka 中Topic 是否创建123456bash-4.4# bin/kafka-topics.sh --list --zookeeper zookeeper:2181__consumer_offsetsconnect-configsconnect-offsetsconnect-statustest_user 这些Topic 都存储了什么？ __consumer_offsets: 记录所有Kafka Consumer Group的Offset connect-configs: 存储连接器的配置，对应Connect 配置文件中config.storage.topic connect-offsets: 存储Source 的Offset，对应Connect 配置文件中offset.storage.topic connect-status: 连接器与Task的状态，对应Connect 配置文件中status.storage.topic 查看topic中数据，此时说明MySQL数据已经成功写入Kafka1234bash-4.4# bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test_user --from-beginning&#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;type&quot;:&quot;int64&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;name&quot;&#125;],&quot;optional&quot;:false,&quot;name&quot;:&quot;test_user&quot;&#125;,&quot;payload&quot;:&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;yyyyyy&quot;&#125;&#125;&#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;type&quot;:&quot;int64&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;name&quot;&#125;],&quot;optional&quot;:false,&quot;name&quot;:&quot;test_user&quot;&#125;,&quot;payload&quot;:&#123;&quot;id&quot;:2,&quot;name&quot;:&quot;qqqq&quot;&#125;&#125;&#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;type&quot;:&quot;int64&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;name&quot;&#125;],&quot;optional&quot;:false,&quot;name&quot;:&quot;test_user&quot;&#125;,&quot;payload&quot;:&#123;&quot;id&quot;:3,&quot;name&quot;:&quot;eeee&quot;&#125;&#125; 数据结构为Json，可以回顾一下上面我们修改的connect-distributed.properties，默认提供的Converter 为JsonConverter，所有的数据包含schema 和 payload 两项是因为配置文件中默认启动了key.converter.schemas.enable=true和value.converter.schemas.enable=true两个选项 启动 Sink 新建sink.json123456789101112&#123; &quot;name&quot; : &quot;example-sink&quot;, &quot;config&quot; : &#123; &quot;connector.class&quot; : &quot;com.github.taven.sink.ExampleSinkConnector&quot;, &quot;topics&quot; : &quot;test_user, test_order&quot;, &quot;tasks.max&quot; : &quot;1&quot;, &quot;redis.host&quot; : &quot;192.168.3.21&quot;, &quot;redis.port&quot; : &quot;6379&quot;, &quot;redis.password&quot; : &quot;&quot;, &quot;redis.database&quot; : &quot;0&quot; &#125;&#125; 创建Sink Connectorcurl -i -X POST -H &quot;Accept:application/json&quot; -H &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @sink.json 然后查看Sink Connector Status，这里我发现由于我的Redis端口只对localhost开发，所以这里我的Task Fail了，修改了Redis配置之后，重启Task curl -X POST localhost:8083/connectors/example-sink/tasks/0/restart 在确认了Sink Status 为RUNNING 后，可以确认下Redis中是否有数据 关于Kafka Connect Rest api 文档，请参考👉https://docs.confluent.io/platform/current/connect/references/restapi.html 如何查看Sink Offset消费情况 使用命令bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group connect-example-sink 下图代表 test_user topic 三条数据已经全部消费 Kafka Connect 高级功能我们的小目标已经达成了。现在两个Task无事可做，正好借此机会我们来体验一下可扩展和故障转移 集群扩展我启动了开发环境中的Kafka Connect Worker，根据官方文档所示通过注册同一个Kafka 并且使用相同的 group.id=connect-cluster 可以自动组成集群 启动我开发环境中的Kafka Connect，之后检查两个连接器状态 12345bash-4.4# curl -X GET localhost:8083/connectors/example-source/status&#123;&quot;name&quot;:&quot;example-source&quot;,&quot;connector&quot;:&#123;&quot;state&quot;:&quot;RUNNING&quot;,&quot;worker_id&quot;:&quot;172.23.176.1:8083&quot;&#125;,&quot;tasks&quot;:[&#123;&quot;id&quot;:0,&quot;state&quot;:&quot;RUNNING&quot;,&quot;worker_id&quot;:&quot;172.23.176.1:8083&quot;&#125;],&quot;type&quot;:&quot;source&quot;&#125;bash-4.4#bash-4.4# curl -X GET localhost:8083/connectors/example-sink/status&#123;&quot;name&quot;:&quot;example-sink&quot;,&quot;connector&quot;:&#123;&quot;state&quot;:&quot;RUNNING&quot;,&quot;worker_id&quot;:&quot;172.21.0.3:8083&quot;&#125;,&quot;tasks&quot;:[&#123;&quot;id&quot;:0,&quot;state&quot;:&quot;RUNNING&quot;,&quot;worker_id&quot;:&quot;172.21.0.3:8083&quot;&#125;],&quot;type&quot;:&quot;sink&quot;&#125; 观察worker_id 可以发现，两个Connectors 已经分别运行在两个Worker上了 故障转移此时我们通过kill pid结束docker中的Worker进程观察是否宕机之后自动转移，但是发现Task并没有转移到仅存的Worker中，Task 状态变为UNASSIGNED，这是为啥呢？难道是有什么操作错了？ 在网上查阅了一番得知，Kafka Connect 的集群扩展与故障转移机制是通过Kafka Rebalance 协议实现的（Consumer也是该协议），当Worker节点宕机时间超过 scheduled.rebalance.max.delay.ms 时，Kafka才会将其踢出集群。踢出后将该节点的连接器和任务分配给其他Worker，scheduled.rebalance.max.delay.ms默认值为五分钟。 后来经测试发现，五分钟之后查看连接器信息，已经转移到存活的Worker节点了 本来还计划写一下如何开发连接器和Kafka Rebalance，但是这篇已经够长了，所以计划后续更新这两篇文章]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>Kafka Connect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[问题分析：Kafka Connect 引入了Fastjson后，Rest API响应为{}]]></title>
    <url>%2Fpost%2F11f7961b.html</url>
    <content type="text"><![CDATA[前言最近在学习Kafka Connect，写了个连接器的demo。在demo提交了几个版本之后，突然发现Kafka Connect Rest API 无法正常响应了，明明有正在运行的连接器，查询status，居然返回{} 问题分析对 Rest API 进行debug后，确认是有数据的，但是数据返回不到客户端，很奇怪。因为我记得之前是好用的，所以我回滚了代码版本，逐一排查之后发现当引入Fastjson 依赖之后，会导致Connect Rest API 不可用 如果懒一点的话，到这里就已经结束了，直接删除Fastjson依赖，使用其他Json包。但是我很好奇，在我的理解里，Fastjson 这种库就是个工具包，如果我们程序没有主动调用的时候，是不会对我们产生任何影响的。 百度谷歌一通之后，一筹莫展之际，点开了Fastjson的源码包，在这里发现了Fastjson为JAXRS提供的SPI扩展 JAXRS：Java API for RESTful Web Services，JavaEE提供的Web服务接口。Jersey 实现了JAXRS，而Kafka Connect 引用了Jersey 。SPI：Service Provider Interface ，是JDK内置的一种服务提供发现机制，可以参考我之前的博客 Java SPI 实战 打开javax.ws.rs.ext.MessageBodyWriter 文件，可以看到提供的实现类是com.alibaba.fastjson.support.jaxrs.FastJsonProvider，定位到FastJsonProvider下writeTo方法，该方法会把object写入到OutputStream中，看起来很靠谱，debug试一下 果然，说明Fastjson果然参与了Rest API的响应。为什么使用Fastjson就响应不了数据呢，看了下源码，这里要求被序列化的Bean必须标记Fastjson相关的注解，而实际的Bean使用的是Jackson的注解，所以Fastjson无法序列化数据。 接下来可以根据调用栈和全局搜索找一下，看看FastJsonProvider是在什么时机加载的，能否干掉他。 调用栈并没有找到什么有用的信息，通过全局搜索MessageBodyWriter找到了FastJsonProvider的加载位置，MessageBodyFactory::initialize 上图字面意思理解，使用 injectionManager (注入管理器)，找到MessageBodyWriter的可用实现 这里的 customMbws size = 2，分别是FastJson和Jackson的实现。但是FastJson在前，而每次需要做JSON序列化的时候，会遍历writers，如果找到支持application/json的MessageBodyWriter则直接返回，所以每次使用的都是FastJson的实现。 至此已经明白了，为什么Fastjson 会影响Kafka Connect了，接下来就是想办法解决了 这个时候还是没有找到Fastjson是在哪加载的，在Fastjson的 wiki 中找到了些灵感，发现Fastjson 在Jersey 中并不是通过SPI的方式进行的扩展，而是通过FastJsonAutoDiscoverable，向Jersey 的 context中注册FastJsonProvider 最后，我们在java 进程启动时指定参数 -Dfastjson.auto.discoverable=false，禁用 FastJsonProvider 参考https://github.com/alibaba/fastjson/wiki/Integrate-Fastjson-in-JAXRS]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人简历备份]]></title>
    <url>%2Fpost%2Fe860cc2d.html</url>
    <content type="text"><![CDATA[基本信息 殷天文/男/1995 专科/大连软件职业学院/软件工程 工作年限：4.5年 电子邮箱：** 电话：** 博客：http://yintianwen.top Github: https://github.com/TavenYin 简书：https://www.jianshu.com/u/cd682de00804 公众号：殷天文 期望岗位：高级后端开发 / 高级Java工程师 专业技能 扎实的Java基础，了解JVM，熟悉常用API实现原理，熟练掌握多线程开发，熟悉IO/NIO 熟练使用Java爬虫技术 熟悉常用设计模式 熟练掌握Spring等常用Java开源框架，并深入分析过其实现原理，对源码学习有浓厚的兴趣 熟练使用MySQL/Redis/Oracle等数据库，了解Elasticsearch/MongoDB/Postgres等数据库 熟练掌握Seata/可靠消息等分布式事务解决方案 熟练掌握数据库迁移，SQL优化，分库分表等技术 熟练掌握Kafka/RabbitMQ/RocketMQ 熟悉Dubbo/SpringCloud等微服务框架 熟悉PHP/JavaScript/Lua等语言的开发 熟悉Linux/Shell/Docker 工作经历思佰益必智信息技术（大连）有限公司 2021.1 ~ 至今 APP DBA 数据迁移系统 2021.1 ~ 至今技术：Kafka, Kafka Connect, Debezium, Oracle, Postgres参与数据迁移系统的二次开发，技术方案调研，测试，部署 掌握了Oracle全量加增量数据迁移方案（闪回查询 + LogMiner/XStream） 掌握了Debezium Oracle-Connector源码 期间自学了Kafka/Kafka Connect 大连云匠科技 2019.8 ~ 2021.1 后端研发工程师 IronERP 2020.8 ~ 2021.1技术：Java8, SpringBoot, Mybatis, MySQL, Redis, RocketMQ参与并主导了订单推送消费的开发 期间自学并掌握了RocketMQ 爱心助农 2020.1 ~ 2020.6技术：PHP, YII2, MySQL, Redis参与并主导了库存的重构，解决超卖问题，并总结了完整的库存实现方案 重构原系统中不合理的库存表设计，利用Redis命令原子性的特点，使用Redis+Lua作为库存扣减的新方案 后来还发现了订单的并发操作情况（例如用户取消和系统自动取消，系统自动取消和支付回调）同样会造成超卖的现象，分别针对这些场景做了优化 期间自学了PHP相关技术 小太熊/电商Saas 2019.8 - 2020.6技术：Java8, SpringBoot, JdbcTemplate, SpringCloud Config, Spring Security, ElasticSearch, Redis, MySQL, RabbitMQ参与并主导了核心业务的设计与开发，如交易功能，保证金相关 掌握MQ实现分布式事务的方案 掌握如何在并发场景增减库存 学习到了如何设计可插拔的业务插件 开发了基于Logback的分布式日志收集 大连东方之星 2018.10 ~ 2019.8 国家义务教育优质均衡发展评估系统 2018/10 ~ 2019/8技术：Java8, SpringBoot, Mybatis, MySQL, Druid, Shiro, JWT, Redis参与并主导了产品基础架构与核心业务的设计。还负责了业务功能的分配，代码Review等 开发了基于Shiro + Jwt实现了用户鉴权以及题出用户功能 开发了基于Spring AOP 和 Redis分布式锁的防止并发提交功能 大连万思动科技 2016.9 ~ 2018.10 通信运行方式管理系统（国家电网）技术：Java8, Springboot, Mybatis, MySQL, Ehcache, ElasticSearch, Thymeleaf, Openlayer3参与并主导了整个后端业务的设计与开发 自学并掌握了Dijkstra算法，用于故障迂回算法的开发 自学并掌握了ElasticSearch的使用，实现了分词搜索，同义词匹配搜索等功能 技术文章/个人项目 数据库CDC (Change Data Capture) 核心技术 深入理解 Dijkstra 算法实现原理 分布式事务 Seata AT模式原理与实战 Spring 源码解析 @RequestBody @ResponseBody 的来龙去脉 浅入浅出 Spring 事务传播实现原理 Redisson 源码解析，如何利用Redis实现分布式可重入锁 致谢感谢您花时间阅读我的简历，期待能有机会和您共事]]></content>
      <categories>
        <category>resume</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分布式事务 Seata AT模式原理与实战]]></title>
    <url>%2Fpost%2F6fe3d525.html</url>
    <content type="text"><![CDATA[Seata 是阿里开源的基于Java的分布式事务解决方案 AT，XA，TCC，SagaSeata 提供四种模式解决分布式事务场景，AT，XA，TCC，Saga。简单叨咕叨咕我对这几种模式的理解 AT 这是Seata的一大特色，AT对业务代码完全无侵入性，使用非常简单，改造成本低。我们只需要关注自己的业务SQL，Seata会通过分析我们业务SQL，反向生成回滚数据 AT 包含两个阶段 一阶段，所有参与事务的分支，本地事务Commit 业务数据和回滚日志（undoLog） 二阶段，事务协调者根据所有分支的情况，决定本次全局事务是Commit 还是 Rollback（二阶段是完全异步） XA也是我们常说的二阶段提交，XA要求数据库本身提供对规范和协议的支持。XA用起来的话，也是对业务代码无侵入性的。 上述其他三种模式，都是属于补偿型，无法保证全局一致性。啥意思呢，例如刚刚说的AT模式，我们是可能读到这一次分布式事务的中间状态，而XA模式不会。 补偿型 事务处理机制构建在 事务资源（数据库） 之上（要么在中间件层面，要么在应用层面），事务资源 本身对分布式事务是无感知的，这也就导致了补偿型事务无法做到真正的 全局一致性 。比如，一条库存记录，处在 补偿型 事务处理过程中，由 100 扣减为 50。此时，仓库管理员连接数据库，查询统计库存，就看到当前的 50。之后，事务因为意外回滚，库存会被补偿回滚为 100。显然，仓库管理员查询统计到的 50 就是 脏 数据。如果是XA的话，中间态数据库存 50 由数据库本身保证，不会被仓库管理员读到（当然隔离级别需要 读已提交 以上） 但是全局一致性带来的结果就是数据的锁定（AT模式也是存在全局锁的，但是隔离级别无法保证，后边我们会详细说），例如全局事务中有一条update语句，其他事务想要更新同一条数据的话，只能等待全局事务结束 传统XA模式是存在一些问题的，Seata也是做了相关的优化，更多关于Seata XA的内容，传送门👉http://seata.io/zh-cn/blog/seata-xa-introduce.html TCC TCC 模式同样包含两个阶段 Try 阶段 ：所有参与分布式事务的分支，对业务资源进行检查和预留 二阶段 Confirm：所有分支的Try全部成功后，执行业务提交 二阶段 Cancel：取消Try阶段预留的业务资源 对比AT或者XA模式来说，TCC模式需要我们自己抽象并实现Try，Confirm，Cancel三个接口，编码量会大一些，但是由于事务的每一个阶段都由开发人员自行实现。而且相较于AT模式来说，减少了SQL解析的过程，也没有全局锁的限制，所以TCC模式的性能是优于AT 、XA模式。PS：果然简单和高效难以两全的 Saga Saga 是长事务解决方案，每个参与者需要实现事务的正向操作和补偿操作。当参与者正向操作执行失败时，回滚本地事务的同时，会调用上一阶段的补偿操作，在业务失败时最终会使事务回到初始状态 Saga与TCC类似，同样没有全局锁。由于相比缺少锁定资源这一步，在某些适合的场景，Saga要比TCC实现起来更简单。由于Saga和TCC都需要我们手动编码实现，所以在开发时我们需要参考一些设计上的规范，由于不是本文重点，这里就不多说了，可以参考分布式事务 Seata 及其三种模式详解 在我们了解完四种分布式事务的原理之后，我们回到本文重点AT模式 AT 如何使用模拟需求：以下订单为例，在分布式的电商场景中，订单服务和库存服务可能是两个数据库 我们先来看看AT模式下的代码是什么样的，这里忽略了Seata的相关配置，只看业务部分 在需要开启分布式事务的方法上标记@GlobalTransactional，然后执行分别执行扣减库存和创建订单操作，事务的参与者可以是本地的数据源，或者RPC的远程调用（远程调用的话需要携带全局事务ID，也就是上图的xid） AT 一阶段之前说过AT模式分为两个阶段，第一阶段包括提交业务数据和回滚日志（undoLog），第一阶段具体流程如下图 GlobalTransactional 切面标记@GlobalTransactional的方法通过AOP实现了，开启全局事务和提交全局事务两个操作，与Spring 事务机制类似，当 GlobalTransactionalInterceptor 在事务执行过程中捕获到Throwable时，会发起全局事务回滚 0.1 步骤中会生成一个全局事务ID 0.2 所有事务参与者执行结束后，一阶段事务提交 undoLog我们先来看看 Seata undoLog 的结构1234567891011// 省略了相关方法public class SQLUndoLog &#123; // insert, update ... private SQLType sqlType; private String tableName; private TableRecords beforeImage; private TableRecords afterImage;&#125; Seata 在执行业务SQL前后，会生成beforeImage和afterImage，在需要回滚时，根据SQLType，决定具体的回滚策略，例如SQLType=update时，将数据回滚到beforeImage的状态，如果SQLType=insert，则根据afterImage删除数据 如2.4所示，每条业务SQL，执行成功后，会为这条SQL生成LockKey，格式为tableName:PrimaryKey 注册分支事务在3.1步骤注册分支事务时，client会把所有的LockKey 拼到一起作为全局锁发送给Seata-server。如果注册成功，写入undoLog，并提交本地事务，一阶段结束，等待二阶段反馈 如果当前有其他分支事务已经持有了相同的锁（即其他事务也在处理相同表的同一行），则client 注册事务分支失败。client会根据客户端定义的重发时间和重发次数进行不断的尝试，如果重试结束仍然没有获得锁，则一阶段失败，本地事务回滚。如果该全局事务存在已经注册成功分支事务，Seata-server 进行二阶段回滚 全局锁会在分支事务二阶段结束后释放 Seata 全局锁的设计是为了什么？以扣减库存场景为例，TX1 完成库存扣减的一阶段，库存从100扣减为99，正在等待二阶段的通知。TX2也要扣减同一商品的库存，如果没有全局锁的限制，TX2库存从99扣减为98，这时如果TX1接收到回滚通知，进行回滚把库存从98回滚到100。因为没有全局锁，造成了脏写 AT 二阶段二阶段是完全异步化的并且完全由Seata控制，Seata根据所有事务参与者的提交情况决定二阶段如何处理 如果所有事务提交成功，则二阶段的任务就是删除一阶段生成 的undoLog，并释放全局锁 如果部分事务参与者提交失败，则需要根据undoLog对已经注册的事务分支进行回滚，并释放全局锁 对Seata提出的疑问至此我们已经初步了解了Seata的AT模式是如何实现的了 如果你也和我一样，仔细思考了上述过程，可能会提出一些问题，这边我列举一下我在学习Seata时，遇到的问题，以及我得出的结论 问题1. Seata如何做到无侵入的分析业务SQL生成undoLog，注册事务分支等操作？ Seata 代理了DataSource，我们可以通过在代码注入一个DataSource来验证我的说法，目前的DataSource 是 io.seata.rm.datasource.DataSourceProxy 所有的Java持久化框架，最终在操作数据库时都会通过DataSource接口获取Connection，通过Connection 实现对数据库的增删改查，事务控制。 Seata 通过代理Connection的方式，做到了无侵入的生成undoLog，注册事务分支，具体源码可以查看io.seata.rm.datasource.ConnectionProxy 问题2. ConnectionProxy 如何判断当前事务是全局事务，还是本地事务？ 通过当前线程是否绑定了全局事务id，在进行全局事务之前，需要调用RootContext.bind(xid); 问题3. 全局事务并发更新 还是以下订单扣减库存的场景为例，如果TX1和TX2同时扣减product_id为1的库存，这时Seata会不会生成相同的beforeImage？ 举个例子，TX1读库存为100，TX1扣减库存1，此时BeforeImage为100紧接着 如果TX2读库存也为100，那么就有问题了，不管TX2扣减多少库存，如果TX1回滚那么相当于覆盖了TX2扣减的库存，出现了脏写 Seata是如何解决这个问题的？ 源码位置：io.seata.rm.datasource.exec.AbstractDMLBaseExecutor::executeAutoCommitFalse 可以看到这里的逻辑和我上面画的图一致，证明我没有瞎说 😄 我们来看一下beforeImage()，这是一个抽象方法，看一下他的子类UpdateExecutor是如何实现的 通过Debug，可以看出Seata这边也是确实考虑了这个问题，直接简单而有效的解决了这个问题 回到我们的例子，由于SELECT FOR UPDATE的存在，TX2如果也想读同一条数据的话，只能等到TX1 提交事务后，才能读到。所以问题解决 问题4. 全局事务外的更新 我们现在可以确认在Seata的保证下，全局事务，不会造成数据的脏写，但是全局事务外会！ 什么意思呢？ 还以库存为例 用户正在抢购，用户A完成了1阶段的库存扣减，这个时候库存为99。 此时库存管理员上线了，他查了一下库存为99。嗯…太少了，我加100个，库存管理员把库存更新为200。 而此时seata给用户A生成beforeImage为100，如果此时用户A的全局事务失败了，发生了回滚，再次将库存更新为100… 再次出现脏写 Seata 针对这个问题，提供了@GlobalLock注解，标记该注解时，会像全局事务一样进行SQL分析，竞争全局锁，就不会出现上述问题了 关于这个问题可以参考Seata的FAQ文档 http://seata.io/zh-cn/docs/overview/faq.html 问题5. @GlobalTransactional 和 @Transactional 同时使用会怎么样 我们上文中已经说过了 @GlobalTransactional 的作用了，他是负责开启全局事务/提交事务1阶段，说白了@GlobalTransactional 只和Seata-server 交互，而 @Transactional 管理的是本地数据库的事务，所以二者不发生冲突。 但是需要注意 @GlobalTransactional AOP 覆盖范围一定要大于 @Transactional 问题6. 如果其中某一个事务分支超时未提交，会发生什么 这个我并没有看源码，而是通过跑demo，验证的 例如现在有A，B两个事务分支 A 正常提交，并向Seata注册分支成功 B 2分钟后提交事务，并向Seata发起注册 Seata的全局事务超时时间，默认是1分钟，Seata-server 在检测到有超时的全局事务时，会向所有已提交的分支，发起回滚。而超时提交的事务，向Seata-server发起分支注册时，响应结果为事务已超时，或者事务不存在，也会回滚本地事务 问题7. Seata-client 如何接收Seata-server发起的通知 Seata-client 包含了Netty服务，在启动时Netty会监听端口，并向Seata-server 发起注册。server中存储了client 的调用地址。 总结我们学习了Seata的AT模式是如何工作的，可以看出Seata模式在开发上是非常简单的，但是Seata的背后为了维持分布式事务的数据一致性，做了大量的工作，AT模式非常适合现有的业务模型直接迁移。 但是他的缺点也很明显，性能并不是那么的优秀。例如我们刚刚看到的全局锁的问题，为了数据不会发生脏写，Seata牺牲了业务的并发能力。在非常要求性能的场景，可能还是需要考虑TCC，SAGA，可靠消息等方案 在使用Seata开发前，建议大家先去阅读一下FAQ文档，避免踩坑 https://seata.io/zh-cn/docs/overview/faq.html DEMOhttps://github.com/TavenYin/taven-springboot-learning/tree/master/springboot-seata 参考Seata是什么 - http://seata.io/zh-cn/docs/overview/what-is-seata.htmlSeata常见问题 - http://seata.io/zh-cn/docs/overview/faq.html分布式事务中间件 Seata 的设计原理 - http://seata.io/zh-cn/blog/seata-at-mode-design.html分布式事务 Seata 及其三种模式详解 - http://seata.io/zh-cn/blog/seata-at-tcc-saga.html分布式事务如何实现？深入解读 Seata 的 XA 模式 - http://seata.io/zh-cn/blog/seata-xa-introduce.html]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>Seata</tag>
        <tag>分布式事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅入浅出 Spring 事务传播实现原理]]></title>
    <url>%2Fpost%2Fbef5c7fd.html</url>
    <content type="text"><![CDATA[本文和大家一起刨析 Spring 事务的相关源码，篇幅较长，代码片段较多，建议使用电脑阅读 本文目标 理解Spring事务管理核心接口 理解Spring事务管理的核心逻辑 理解事务的传播类型及其实现原理 版本SpringBoot 2.3.3.RELEASE 什么是事务的传播？Spring 除了封装了事务控制之外，还抽象出了 事务的传播 这个概念，事务的传播并不是关系型数据库所定义的，而是Spring在封装事务时做的增强扩展，可以通过@Transactional 指定事务的传播，具体类型如下 事务传播行为类型 说明 PROPAGATION_REQUIRED 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。Spring的默认事务传播类型 PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY 使用当前的事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起（暂停）。 PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 举个栗子以嵌套事务为例123456789101112131415161718192021222324252627282930313233343536@Servicepublic class DemoServiceImpl implements DemoService &#123; @Autowired private JdbcTemplate jdbcTemplate; @Autowired private DemoServiceImpl self; @Transactional @Override public void insertDB() &#123; String sql = "INSERT INTO sys_user(`id`, `username`) VALUES (?, ?)"; jdbcTemplate.update(sql, uuid(), "taven"); try &#123; // 内嵌事务将会回滚，而外部事务不会受到影响 self.nested(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; @Transactional(propagation = Propagation.NESTED) @Override public void nested() &#123; String sql = "INSERT INTO sys_user(`id`, `username`) VALUES (?, ?)"; jdbcTemplate.update(sql, uuid(), "nested"); throw new RuntimeException("rollback nested"); &#125; private String uuid() &#123; return UUID.randomUUID().toString(); &#125;&#125; 上述代码中，nested()方法标记了事务传播类型为嵌套，如果nested()中抛出异常仅会回滚nested()方法中的sql，不会影响到insertDB()方法中已经执行的sql 注意：service 调用内部方法时，如果直接使用this调用，事务不会生效。因此使用this调用相当于跳过了外部的代理类，所以AOP不会生效，无法使用事务 思考众所周知，Spring 事务是通过AOP实现的，如果是我们自己写一个AOP控制事务，该怎么做呢？1234567891011121314151617// 伪代码public Object invokeWithinTransaction() &#123; // 开启事务 connection.beginTransaction(); try &#123; // 反射执行方法 Object result = invoke(); // 提交事务 connection.commit(); return result; &#125; catch(Exception e) &#123; // 发生异常时回滚 connection.rollback(); throw e; &#125; &#125; 在这个基础上，我们来思考一下如果是我们自己做的话，事务的传播该如何实现 以PROPAGATION_REQUIRED为例，这个似乎很简单，我们判断一下当前是否有事务（可以考虑使用ThreadLocal存储已存在的事务对象），如果有事务，那么就不开启新的事务。反之，没有事务，我们就创建新的事务 如果事务是由当前切面开启的，则提交/回滚事务，反之不做处理 那么事务传播中描述的挂起（暂停）当前事务，和内嵌事务是如何实现的？ 源码入手要阅读事务传播相关的源码，我们先来了解下Spring 事务管理的核心接口与类 TransactionDefinition该接口定义了事务的所有属性（隔离级别，传播类型，超时时间等等），我们日常开发中经常使用的 @Transactional 其实最终会被转化为 TransactionDefinition TransactionStatus事务的状态，以最常用的实现 DefaultTransactionStatus 为例，该类存储了当前的事务对象，savepoint，当前挂起的事务，是否完成，是否仅回滚等等 TransactionManager这是一个空接口，直接继承他的 interface 有 PlatformTransactionManager（我们平时用的就是这个，默认的实现类DataSourceTransactionManager）以及ReactiveTransactionManager（响应式事务管理器，由于不是本文重点，我们不多说） 从上述两个接口来看，TransactionManager 的主要作用 通过TransactionDefinition开启一个事务，返回TransactionStatus 通过TransactionStatus 提交、回滚事务（实际开启事务的Connection通常存储在TransactionStatus中） 123456789101112public interface PlatformTransactionManager extends TransactionManager &#123; TransactionStatus getTransaction(@Nullable TransactionDefinition definition) throws TransactionException; void commit(TransactionStatus status) throws TransactionException; void rollback(TransactionStatus status) throws TransactionException;&#125; TransactionInterceptor事务拦截器，事务AOP的核心类（支持响应式事务，编程式事务，以及我们常用的标准事务），由于篇幅原因，本文只讨论标准事务的相关实现 下面我们从事务逻辑的入口 TransactionInterceptor 入手，来看下Spring事务管理的核心逻辑以及事务传播的实现 TransactionInterceptorTransactionInterceptor 实现了MethodInvocation（这是实现AOP的一种方式），其核心逻辑在父类TransactionAspectSupport 中，方法位置：TransactionInterceptor::invokeWithinTransaction 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950protected Object invokeWithinTransaction(Method method, @Nullable Class&lt;?&gt; targetClass, final InvocationCallback invocation) throws Throwable &#123; // If the transaction attribute is null, the method is non-transactional. TransactionAttributeSource tas = getTransactionAttributeSource(); // 当前事务的属性 TransactionAttribute extends TransactionDefinition final TransactionAttribute txAttr = (tas != null ? tas.getTransactionAttribute(method, targetClass) : null); // 事务属性中可以定义当前使用哪个事务管理器 // 如果没有定义就去Spring上下文找到一个可用的 TransactionManager final TransactionManager tm = determineTransactionManager(txAttr); // 省略了响应式事务的处理 ... PlatformTransactionManager ptm = asPlatformTransactionManager(tm); final String joinpointIdentification = methodIdentification(method, targetClass, txAttr); if (txAttr == null || !(ptm instanceof CallbackPreferringPlatformTransactionManager)) &#123; // Standard transaction demarcation with getTransaction and commit/rollback calls. TransactionInfo txInfo = createTransactionIfNecessary(ptm, txAttr, joinpointIdentification); Object retVal; try &#123; // This is an around advice: Invoke the next interceptor in the chain. // This will normally result in a target object being invoked. // 如果有下一个拦截器则执行，最终会执行到目标方法，也就是我们的业务代码 retVal = invocation.proceedWithInvocation(); &#125; catch (Throwable ex) &#123; // target invocation exception // 当捕获到异常时完成当前事务 （提交或者回滚） completeTransactionAfterThrowing(txInfo, ex); throw ex; &#125; finally &#123; cleanupTransactionInfo(txInfo); &#125; if (retVal != null &amp;&amp; vavrPresent &amp;&amp; VavrDelegate.isVavrTry(retVal)) &#123; // Set rollback-only in case of Vavr failure matching our rollback rules... TransactionStatus status = txInfo.getTransactionStatus(); if (status != null &amp;&amp; txAttr != null) &#123; retVal = VavrDelegate.evaluateTryFailure(retVal, txAttr, status); &#125; &#125; // 根据事务的状态提交或者回滚 commitTransactionAfterReturning(txInfo); return retVal; &#125; // 省略了编程式事务的处理 ...&#125; 这里代码很多，根据注释的位置，我们可以把核心逻辑梳理出来 获取当前事务属性，事务管理器（以注解事务为例，这些都可以通过@Transactional来定义） createTransactionIfNecessary，判断是否有必要创建事务 invocation.proceedWithInvocation 执行拦截器链，最终会执行到目标方法 completeTransactionAfterThrowing当抛出异常后，完成这个事务，提交或者回滚，并抛出这个异常 commitTransactionAfterReturning 从方法命名来看，这个方法会提交事务。但是深入源码中会发现，该方法中也包含回滚逻辑，具体行为会根据当前TransactionStatus的一些状态来决定（也就是说，我们也可以通过设置当前TransactionStatus，来控制事务回滚，并不一定只能通过抛出异常），详见AbstractPlatformTransactionManager::commit 我们继续，来看看createTransactionIfNecessary做了什么 TransactionAspectSupport::createTransactionIfNecessary1234567891011121314151617181920212223242526272829protected TransactionInfo createTransactionIfNecessary(@Nullable PlatformTransactionManager tm, @Nullable TransactionAttribute txAttr, final String joinpointIdentification) &#123; // If no name specified, apply method identification as transaction name. if (txAttr != null &amp;&amp; txAttr.getName() == null) &#123; txAttr = new DelegatingTransactionAttribute(txAttr) &#123; @Override public String getName() &#123; return joinpointIdentification; &#125; &#125;; &#125; TransactionStatus status = null; if (txAttr != null) &#123; if (tm != null) &#123; // 通过事务管理器开启事务 status = tm.getTransaction(txAttr); &#125; else &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Skipping transactional joinpoint [" + joinpointIdentification + "] because no transaction manager has been configured"); &#125; &#125; &#125; return prepareTransactionInfo(tm, txAttr, joinpointIdentification, status);&#125; createTransactionIfNecessary中的核心逻辑 通过PlatformTransactionManager（事务管理器）开启事务 prepareTransactionInfo 准备事务信息，这个具体做了什么我们稍后再讲 继续来看PlatformTransactionManager::getTransaction，该方法只有一个实现 AbstractPlatformTransactionManager::getTransaction 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public final TransactionStatus getTransaction(@Nullable TransactionDefinition definition) throws TransactionException &#123; // Use defaults if no transaction definition given. TransactionDefinition def = (definition != null ? definition : TransactionDefinition.withDefaults()); // 获取当前事务，该方法有继承 AbstractPlatformTransactionManager 的子类自行实现 Object transaction = doGetTransaction(); boolean debugEnabled = logger.isDebugEnabled(); // 如果目前存在事务 if (isExistingTransaction(transaction)) &#123; // Existing transaction found -&gt; check propagation behavior to find out how to behave. return handleExistingTransaction(def, transaction, debugEnabled); &#125; // Check definition settings for new transaction. if (def.getTimeout() &lt; TransactionDefinition.TIMEOUT_DEFAULT) &#123; throw new InvalidTimeoutException("Invalid transaction timeout", def.getTimeout()); &#125; // 传播类型PROPAGATION_MANDATORY, 要求当前必须有事务 // No existing transaction found -&gt; check propagation behavior to find out how to proceed. if (def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_MANDATORY) &#123; throw new IllegalTransactionStateException( "No existing transaction found for transaction marked with propagation 'mandatory'"); &#125; // PROPAGATION_REQUIRED, PROPAGATION_REQUIRES_NEW, PROPAGATION_NESTED 不存在事务时创建事务 else if (def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRED || def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW || def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; SuspendedResourcesHolder suspendedResources = suspend(null); if (debugEnabled) &#123; logger.debug("Creating new transaction with name [" + def.getName() + "]: " + def); &#125; try &#123; // 开启事务 return startTransaction(def, transaction, debugEnabled, suspendedResources); &#125; catch (RuntimeException | Error ex) &#123; resume(null, suspendedResources); throw ex; &#125; &#125; else &#123; // Create "empty" transaction: no actual transaction, but potentially synchronization. if (def.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT &amp;&amp; logger.isWarnEnabled()) &#123; logger.warn("Custom isolation level specified but no actual transaction initiated; " + "isolation level will effectively be ignored: " + def); &#125; boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); return prepareTransactionStatus(def, null, true, newSynchronization, debugEnabled, null); &#125;&#125; 代码很多，重点关注注释部分即可 doGetTransaction获取当前事务 如果存在事务，则调用handleExistingTransaction处理，这个我们稍后会讲到 接下来，会根据事务的传播决定是否开启事务 如果事务传播类型为PROPAGATION_MANDATORY，且不存在事务，则抛出异常 如果传播类型为 PROPAGATION_REQUIRED, PROPAGATION_REQUIRES_NEW, PROPAGATION_NESTED，且当前不存在事务，则调用startTransaction创建事务 当不满足 3、4时，例如 PROPAGATION_NOT_SUPPORTED，此时会执行事务同步，但是不会创建真正的事务 Spring 事务同步在之前一篇博客中有讲到，传送门👉https://www.jianshu.com/p/7880d9a98a5f Spring 如何管理当前的事务接下来讲讲上面提到的doGetTransaction、handleExistingTransaction，这两个方法是由不同的TransactionManager自行实现的 我们以SpringBoot默认的TransactionManager，DataSourceTransactionManager为例123456789101112131415@Overrideprotected Object doGetTransaction() &#123; DataSourceTransactionObject txObject = new DataSourceTransactionObject(); txObject.setSavepointAllowed(isNestedTransactionAllowed()); ConnectionHolder conHolder = (ConnectionHolder) TransactionSynchronizationManager.getResource(obtainDataSource()); txObject.setConnectionHolder(conHolder, false); return txObject;&#125;@Overrideprotected boolean isExistingTransaction(Object transaction) &#123; DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; return (txObject.hasConnectionHolder() &amp;&amp; txObject.getConnectionHolder().isTransactionActive());&#125; 结合 AbstractPlatformTransactionManager::getTransaction 一起来看，doGetTransaction 其实获取的是当前的Connection。判断当前是否存在事务，是判断DataSourceTransactionObject 对象中是否包含connection，以及connection是否开启了事务。 我们继续来看下TransactionSynchronizationManager.getResource(obtainDataSource())获取当前connection的逻辑 TransactionSynchronizationManager::getResource12345678910111213141516171819202122232425262728293031323334353637383940private static final ThreadLocal&lt;Map&lt;Object, Object&gt;&gt; resources = new NamedThreadLocal&lt;&gt;("Transactional resources");@Nullable// TransactionSynchronizationManager::getResourcepublic static Object getResource(Object key) &#123; // DataSourceTransactionManager 调用该方法时，以数据源作为key // TransactionSynchronizationUtils::unwrapResourceIfNecessary 如果key为包装类，则获取被包装的对象 // 我们可以忽略该逻辑 Object actualKey = TransactionSynchronizationUtils.unwrapResourceIfNecessary(key); Object value = doGetResource(actualKey); if (value != null &amp;&amp; logger.isTraceEnabled()) &#123; logger.trace("Retrieved value [" + value + "] for key [" + actualKey + "] bound to thread [" + Thread.currentThread().getName() + "]"); &#125; return value;&#125;/** * Actually check the value of the resource that is bound for the given key. */@Nullableprivate static Object doGetResource(Object actualKey) &#123; Map&lt;Object, Object&gt; map = resources.get(); if (map == null) &#123; return null; &#125; Object value = map.get(actualKey); // Transparently remove ResourceHolder that was marked as void... if (value instanceof ResourceHolder &amp;&amp; ((ResourceHolder) value).isVoid()) &#123; map.remove(actualKey); // Remove entire ThreadLocal if empty... if (map.isEmpty()) &#123; resources.remove(); &#125; value = null; &#125; return value;&#125; 看到这里，我们能明白DataSourceTransactionManager是如何管理线程之间的Connection，ThreadLocal 中存储一个Map，key为数据源对象，value为该数据源在当前线程的Connection DataSourceTransactionManager 在开启事务后，会调用TransactionSynchronizationManager::bindResource将指定数据源的Connection绑定到当前线程 AbstractPlatformTransactionManager::handleExistingTransaction我们继续回头看，如果存在事务的情况，如何处理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899private TransactionStatus handleExistingTransaction( TransactionDefinition definition, Object transaction, boolean debugEnabled) throws TransactionException &#123; // 如果事务的传播要求以非事务方式执行 抛出异常 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NEVER) &#123; throw new IllegalTransactionStateException( "Existing transaction found for transaction marked with propagation 'never'"); &#125; // PROPAGATION_NOT_SUPPORTED 如果存在事务，则挂起当前事务，以非事务方式执行 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NOT_SUPPORTED) &#123; if (debugEnabled) &#123; logger.debug("Suspending current transaction"); &#125; // 挂起当前事务 Object suspendedResources = suspend(transaction); boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); // 构建一个无事务的TransactionStatus return prepareTransactionStatus( definition, null, false, newSynchronization, debugEnabled, suspendedResources); &#125; // PROPAGATION_REQUIRES_NEW 如果存在事务，则挂起当前事务，新建一个事务 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW) &#123; if (debugEnabled) &#123; logger.debug("Suspending current transaction, creating new transaction with name [" + definition.getName() + "]"); &#125; SuspendedResourcesHolder suspendedResources = suspend(transaction); try &#123; return startTransaction(definition, transaction, debugEnabled, suspendedResources); &#125; catch (RuntimeException | Error beginEx) &#123; resumeAfterBeginException(transaction, suspendedResources, beginEx); throw beginEx; &#125; &#125; // PROPAGATION_NESTED 内嵌事务，就是我们开头举得例子 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; if (!isNestedTransactionAllowed()) &#123; throw new NestedTransactionNotSupportedException( "Transaction manager does not allow nested transactions by default - " + "specify 'nestedTransactionAllowed' property with value 'true'"); &#125; if (debugEnabled) &#123; logger.debug("Creating nested transaction with name [" + definition.getName() + "]"); &#125; // 非JTA事务管理器都是通过savePoint实现的内嵌事务 // savePoint：关系型数据库中事务可以创建还原点，并且可以回滚到还原点 if (useSavepointForNestedTransaction()) &#123; // Create savepoint within existing Spring-managed transaction, // through the SavepointManager API implemented by TransactionStatus. // Usually uses JDBC 3.0 savepoints. Never activates Spring synchronization. DefaultTransactionStatus status = prepareTransactionStatus(definition, transaction, false, false, debugEnabled, null); // 创建还原点 status.createAndHoldSavepoint(); return status; &#125; else &#123; // Nested transaction through nested begin and commit/rollback calls. // Usually only for JTA: Spring synchronization might get activated here // in case of a pre-existing JTA transaction. return startTransaction(definition, transaction, debugEnabled, null); &#125; &#125; // 如果执行到这一步传播类型一定是，PROPAGATION_SUPPORTS 或者 PROPAGATION_REQUIRED // Assumably PROPAGATION_SUPPORTS or PROPAGATION_REQUIRED. if (debugEnabled) &#123; logger.debug("Participating in existing transaction"); &#125; // 校验目前方法中的事务定义和已存在的事务定义是否一致 if (isValidateExistingTransaction()) &#123; if (definition.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT) &#123; Integer currentIsolationLevel = TransactionSynchronizationManager.getCurrentTransactionIsolationLevel(); if (currentIsolationLevel == null || currentIsolationLevel != definition.getIsolationLevel()) &#123; Constants isoConstants = DefaultTransactionDefinition.constants; throw new IllegalTransactionStateException("Participating transaction with definition [" + definition + "] specifies isolation level which is incompatible with existing transaction: " + (currentIsolationLevel != null ? isoConstants.toCode(currentIsolationLevel, DefaultTransactionDefinition.PREFIX_ISOLATION) : "(unknown)")); &#125; &#125; if (!definition.isReadOnly()) &#123; if (TransactionSynchronizationManager.isCurrentTransactionReadOnly()) &#123; throw new IllegalTransactionStateException("Participating transaction with definition [" + definition + "] is not marked as read-only but existing transaction is"); &#125; &#125; &#125; boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); // 构建一个TransactionStatus，但不开启事务 return prepareTransactionStatus(definition, transaction, false, newSynchronization, debugEnabled, null);&#125; 这里代码很多，逻辑看上述注释即可。这里终于看到了期待已久的挂起事务和内嵌事务了，我们还是看一下DataSourceTransactionManager的实现 挂起事务：通过TransactionSynchronizationManager::unbindResource 根据数据源获取当前的Connection，并在resource中移除该Connection。之后会将该Connection存储到TransactionStatus对象中 1234567// DataSourceTransactionManager::doSuspend@Overrideprotected Object doSuspend(Object transaction) &#123; DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; txObject.setConnectionHolder(null); return TransactionSynchronizationManager.unbindResource(obtainDataSource());&#125; 在事务提交或者回滚后，调用 AbstractPlatformTransactionManager::cleanupAfterCompletion会将TransactionStatus 中缓存的Connection重新绑定到resource中 内嵌事务：通过关系型数据库的savePoint实现，提交或回滚的时候会判断如果当前事务为savePoint则释放savePoint或者回滚到savePoint，具体逻辑参考AbstractPlatformTransactionManager::processRollback 和 AbstractPlatformTransactionManager::processCommit 至此，事务的传播源码分析结束 prepareTransactionInfo上文留下了一个问题，prepareTransactionInfo 方法做了什么，我们先来看下TransactionInfo的结构123456789101112131415161718protected static final class TransactionInfo &#123; @Nullable private final PlatformTransactionManager transactionManager; @Nullable private final TransactionAttribute transactionAttribute; private final String joinpointIdentification; @Nullable private TransactionStatus transactionStatus; @Nullable private TransactionInfo oldTransactionInfo; // ...&#125; 该类在Spring中的作用，是为了内部传递对象。ThreadLocal中存储了最新的TransactionInfo，通过当前TransactionInfo可以找到他的oldTransactionInfo。每次创建事务时会新建一个TransactionInfo（无论有没有真正的事务被创建）存储到ThreadLocal中，在每次事务结束后，会将当前ThreadLocal中的TransactionInfo重置为oldTransactionInfo，这样的结构形成了一个链表，使得Spring事务在逻辑上可以无限嵌套下去 如果觉得有收获，可以关注我的公众号，你的点赞和关注就是对我最大的支持]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>SpringBoot</tag>
        <tag>事务传播</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java SPI 实战]]></title>
    <url>%2Fpost%2Fe971632b.html</url>
    <content type="text"><![CDATA[SPI 全称为 (Service Provider Interface) ，是JDK内置的一种服务提供发现机制，可以轻松实现面向服务的注册与发现，完成服务提供与使用的解耦，并且可以实现动态加载 SPI 能做什么利用SPI机制，sdk的开发者可以为使用者提供扩展点，使用者无需修改源码，有点类似Spring @ConditionalOnMissingBean 的意思 动手实现一个SPI例如我们要正在开发一个sdk其中有一个缓存的功能，但是用户很可能不想使用我们的缓存实现，用户想要自定义缓存的实现，此时使用spi就非常的合适了 新建一个maven工程命名为sdk Cache 接口12345678910111213import java.util.ServiceLoader;public interface Cache &#123; String getName(); static Cache load() &#123; // ServiceLoader 实现了 Iterable，可以加载到Cache接口的多个实现类 ServiceLoader&lt;Cache&gt; cacheServiceLoader = ServiceLoader.load(Cache.class); return cacheServiceLoader.iterator().next(); &#125;&#125; ServiceLoader 是Java提供服务发现工具类，这是我们实现SPI的关键 CacheDefaultImpl12345public class CacheDefaultImpl implements Cache &#123; public String getName() &#123; return &quot;defaultImpl&quot;; &#125;&#125; 除此之外，ServiceLoader 还需要在classpath:META-INF/services 下找到以该接口全名命名的文件，这里我们直接在resource 目录下创建META-INF/services/ com.github.tavenyin.Cache文件即可，文件中指定Cache的实现类 12# 此处可以指定多个实现类com.github.tavenyin.CacheDefaultImpl Run我们建立一个新的maven子工程，并引入sdk模块，执行测试代码12System.out.println(Cache.load().getName()) # 输出结果为 defaultImpl 使用者定制化那么如果sdk的使用者不想使用我们的CacheDefaultImpl了怎么办，没关系使用者只需要覆盖 classpath:META-INF/services/com.github.tavenyin.Cache 就可以了 （使用者在同样在resource下创建即可覆盖） 我们再来运行一下测试代码，输出结果为 newImpl ServiceLoader 实现原理ServiceLoader 的实现原理还是比较简单的，试想一下，如果我们自己实现一个ServiceLoader，我们会怎么做？ 通过指定的文件加载出所有的类名 通过反射构建这些对象 没错，ServiceLoader 就是这么做的，我们来简单看一下源码 入口 ServiceLoader::iterator::next 1234567891011121314151617181920212223242526272829303132// Cached providers, in instantiation orderprivate LinkedHashMap&lt;String,S&gt; providers = new LinkedHashMap&lt;&gt;();// The current lazy-lookup iteratorprivate LazyIterator lookupIterator; // ServiceLoader::iteratorpublic Iterator&lt;S&gt; iterator() &#123; return new Iterator&lt;S&gt;() &#123; Iterator&lt;Map.Entry&lt;String,S&gt;&gt; knownProviders = providers.entrySet().iterator(); public boolean hasNext() &#123; if (knownProviders.hasNext()) return true; return lookupIterator.hasNext(); &#125; // ServiceLoader::iterator::next public S next() &#123; if (knownProviders.hasNext()) return knownProviders.next().getValue(); return lookupIterator.next(); &#125; public void remove() &#123; throw new UnsupportedOperationException(); &#125; &#125;;&#125; 从providers 初始为一个空的LinkedHashMap，我们无需关注，所以knownProviders::hasNext 一定返回false，我们直奔knownProviders::next knownProviders::next 中核心逻辑在nextService() 中1234567891011121314151617181920212223242526272829303132private S nextService() &#123; // hasNextService 中做了两件事 // 1. 判断是否还有服务的提供者 // 2. 通过 &quot;META-INF/services/&quot; + 接口全名 加载所有提供者ClassName if (!hasNextService()) throw new NoSuchElementException(); String cn = nextName; nextName = null; Class&lt;?&gt; c = null; try &#123; // 通过ClassName 创建Class c = Class.forName(cn, false, loader); &#125; catch (ClassNotFoundException x) &#123; fail(service, &quot;Provider &quot; + cn + &quot; not found&quot;); &#125; if (!service.isAssignableFrom(c)) &#123; fail(service, &quot;Provider &quot; + cn + &quot; not a subtype&quot;); &#125; try &#123; // 反射创建实现类实例 S p = service.cast(c.newInstance()); providers.put(cn, p); return p; &#125; catch (Throwable x) &#123; fail(service, &quot;Provider &quot; + cn + &quot; could not be instantiated&quot;, x); &#125; throw new Error(); // This cannot happen&#125; 与我们上述分析的实现过程一致，更多细节感兴趣的童鞋可自行阅读 ServiceLoader 如何实现动态加载同一个 ServiceLoader 对象的话，不会重新加载META-INF/services/下的信息。如果我们需要动态加载的话，可以考虑每次重新创建新的ServiceLoader 对象，或者调用 ServiceLoader::reload demo 地址https://github.com/TavenYin/java-spi.git 如果觉得有收获，可以关注我的公众号【殷天文】，你的点赞和关注就是对我最大的支持]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot Websocket 实战]]></title>
    <url>%2Fpost%2Fa77ef6c7.html</url>
    <content type="text"><![CDATA[Websocket 是一种在单个TCP连接上进行全双工通信的协议。WebSocket连接成功后，服务端与客户端可以双向通信。在需要消息推送的场景，Websocket 相对于轮询能更好的节省服务器资源和带宽，并且能够更实时地进行通讯。 与 HTTP 协议有着良好的兼容性。默认端口也是80和443，并且握手阶段采用 HTTP 协议，因此握手时不容易屏蔽，能通过各种 HTTP 代理服务器。 依赖于TCP协议 数据格式比较轻量，性能开销小，通信高效。 可以发送文本，也可以发送二进制数据。 没有同源限制，客户端可以与任意服务器通信。 协议标识符是ws（如果加密，则为wss），服务器网址就是 URL。 SpringBoot 中使用 Websocket在简单了解Websocket 之后，我们来动手实践一下。SpringBoot 中有多种方式可以实现Websocket Server，这里我选择使用Tomcat 中 javax.websocket.server 的api来实现，结尾会给出demo地址 引入Maven依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-websocket&lt;/artifactId&gt;&lt;/dependency&gt; 创建一个Bean用于处理Websocket 请求，通过ServerEndpoint 声明当前Bean 接受的Websocket URL 这里为什么声明的是 @Controller，后文会解释 1234567891011121314151617181920212223242526272829303132333435363738394041import org.springframework.stereotype.Controller;import javax.websocket.*;import javax.websocket.server.ServerEndpoint;@ServerEndpoint(value = &quot;/message_websocket&quot;)@Controllerpublic class MsgWebsocketController &#123; @OnOpen public void onOpen(Session session) &#123; // 先鉴权，如果鉴权通过则存储WebsocketSession，否则关闭连接，这里省略了鉴权的代码 WebSocketSupport.storageSession(session); System.out.println(&quot;session open. ID:&quot; + session.getId()); &#125; /** * 连接关闭调用的方法 */ @OnClose public void onClose(Session session) &#123; System.out.println(&quot;session close. ID:&quot; + session.getId()); &#125; /** * 收到客户端消息后调用的方法 */ @OnMessage public void onMessage(String message, Session session) &#123; System.out.println(&quot;get client msg. ID:&quot; + session.getId() + &quot;. msg:&quot; + message); &#125; /** * 发生错误时调用 */ @OnError public void onError(Session session, Throwable error) &#123; error.printStackTrace(); &#125;&#125; 声明 ServerEndpointExporter 123456789@Configurationpublic class WebsocketConfig &#123; @Bean public ServerEndpointExporter serverEndpointExporter() &#123; return new ServerEndpointExporter(); &#125;&#125; 至此，Websocket Server 已经搭建完成，客户端已经可以和服务端通信了 服务端 向客户端推送消息 通过 session.getBasicRemote().sendText(message); 即可 源码浅析我们来看下上述的短短几行代码是如何为我们构建 Websocket Server ServerEndpointExporter 重点关注下红框中的内容 ServerEndpointExporter 实现了 SmartInitializingSingleton，会在bean 实例化结束后调用 afterSingletonsInstantiated 从Spring上下文中获取所有标记@ServerEndpoint的Bean的name 其实 我们声明的 MsgWebsocketController 中并不是只能标记@Controller，只是为了将其注册到Spring容器中，方便ServerEndpoint的注册而已，标记 @Controller 更符合Spring的开发规范 通过ServerContainer 将所有标记@ServerEndpoint的Bean 注册 ServerContainer 默认的实现类为 WsServerContainer，会对我们的ServerEndpoint做一个映射，URL =&gt; 对应的class，然后针对不同的事件调用指定的方法（例如建立连接时调用标记@Onopen的方法），这有点Spring DispatcherServlet 那味，感兴趣的同学可以自己看下 在了解了 Spring 为我们做了什么后，我们来完善一下我们的Demo 建立一个SessionManager当我们想向客户端推送消息的时候，首先我们需要找到客户端与服务端建立的连接，也就是WebscoketSession WsServerContainer 中虽然已经存储了 WebscoketSession，但是并没有办法直接通过SessionId，或者我们的业务Id 直接定位到指定的Session，所以我们需要实现一个自己的SessionManager 1final ConcurrentHashMap&lt;Object, Session&gt; sessionPool = new ConcurrentHashMap&lt;&gt;(); 使用 ConcurrentHashMap 管理即可 分布式推送解决 如图，用户1与服务器A建立Webscoket，用户2与服务器B建立Webscoket，那么用户1如果想向用户2推送一条消息，该如何实现？ WebscoketSession 实际上是网络连接，并不像我们传统应用的Session可以序列化到Redis，只能每个服务器管理自己的WebscoketSession，所以此时服务器A通知服务器B，你要给用户2推送一条消息。 一个比较简单有效的实现方法，利用消息队列，如下图 这个方案优点是实现简单，缺点是每台服务器都需要判断一遍当前是否存在指定的WebscoketSession ，方案细化的话则需要维护用户Session与每台服务器的关系，这样直接将消息推送给指定服务器即可 其他问题测试时发现，当客户端断网后，服务端检测不到客户端失去连接的情况，依然可以调用Session的推送方法，服务端会一直持有这个无效的Session 目前想到的解决方案：设置WebsocketSession的最大空闲时间（session.setMaxIdleTimeout(milliseconds);），当超过这个时间时，服务端会关闭Session。前端定期发送一条心跳包，用于维持Session，当出现上述情况时，服务端也不会一直持有Session了 完整demo地址关于demo的细节参考项目地址中Readme Github 👉 https://github.com/TavenYin/taven-springboot-learning/tree/master/sp-websocket Gitee 👉 https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/sp-websocket 参考http://www.ruanyifeng.com/blog/2017/05/websocket.html 部分代码参考了一位兄弟的博客，但是由于时间有点长，找不到了，在此说一声抱歉 如果觉得有收获，可以关注我的公众号【殷天文】，第一时间接收到我的更新]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>Websocket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis能否保证数据高可靠性]]></title>
    <url>%2Fpost%2Ff4cc44a4.html</url>
    <content type="text"><![CDATA[记录下工作中关于Redis的一些思考，主要关于Redis的事务，脚本，持久化 本文讨论的问题： Redis的事务或者执行lua脚本可以像关系型数据库事务那样，要么全部提交，要么全部回滚吗？ 当脚本或者事务执行过程中发生宕机Redis中的数据会丢失吗？ 原子性在Redis的开发文档中可以了解到，Redis的事务以及Redis执行lua脚本都可以保证原子性（Redis的每条命令也是原子的），那么原子性可以保证什么？能否解决我们的问题？先来看下原子性的定义 来自维基百科对关系型数据库事务原子性的定义 事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行 来自百度百科原子操作的定义 原子操作是不可分割的，在执行完毕之前不会被任何其它任务或事件中断 Redis 所保证的原子性正是如此 不可分割，保证事务和脚本执行过程中，不会被其他客户端的命令打断 事务和脚本中的命令，要么都执行，要么都不执行 Redis Transactions如何使用Redis 事务12345&gt; MULTI // 开启事务OK&gt; SET KEY VALUE // 执行命令，此时的命令只是入队，会在 EXEC 之后原子性的执行事务中所有命令QUEUED&gt; EXEC / DISCARD // 执行或者取消事务 Redis的事务保证 在Redis事务的执行过程中，永远不会执行另一个客户端发出的请求 所有命令都会执行，或者不执行。如果在执行 EXEC 之前，客户端与服务端失去响应，那么事务中所有的命令都不会执行，相反如果执行 EXEC 保证所有的命令都执行。 事务中可能发生的错误 EXEC 之前：命令无法Queued，例如语法错误（错误的参数数量，错误的命令），或者一些其他错误，例如内存不足（Redis使用了maxmemory 限制最大内存） EXEC 之后：命令也可能会失败，例如针对string类型使用list的命令 EXEC 之前的错误，客户端可以通过检查服务端的响应来解决，当发生入队失败时（Redis响应值非QUEUED），客户端DISCARD当前事务 从Redis 2.6.5开始，Redis 会记录事务期间发生的错误，并拒绝执行事务，在执行EXEC时返回错误信息并自动丢弃该事务 但是EXEC 之后的错误，即使导致部分命令执行失败，Redis还是会执行其他的剩余命令 Redis 事务的不回滚机制虽然Redis保证了原子性，但是他的事务并不会像关系型数据库那样，在Redis事务中如果某条命令发生了错误，其他的命令会依旧执行，这点相比较关系型数据库来说不免有些”奇怪”了 Redis开发文档中给出的解释如下 只有语法错误才可能导致命令执行失败，大多数情况都是编程错误导致的 因为不需要回滚，使得 Redis 更简单 更高效 关于事务的更多内容👉 https://redis.io/topics/transactions Redis lua2.6.0 版本后，Redis 增加了对lua脚本的支持，脚本和Redis事务一样保证了原子性，执行脚本时不会执行其他脚本或Redis命令（所以不要让脚本运行时间过长），同样脚本也没有回滚机制，当脚本中出现lua的异常，或者Redis命令错误，也无法保证全部执行成功 Redis lua 和事务有点类似，但是有些场景使用事务是无法做到的，例如我想对Redis中的数据先读，然后根据原有数据变更，整个过程想要保证原子性，由于事务在EXEC之前无法获取返回值，使用lua 就非常合适 关于Redis lua 更多内容 👉 https://redis.io/commands/eval Redis 执行命令时宕机数据会丢失吗看到了这，第一个问题我们已经清楚了，Redis并没有回滚的能力，但是通常情况下，这些需要回滚的场景都是编码错误，我们是可以避免的。我们继续探寻第二个问题的答案 Redis是基于内存的数据库，所以当发生宕机或者停止后重新启动时，Redis会使用磁盘上的持久化文件来恢复数据，所以是否能恢复数据，能恢复多少数据，取决于使用哪一种持久化策略 简单说下两种持久化策略 RDB按指定的时间间隔将内存中数据集写入快照 AOFAOF会记录服务器接收的每个写入操作。当Redis命令执行成功后，命令会被传播到AOF程序中。AOF 的三种同步策略 appendfsync always ：每次有数据修改发生时都会同步到AOF文件 appendfsync everysec ：每秒钟同步一次，AOF的默认策略 appendfsync no ：将数据同步将给操作系统管理，通常linux系统30s会同步一次数据，但这取决于操作系统 即使使用always 也无法保证写入的每一条命令都被持久化，从命令执行成功到数据保存到硬盘之间，还是有一段非常小的间隔 回到我们的问题上，如果使用RDB，毫无疑问，数据只能恢复到上次的备份 即使使用AOF的话，如果在Redis事务执行期间宕机，那么这次事务还是相当于”没有执行”，由于命令还没来及写入AOF，在服务恢复后更不可能恢复数据。对于Redis客户端而言，会收到服务端的异常响应 写入AOF的过程也是会被打断的，Redis 文档中提到，如果Redis服务器突然崩溃，导致出现了”半写状态”的AOF文件时，服务器重新启动时，会检测到这种情况，并且退出提示用户使用 redis-check-aof 修复 AOF 文件。”半写状态”的事务或者命令会被删除，服务器可以重新启动。 如果写入AOF过程被打断，对于客户端而言可能是毫无感知的（看了下Redis命令执行相关，AOF应该是发生在响应客户端之后） 所以第二个问题，我们也清楚了，Redis 并不能保证我们写入的数据都安全的持久化 关于持久化的更多内容👉 https://redis.io/topics/persistence 扩展：脚本如何持久化这里说的是AOF的情况 在Redis 5 之前，默认是将脚本本身传播到AOF中。这种传播方式的好处，不需要将脚本转成Redis命令，在写入AOF或者其他Redis实例时不会占用过多的带宽和CPU 复制脚本不允许脚本中出现随机性的写入，因为这会导致通过AOF恢复数据时，数据不一致，在这点上Redis做了一些限制，由于不是本文重点就不多说了，可以参考Redis开发文档 从Redis 3.2 开始新增了一种脚本复制方式 script effects replication（Redis 5 开始默认使用这种方式处理脚本）。这种模式下，Redis 会收集脚本中所有修改数据的命令。当脚本执行完成后，这些命令被包装成一个事务，传播到AOF 和其他实例。 这种方式的好处 当脚本执行很慢的时候，会影响加载AOF重建数据的时间，这种情况使用 script effects replication 效率更高 这种方式会允许脚本中存在随机性的写入 使用方式12-- 在执行Redis命令前调用，成功启用 script effects replication 返回trueredis.replicate_commands() 总结所以说Redis无论是事务还是脚本，并不能做到像关系型数据事务一样，所以针对数据一致性要求较高的业务场景，并不适合使用Redis 而且从持久化方面来考虑，这也不是Redis的强项，Redis的优势正是基于内存，所以读写性能高。虽然宕机的可能性看似很极端，通常我们使用了某个服务后，我们会尽可能的保证它的高可用，但是我们需要知道Redis的”持久化”并不能保证我们的数据绝对安全，所以当我们的业务场景对数据一致性，持久化要求很高的时候，关系型数据库依旧是很好的选择 参考Redis 设计与实现 AOFRedis 设计与实现 事务Redis 命令执行过程(上)Redis 命令执行过程(下)]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java ThreadLocal 实现原理]]></title>
    <url>%2Fpost%2F764a84a5.html</url>
    <content type="text"><![CDATA[ThreadLocal 线程本地变量，算是Java开发中比较常用的API了，今天我们来一探究竟 使用场景ThreadLocal 适用于每个线程需要自己独立的实例且该实例需要在多个方法中被使用，也就是变量在线程间隔离，而在同一线程共享的场景。例如管理Connection，我们希望每个线程只使用一个Connection实例，这个时候用ThreadLocal就很合适。 12345678910111213141516public class ThreadLocalDemo &#123; private static final ThreadLocal&lt;Object&gt; threadLocal = new ThreadLocal&lt;&gt;(); public static void main(String[] args) &#123; threadLocal.set(new Object()); someMethod(); &#125; static void someMethod() &#123; // 获取在threadLocal中存储的对象 threadLocal.get(); // 清除ThreadLocal中数据 threadLocal.remove(); &#125;&#125; 还有之前写过的一篇动态切换数据源 https://www.jianshu.com/p/0a485c965b8b，AOP 通过 ThreadLocal 保存当前线程需要访问的数据源的key，AbstractRoutingDataSource 再通过 ThreadLocal 中的数据切换到指定的数据源，对业务代码毫无入侵 原理在我们了解了如何使用之后，来看下 ThreadLocal 是如何实现的 ThreadLocal.get()我们从get方法来分析，可以看到方法中获取当前线程，并通过当前线程得到一个 ThreadLocalMap，我们可以暂时把这个ThreadLocalMap 理解为我们熟悉的HashMap，然后通过 this（当前ThreadLocal对象）作为key，从Map中获取Entry 我们再来看下，ThreadLocalMap 以及ThreadLocalMap.Entry 中的核心成员变量，ThreadLocalMap 中实现了一个简单的hash表 看到这里你可能还不是很清晰，结合下面这张图理解一下，每个线程（Thread对象）中有一个ThreadLocalMap，使线程之间的数据天然隔离，ThreadLocalMap 有一张hash表 Entry[]，每个 Entry 中对应存储着一个ThreadLocal实例 - value，这样使得不同的ThreadLocal 对象之间也形成了隔离 ThreadLocalMap 中的hash表我们通过 ThreadLocalMap.set() 来了解下内部的hash表是如何实现的 线性探测是指当发生hash冲突时，利用固定的算法寻找一定步长的下个位置（ThreadLocal中发生hash冲突时，index+1），依次判断，直至找到能够存放的位置 如果线程中操作了大量的 ThreadLocal 对象，势必会造成hash冲突，这是没有必要的性能开销，如果可以的话，我们可以只保留一个ThreadLocal对象 关于 ThreadLocal 的一些思考 为什么要使用弱引用 图3中，我们看到hash表中会出现 key == null的Entry，这是因为 ThreadLocalMap.Entry 的key （Entry 对ThreadLocal设置了弱引用，可以回顾一下图2） 弱引用的对象拥有更短暂的生命周期。在GC时，一旦发现了对象只具有弱引用，这个对象一定被回收 这么做的原因：如果ThreadLocal 对象需要被回收时（此时并没有调用ThreadLocal.remove），线程中的ThreadLocalMap 一直强引用着 ThreadLocal对象，这会让 ThreadLocal对象 以及对应的value对象内存无法释放，导致内存泄漏。这算是ThreadLocal的一种容错机制，这样做使得了ThreadLocal对象得到了回收，但是value的内存并没有释放，所以ThreadLocalMap 的get、set方法中都会去尝试清理ThreadLocal已经被回收的entry。 使用过后不及时remove会怎么样 很多博客中都强调了，ThreadLocal.remove的重要性。举个例子，我们新启了一个线程在这个线程中使用了ThreadLocal，我们并没有调用remove，这会导致存储的value对象一直没有办法被回收，直到线程被销毁 线程池中也需要remove吗 以web线程池为例，如果每次都在过滤器中操作同一个ThreadLocal.set，然后业务代码中get，似乎没什么问题。计算出的hash值都是一样的，槽位也是一样的会覆盖上一次的值。确实业务不会有问题，但是还是推荐大家在使用完之后remove，因为这样会让无用的value对象早点被回收，在很多java源码中都会看到，对一些不再使用的对象进行如下的help GC操作1object = null // help GC 所以我们也需要让无用的对象失去引用，帮助GC 综上所述 ThreadLocal 使用过后要及时remove，帮助JVM释放内存 参考https://www.jianshu.com/p/98b68c97df9b]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>源码</tag>
        <tag>Thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电商技术--库存设计指北]]></title>
    <url>%2Fpost%2Fc4f13c61.html</url>
    <content type="text"><![CDATA[前言最近在解决一套老电商系统的库存”超卖”问题。一直以为超卖问题，最难解决的是库存扣减，实则不然，我们的系统在解决了库存扣减问题之后，还会一直有“超卖”现象？这一切的背后到底是道德的沦丧，还是人性的扭曲，欢迎收看本期走近科学 本文带你解决以下电商场景问题 保证库存线程安全的扣减 防止库存的多次扣减、回滚 超时未支付被取消的订单（取消会回滚库存）， 如果收到了支付回调怎么办 如何线程安全的扣减库存先来说说库存扣减的问题，这是我们原来老系统的逻辑，注意！这里是错误的示例 123456789// 以下是伪代码，错误的示例// 查询出Goods对象$goods = selectGoodsById($id);if ($goods-&gt;num - $order_num &gt; 0) &#123; // 计算出扣减后的库存 $goods-&gt;num = $goods-&gt;num - $order_num; // 保存 save($goods);&#125; 上述代码犯了大忌，并发情况会导致多个线程读到相同的库存数，然后扣减，然后保存到DB，下面我们来说下正确的姿势 正确的做法利用MySQL update 会持有当前记录锁的特点，保证线程安全的扣减 SQL 示例：1update kucun set num = num - ? where id = ? and num - ? &gt;= 0 我们的这条记录根据主键更新，当事务A update 这条记录时，会持有当前记录的锁，当事务A未提交时，其他想要更新这条记录的事务只能等待锁释放 关于MySQL update 锁的细节，本文不讨论，可以参考MySQL文档 https://dev.mysql.com/doc/refman/8.0/en/innodb-locks-set.htmlhttps://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.htmlhttps://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html 虽然MySQL可以保证数据的准确性，但是大并发量场景下，大量的锁竞争，导致库存的扣减可能成为系统性能的瓶颈 使用 Redis 库存扣减使用Redis的优势很多，单线程的文件事件处理器保证了并发下可以线程的安全扣减、回滚库存， 以及Redis高性能。 虽然Redis解决了线程安全和性能的问题，但是Redis并不能做到像MySQL那样一条SQL命令完成库存扣减，我们需要先读出已有库存，再和当前下单库存做一个判断是否可以库存扣减。所以最佳的实现方案是通过Redis 执行lua脚本，保证整个逻辑处理期间，不会有其他客户端插进来 12345678910111213141516171819202122232425262728293031/** * * 扣减库存Lua脚本 * 库存（stock）-1：表示不限库存 * 库存（stock）0：表示没有库存 * 库存（stock）大于0：表示剩余库存 * * @params 库存key * @return * -3:库存未初始化 * -2:库存不足 * -1:不限库存 * 大于等于0:剩余库存（扣减之后剩余的库存） */const SUB_STOCK_LUA = &quot; if (redis.call(&apos;exists&apos;, KEYS[1]) == 1) then local stock = tonumber(redis.call(&apos;get&apos;, KEYS[1])); local num = tonumber(ARGV[1]); if (stock == -1) then return -1; end; if (stock &gt;= num) then return redis.call(&apos;incrby&apos;, KEYS[1], 0 - num); end; return -2; end; return -3;&quot;; 注意：当对一个订单中的 good_list 扣减库存的时候，需要注意，当某一个商品库存扣减失败时，之前的扣减的商品库存需要回滚。这会涉及到对redis的多次操作，你可以把整体逻辑写到一个lua脚本中 使用Redis 做库存扣减会有一个问题（伪代码如下），Redis数据和MySQL数据并不能保证强一致性，因为Redis的数据相当于直接写进去了，如果在需要回滚的时候，Redis不可用了导致数据无法回滚，最终会造成MySQL没有写入订单数据，Redis却扣减了库存 123456789101112try &#123; $db-&gt;beginTransaction(); $db-&gt;saveOrder(); $redis-&gt;reduceStock(); $db-&gt;commit(); &#125; catch (Exception $e) &#123; $db-&gt;rollback(); $redis-&gt;rollbackStock();&#125; 这种情况并没有什么好的解决办法，这是一个几率非常小的故障，首先我们肯定要尽可能的保证Redis的高可用性，其次在发生这种情况后，我们要想办法恢复Redis中的数据，例如我们可以在整个逻辑之后，选择异步的方式（例如MQ）向MySQL中同步库存，当发生故障后，以MySQL数据为准恢复数据 所以Redis是一把双刃剑，提升性能的同时，也带来了问题 AliSQL这是后来我在网上看到的方案，AliSQL 是阿里自研 MySQL 分支，AliSQL 针对并发修改同一记录的情况，使用数据库层面的缓冲队列，避免大量争锁的代价。感兴趣的同学可以试下（阿里云MySQL 8.0 集成了这一功能），如果AliSQL解决了性能问题的话，那么这个方案相比Redis要更好 关于库存多次扣减的问题当订单的提交和库存的扣减同步进行的时候，不需要考虑这个问题。 举例：订单系统生成订单之后，通过MQ通知库存系统，库存系统异步扣减库存，这个时候库存系统可能会多次消费，这个时候就需要考虑这个问题了。 或者我们上面说的通过MQ同步MySQL库存也需要考虑可能发生多次扣减 解决方案如图，通过订单做为唯一索引保证流水记录的唯一性，从而保证只能有一次成功的扣减 库存回滚问题多数博客对于超卖的讲解只在于库存的扣减，但是库存扣减安全了，真的就可以保证不超卖吗？我们的系统在解决了库存扣减问题后，还是出现成交订单 &gt; 库存的问题，为此我也是绞尽脑汁，抓破了头 在对下单进行压力测试之后，我坚信下单不会出现超卖的问题，后来我怀疑问题出在了库存回滚，如果一个订单回滚了两次库存（取消超时未支付订单的线程和用户线程同时取消一个订单），同样也会出现超卖的现象。 解决方法：和防止多次扣减一样，采用写入订单回滚流水的方式，个人认为这种方法比较加锁要好，数据有迹可循 超时未支付被取消的订单收到了支付回调在解决了库存回滚问题之后，超卖问题还没有解决，最后通过日志定位到了这个问题。 问题描述：用户在系统即将自动取消订单的前一瞬间完成了支付，系统取消了该订单并回滚了库存，同时系统收到了该订单的支付回调，该订单的状态更改为已支付，因为不该出现的库存回滚导致了“超卖” 下面说下我们的解决方案，以微信支付为例 我们的系统在提交订单之后，会调用微信的统一下单接口，这时候微信收到了我们的商户订单号（微信已经生成订单），用户选择不支付。超时自动取消逻辑处理之前，先调用微信的关闭订单接口，如果关闭成功，则这个时候用户后续无法对该订单发起支付。如果返回订单已支付，则无需处理该订单，该订单会收到微信支付的回调 参考https://www.jianshu.com/p/76bc0e963172https://www.zhihu.com/question/268937734https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=9_3 如果觉得文章有帮助，欢迎点赞、转发、关注我的公众号，你的支持就是我最大的动力]]></content>
      <categories>
        <category>电商技术</category>
      </categories>
      <tags>
        <tag>电商</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教你如何使用MySQL8递归.]]></title>
    <url>%2Fpost%2F2e0c1b4e.html</url>
    <content type="text"><![CDATA[之前写过一篇 MySQL通过自定义函数的方式，递归查询树结构，从MySQL 8.0 开始终于支持了递归查询的语法 CTE首先了解一下什么是 CTE，全名 Common Table Expressions12345WITH cte1 AS (SELECT a, b FROM table1), cte2 AS (SELECT c, d FROM table2)SELECT b, d FROM cte1 JOIN cte2WHERE cte1.a = cte2.c; cte1, cte2 为我们定义的CTE，可以在当前查询中引用 可以看出 CTE 就是一个临时结果集，和派生表类似，二者的区别这里不细说，可以参考下MySQL开发文档：https://dev.mysql.com/doc/refman/8.0/en/with.html#common-table-expressions-recursive-examples 递归查询先来看下递归查询的语法1234567WITH RECURSIVE cte_name AS( SELECT ... -- return initial row set UNION ALL / UNION DISTINCT SELECT ... -- return additional row sets)SELECT * FROM cte; 定义一个CTE，这个CTE 最终的结果集就是我们想要的 ”递归得到的树结构”，RECURSIVE代表当前 CTE 是递归的 第一个SELECT 为 “初始结果集” 第二个SELECT 为递归部分，利用 “初始结果集/上一次递归返回的结果集” 进行查询得到 “新的结果集” 直到递归部分结果集返回为null，查询结束 最终UNION ALL 会将上述步骤中的所有结果集合并（UNION DISTINCT 会进行去重），再通过 SELECT * FROM cte; 拿到所有的结果集 递归部分不能包括： 聚合函数例如 SUM() GROUP BY ORDER BY LIMIT DISTINCT 上面的讲解可能有点抽象，通过例子慢慢来理解12345678910111213141516171819WITH RECURSIVE cte (n) AS -- 这里定义的n相当于结果集的列名，也可在下面查询中定义( SELECT 1 UNION ALL SELECT n + 1 FROM cte WHERE n &lt; 5)SELECT * FROM cte;-- result+------+| n |+------+| 1 || 2 || 3 || 4 || 5 |+------+ 初始结果集为 n =1 这时候看递归部分，第一次执行 CTE结果集即是 n =1，条件发现并不满足 n &lt; 5，返回 n + 1 第二次执行递归部分，CTE结果集为 n = 2，递归… 直至条件不满足 最后合并结果集 EXAMPLE最后来看一个树结构的例子123456CREATE TABLE `c_tree` ( `id` int(11) NOT NULL AUTO_INCREMENT, `cname` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL, `parent_id` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=13 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; 1234567891011121314151617mysql&gt; select * from c_tree;+----+---------+-----------+| id | cname | parent_id |+----+---------+-----------+| 1 | 1 | 0 || 2 | 2 | 0 || 3 | 3 | 0 || 4 | 1-1 | 1 || 5 | 1-2 | 1 || 6 | 2-1 | 2 || 7 | 2-2 | 2 || 8 | 3-1 | 3 || 9 | 3-1-1 | 8 || 10 | 3-1-2 | 8 || 11 | 3-1-1-1 | 9 || 12 | 3-2 | 3 |+----+---------+-----------+ 1234567891011121314151617mysql&gt; WITH RECURSIVE tree_cte as( select * from c_tree where parent_id = 3 UNION ALL select t.* from c_tree t inner join tree_cte tcte on t.parent_id = tcte.id)SELECT * FROM tree_cte;+----+---------+-----------+| id | cname | parent_id |+----+---------+-----------+| 8 | 3-1 | 3 || 12 | 3-2 | 3 || 9 | 3-1-1 | 8 || 10 | 3-1-2 | 8 || 11 | 3-1-1-1 | 9 |+----+---------+-----------+ 初始结果集R0 = select * from c_tree where parent_id = 3 递归部分，第一次 R0 与 c_tree inner join 得到 R1 R1 再与 c_tree inner join 得到 R2 … 合并所有结果集 R0 + … + Ri 更多信息https://dev.mysql.com/doc/refman/8.0/en/with.html]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redisson源码解析，如何利用Redis实现分布式可重入锁]]></title>
    <url>%2Fpost%2Fd5899455.html</url>
    <content type="text"><![CDATA[最开始使用Redisson 的api的时候，我觉得哇，这个api 太牛逼了居然有分布式的可重入锁，正好最近研究了下Redisson的源码，和大家分享一下 前言首先我们先回顾一下 Java 中的 ReentrantLock 是如何实现的？ 这里我先简单介绍一下ReentrantLock 实现的思路 锁标识：通过AQS的state变量作为锁标识，利用Java的CAS保证多线程竞争锁时的线程安全问题 队列：未竞争到锁的线程进入AQS的队列并挂起，等待解锁时被唤醒（或者超时） 如何设计分布式可重入锁首先锁标识，这个在Redis中很容易实现，可以用lock name 作为key，当前线程生成一个uuid，作为value，加上Redis 单线程模型，实现线程安全的锁竞争 这种方式在之前的博客里也提到过，可以参考下 Redis分布式锁的正确实现方式 但是如何基于Redis 做一个队列，像Java那样可以挂起唤醒线程呢？这点我在看源码之前一直没有想到… 那么Redisson 是如何做的呢？ 答案：利用Redis的发布订阅，加上Java的Semaphore（信号量，不了解Semaphore的小伙伴可以Google一下） Redisson 分布式锁实现思路锁标识：Hash 数据结构，key 为锁的名字，filed 当前竞争锁成功线程的“唯一标识”，value 重入次数 队列：所有竞争锁失败的线程，会订阅当前锁的解锁事件，利用 Semaphore 实现线程的挂起和唤醒 源码分析我们来看一下tryLock方法的源码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 public boolean tryLock(long waitTime, long leaseTime, TimeUnit unit) throws InterruptedException &#123; long time = unit.toMillis(waitTime); long current = System.currentTimeMillis(); long threadId = Thread.currentThread().getId(); // 尝试获取锁，返回null 代表获取锁成功，当获取锁失败时返回当前锁的释放时间 Long ttl = tryAcquire(leaseTime, unit, threadId); // lock acquired if (ttl == null) &#123; return true; &#125; // 如果此时已经超过等待时间则获取锁失败 time -= System.currentTimeMillis() - current; if (time &lt;= 0) &#123; acquireFailed(threadId); return false; &#125; current = System.currentTimeMillis(); // 订阅解锁事件 RFuture&lt;RedissonLockEntry&gt; subscribeFuture = subscribe(threadId); // 等待订阅成功，成功后唤醒当前线程 if (!await(subscribeFuture, time, TimeUnit.MILLISECONDS)) &#123; if (!subscribeFuture.cancel(false)) &#123; subscribeFuture.onComplete((res, e) -&gt; &#123; if (e == null) &#123; unsubscribe(subscribeFuture, threadId); &#125; &#125;); &#125; acquireFailed(threadId); return false; &#125; try &#123; // 再次判断一下是否超时 time -= System.currentTimeMillis() - current; if (time &lt;= 0) &#123; acquireFailed(threadId); return false; &#125; while (true) &#123; long currentTime = System.currentTimeMillis(); // 尝试获取锁 ttl = tryAcquire(leaseTime, unit, threadId); // lock acquired if (ttl == null) &#123; return true; &#125; time -= System.currentTimeMillis() - currentTime; if (time &lt;= 0) &#123; acquireFailed(threadId); return false; &#125; // waiting for message currentTime = System.currentTimeMillis(); if (ttl &gt;= 0 &amp;&amp; ttl &lt; time) &#123; // 等待解锁消息，此处利用Semaphore，锁未释放时，permits=0，线程处于挂起状态 // 当发布解锁消息时，当前的Semaphore对象的release() permits=1 // 所有的客户端都会有一个线程被唤醒，去尝试竞争锁 getEntry(threadId).getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS); &#125; else &#123; getEntry(threadId).getLatch().tryAcquire(time, TimeUnit.MILLISECONDS); &#125; time -= System.currentTimeMillis() - currentTime; if (time &lt;= 0) &#123; acquireFailed(threadId); return false; &#125; &#125; &#125; finally &#123; unsubscribe(subscribeFuture, threadId); &#125;// return get(tryLockAsync(waitTime, leaseTime, unit)); &#125; tryAcquire(leaseTime, unit, threadId); 这个方法我们下面会分析，现在我们只需要知道这个方法是用来获取锁就可以了 这个时候我们已经可以理清Redisson可重入锁的思路了 获取锁 如果获取锁失败，订阅解锁事件 之后是一个无限循环12345678910while(true) &#123; // 尝试获取锁 // 判断是否超时 // 等待解锁消息释放信号量 //（此时每个Java客户端都可能会有多个线程被挂起，但是只有一个线程会被唤醒） // 判断是否超时&#125; 利用信号量，合理控制线程对锁的竞争，合理利用系统资源，可以说做的灰常的奈斯了 需要注意：!await(subscribeFuture, time, TimeUnit.MILLISECONDS) ，这里很多博客都解释错了，这里并不是等待发布解锁消息，只要订阅事件成功后，就会往下执行，真正等待解锁消息的是 getEntry(threadId).getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS); 这里你可能不信，为什么我说的就对啊，debug一下你就知道 tryLockInnerAsynctryAcquire 内部依靠 tryLockInnerAsync 来实现获取锁的逻辑，我们来看下源码123456789101112131415161718192021222324252627&lt;T&gt; RFuture&lt;T&gt; tryLockInnerAsync(long leaseTime, TimeUnit unit, long threadId, RedisStrictCommand&lt;T&gt; command) &#123; internalLockLeaseTime = unit.toMillis(leaseTime); return commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, command, // 是否存在锁 "if (redis.call('exists', KEYS[1]) == 0) then " + // 不存在则创建 "redis.call('hset', KEYS[1], ARGV[2], 1); " + // 设置过期时间 "redis.call('pexpire', KEYS[1], ARGV[1]); " + // 竞争锁成功 返回null "return nil; " + "end; " + // 如果锁已经被当前线程获取 "if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " + // 重入次数加1 "redis.call('hincrby', KEYS[1], ARGV[2], 1); " + "redis.call('pexpire', KEYS[1], ARGV[1]); " + "return nil; " + "end; " + // 锁被其他线程获取，返回锁的过期时间 "return redis.call('pttl', KEYS[1]);", // 下面三个参数分别为 KEYS[1], ARGV[1], ARGV[2] // 即锁的name，锁释放时间，当前线程唯一标识 Collections.&lt;Object&gt;singletonList(getName()), internalLockLeaseTime, getLockName(threadId));&#125; tryLockInnerAsync 中利用lua脚本 和 Redis 单线程的特点来实现锁的竞争 这里可以看到锁的结构，和我们上文所说的一样，Hash 数据结构，key 为锁的name，filed 当前竞争锁成功线程的”唯一标识”，value 重入次数 unlockInnerAsync接下来我们再来看解锁的核心代码123456789101112131415161718192021222324252627282930protected RFuture&lt;Boolean&gt; unlockInnerAsync(long threadId) &#123; return commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN, // 用锁的name和线程唯一标识去判断是否存在这样的键值对 // 解铃还须系铃人，不存在则无权解锁，返回null "if (redis.call('hexists', KEYS[1], ARGV[3]) == 0) then " + "return nil;" + "end; " + // 解锁逻辑 // 冲入次数-1 "local counter = redis.call('hincrby', KEYS[1], ARGV[3], -1); " + // 如果大于0 代表当前线程重入锁多次无法解锁，更新锁的有效时间 "if (counter &gt; 0) then " + "redis.call('pexpire', KEYS[1], ARGV[2]); " + "return 0; " + "else " + // 解锁，删除key "redis.call('del', KEYS[1]); " + // 发布解锁消息 "redis.call('publish', KEYS[2], ARGV[1]); " + "return 1; "+ "end; " + "return nil;", // KEYS[1]，KEYS[2] // 锁的name，发布订阅的Channel Arrays.&lt;Object&gt;asList(getName(), getChannelName()), // ARGV[1] ~ ARGV[3] // 解锁消息，释放时间，当前线程唯一标识 LockPubSub.UNLOCK_MESSAGE, internalLockLeaseTime, getLockName(threadId));&#125; 发布解锁消息后，会调用到LockPubSub 的 onMessage，释放信号量，唤醒等待锁的线程1234567891011121314151617181920212223242526272829303132333435363738public class LockPubSub extends PublishSubscribe&lt;RedissonLockEntry&gt; &#123; public static final Long UNLOCK_MESSAGE = 0L; public static final Long READ_UNLOCK_MESSAGE = 1L; public LockPubSub(PublishSubscribeService service) &#123; super(service); &#125; @Override protected RedissonLockEntry createEntry(RPromise&lt;RedissonLockEntry&gt; newPromise) &#123; return new RedissonLockEntry(newPromise); &#125; @Override protected void onMessage(RedissonLockEntry value, Long message) &#123; if (message.equals(UNLOCK_MESSAGE)) &#123; Runnable runnableToExecute = value.getListeners().poll(); if (runnableToExecute != null) &#123; runnableToExecute.run(); &#125; // 释放信号量 value.getLatch().release(); &#125; else if (message.equals(READ_UNLOCK_MESSAGE)) &#123; while (true) &#123; Runnable runnableToExecute = value.getListeners().poll(); if (runnableToExecute == null) &#123; break; &#125; runnableToExecute.run(); &#125; value.getLatch().release(value.getLatch().getQueueLength()); &#125; &#125;&#125; 参考 慢谈 Redis 实现分布式锁 以及 Redisson 源码解析 https://www.programcreek.com/java-api-examples/?code=rollenholt-SourceReading/redisson/redisson-master/src/main/java/org/redisson/RedissonLock.java 欢迎点赞、转发。你的支持就是对我最大的帮助]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>锁</tag>
        <tag>Redis</tag>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>源码</tag>
        <tag>分布式</tag>
        <tag>Redisson</tag>
        <tag>ReentrantLock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL优化实战--索引篇]]></title>
    <url>%2Fpost%2F1e0091db.html</url>
    <content type="text"><![CDATA[关于SQL优化，这个问题，相信大家过多过少都有过一些了解。最近我也在研究SQL优化方面的东西，分享一些经验。 首先简单介绍下索引，“索引” 是SQL优化中很重要的一部分（但是索引并不是优化的唯一选项） 索引原理简述如何理解索引？索引其实就是一种数据结构，用于快速定位和访问数据库中的数据。 通常来说索引使用的数据结构是 B-Tree / B+Tree。以B-Tree为例，假设每个节点存储100个Key，三层的B-Tree 可存储一百万数据，如果将根节点存入内存中的话，只需要读取两次磁盘就可以从100万数据中找到指定数据 关于B-Tree 推荐阅读这篇 https://www.geeksforgeeks.org/introduction-of-b-tree-2/ 包含B-Tree的查询，新增，删除操作如何实现 MySQL 执行计划SQL优化中查看执行计划是必不可少的一项，通过 explain 关键字可以查看MySQL中的执行计划 注：\G 含义是纵向显示结果 如果之前没有了解的 EXPLAIN 的同学，看到这个列表肯定是一脸懵逼。没关系我们先来挑几个重要的属性认识一下。 type：ALL 代表全表扫描 key：代表使用的索引，NULL 代表没有使用索引 rows：扫描行数 关于explain 再扩展一下，先执行 explain extended ...; ，再执行 SHOW WARNINGS 可以看到MySQL优化器对我们的SQL做了什么优化。如下图所示 利用索引来优化SQL 使用索引的优点：减少服务器扫描的数据量、避免排序和临时表、将随机I/O变为顺序I/O 通过下图，我们可以看到，添加了索引之后扫描行数从三十万行降到了1，性能提升可想而知 生产环境要注意，创建索引是一个非常耗时的操作，并且会阻塞其他操作。 生产环境添加索引有没有什么完美方案？有的，如果你的MySQL使用主从策略的时候，可以像Nginx不停机升级web服务那样，先移除一个节点为该节点执行 ALTER TABLE 操作，然后巴拉巴拉，因为具体我也没操作过就不细说了，感兴趣大家可以Google一下，动手尝试一下。如果是单机部署的话，只能用户少的时候在执行这种操作了 使用索引连接表索引也可以提高表连接的性能，下面是个例子，用户表左连订单表，对user_id 添加索引的前后对比 like优化 通过上述例子，我们可以看出，如果模糊查询时以%开头的话，MySQL无法使用索引，但是通常来说模糊查询时我们的匹配方式都会是 %xxx%，那么如何优化呢？ 这里可以通过存“反值”的方式巧妙的解决这个问题，例如我现在在数据库加一列 reverse_order_no 存储订单号的反值（并添加索引），匹配的时候再通过 REVERSE(‘%910’) 函数将参数取反。 这里也可以使用 or，如下图，查看执行计划会发现Extra 属性返回 &quot;Using sort_union(order_no,reverse_order_no); Using where&quot; 这里代表MySQL发生了索引合并，后文我们会讲到 排序以及多列索引排序需要加索引！相信大家可能知道这个道理，但是如下图所示，user_id 和 addtime 两列都建立了索引，那么下面这条查询排序使用索引了吗？ 答案是：并没有！为什么？注意 Extra 中的 using filesort，代表MySQL 使用了内部文件排序算法对结果集进行了排序。MySQL 通常在一个表上只选择一个索引（有例外的情况），这种情况如果我们希望排序使用索引的话，可以建立一个多列索引，如下图所示 而且多列索引最左边的列，可以当作单列索引来使用 MySQL 优化器特性我们刚刚说过 MySQL 通常在一个表上只选择一个索引，如何理解？例如索引A和索引B 一个需要扫描十万行，一个需要扫描五万行，那么MySQL一定选择开销最小的索引方式。 在一些特殊情况下，MySQL 会选择 Index Merge（索引合并），即在一个表上使用多个索引 Union：两个基数很高的索引执行OR操作时 Sort-Union：与上述类似，一旦or的左右两边出现范围查询，会使用该算法，区别是Sort-Union会进行排序 intersect：针对唯一值不多的索引列，例如在 is_pay（0-未支付，1-支付），is_send（0-未发货，1-发货） 两列建立索引，查询已支付并且未发货的订单，如下图所示 根据MySQL 5.7开发文档所示，还有一种会使用intersect，InnoDB 主键上的任何范围搜索 关于Index Merge的更多信息，参考MySQL开发文档https://dev.mysql.com/doc/refman/5.7/en/index-merge-optimization.html 索引的影响添加索引虽然可以提升我们的SQL性能，但是随之而来也会带来一定的开销 数据插入和更新的性能，因为需要构建索引的原因，在数据量大的时候会比较明显，下图是 《Effective MySQL之SQL语句最优化》中对添加索引前后的插入性能对比 磁盘空间的影响，同样也是来自于书中的测试 可以看到在添加了索引之后，空间占用是原来的7倍，在数据量庞大时，这是一个需要关注的点。 还有需要注意的一点是，在MySQL Innodb 中有聚簇索引和二级索引，一般来说主键就是聚簇索引，而其他的索引都是二级索引。 二级索引所存储的值是聚簇索引。所以当使用二级索引来进行检索时，MySQL 会先通过该索引找到对应的聚簇索引，再通过该聚簇索引找到对应的数据。这时使用占用字节更小的类型来做主键会更好，会节省索引占用空间 参考 Effective MySQL之SQL语句最优化]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>索引</tag>
        <tag>sql优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security实战(二) 动态权限]]></title>
    <url>%2Fpost%2Fb718a659.html</url>
    <content type="text"><![CDATA[好消息好消息！Security系列终于有了第二期，最近在看项目源码忍不住又搞起来Spring Security，来给大家分享一下，虽然和上一节说好的内容不同🤭 回顾上节我们介绍了如何进行简单的权限配置，包括url权限和方法权限，还有如何授予用户权限。12345678910111213141516171819protected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers("/", "/home").permitAll() // 测试配置URL权限 .antMatchers("/match/**").hasAuthority("sys:match") // 对某URL添加多个权限，可以多次配置 .antMatchers("/match/**").hasAuthority("sys:mm") .anyRequest().authenticated() .and() .formLogin() .loginPage("/login") .permitAll() .and() .logout() .permitAll() .and() ;&#125; 但是如果现在你的业务系统要求动态权限呢？ 比如用户权限变更了，我们可以重新构建Security上下文中Authentication 对象，这还好说。如果说某个接口的权限修改了，如果按照上述的方法来做的话，是不可能实现动态修改的。 本节我们来介绍一下Spring Security 如何实现动态权限 实现原理FilterSecurityInterceptor 负责 Security中的权限控制，其核心代码在父类AbstractSecurityInterceptor中，我们来看一下 这里我删了一些与核心逻辑无关的代码，我们只需要关注红框里的内容 这时候聪明的你应该已经明白了FilterSecurityInterceptor是如何管理权限的，我们完全可以自己实现上面的AccessDecisionManager和SecurityMetadataSource来实现我们的动态权限 但是先别急，先看看AccessDecisionManager的默认实现AffirmativeBased 这代码写的有意思了，通过遍历所有的Voter，每个Voter实现具体的判断逻辑，返回 1，0，-1（分别代表同意、弃权、拒绝），当存在拒绝时直接抛出AccessDeniedException 非常的民主，我们只需要实现一个Voter即可 注：AccessDecisionManager 还有其他的默认实现，感兴趣的同学可以自行查看源码 CodingOK，首先我们先捋一下思路 实现SecurityMetadataSource，提供当前资源要求的权限 实现AccessDecisionVoter，用于判断当前用户是否有权限访问 将我们自己的实现注册到FilterSecurityInterceptor中 OK，可以开搞了 SecurityService实现一个Service，用于从数据库加载数据1234567891011121314151617181920212223242526@Servicepublic class SecurityService &#123; @Autowired private JdbcTemplate jdbcTemplate; /** * 加载资源要求的权限 * * @param resource * @return */ public List&lt;String&gt; getPermByResource(String resource) &#123; return jdbcTemplate.queryForList(Sql.getPermByResource, String.class, resource); &#125; /** * 当前用户的权限 * * @param username * @return */ public List&lt;String&gt; getPermByUsername(String username) &#123; return jdbcTemplate.queryForList(Sql.getPermByUsername, String.class, username); &#125;&#125; 注：这里两个方法都可以加上缓存，由于demo演示，我就没有这么做 实现SecurityMetadataSourceMySecurityMetadataSource 很简单，就是通过SecurityService加载一下数据12345678910111213141516171819202122232425262728public class MySecurityMetadataSource implements FilterInvocationSecurityMetadataSource &#123; private SecurityService securityService; public MySecurityMetadataSource(SecurityService securityService) &#123; this.securityService = securityService; &#125; @Override public Collection&lt;ConfigAttribute&gt; getAttributes(Object object) throws IllegalArgumentException &#123; String uri = ((FilterInvocation) object).getHttpRequest().getRequestURI(); List&lt;String&gt; list = securityService.getPermByResource(uri); if (list != null &amp;&amp; list.size() != 0) &#123; return SecurityConfig.createList(list.toArray(new String[0])); &#125; return null; &#125; @Override public Collection&lt;ConfigAttribute&gt; getAllConfigAttributes() &#123; return null; &#125; @Override public boolean supports(Class&lt;?&gt; clazz) &#123; return FilterInvocation.class.isAssignableFrom(clazz); &#125;&#125; 实现AccessDecisionVoter这里加载一下当前用户的权限，判断用户是否满足当前资源所要求的权限12345678910111213141516171819202122232425262728293031323334public class MyAccessDecisionVoter implements AccessDecisionVoter&lt;Object&gt; &#123; private SecurityService securityService; public MyAccessDecisionVoter(SecurityService securityService) &#123; this.securityService = securityService; &#125; @Override public boolean supports(ConfigAttribute attribute) &#123; return true; &#125; @Override public boolean supports(Class&lt;?&gt; clazz) &#123; return true; &#125; @Override public int vote(Authentication authentication, Object object, Collection&lt;ConfigAttribute&gt; attributes) &#123; Object principal = authentication.getPrincipal(); if ("anonymousUser".equals(principal)) &#123; // 当前用户未登录，如果不要求权限-&gt;允许访问，否则拒绝访问 return CollectionUtils.isEmpty(attributes) ? ACCESS_GRANTED : ACCESS_DENIED; &#125; else &#123; // 这里我的逻辑是，当前资源的要求权限，用户必须全部满足时才可以访问 User user = (User) principal; List&lt;String&gt; permitList = securityService.getPermByUsername(user.getUsername()); List&lt;String&gt; stringAttributes = attributes.stream().map(ConfigAttribute::getAttribute).collect(Collectors.toList()); return permitList.containsAll(stringAttributes) ? ACCESS_GRANTED : ACCESS_DENIED; &#125; &#125;&#125; 注册到FilterSecurityInterceptor我们核心的业务已经实现完了，现在需要把MySecurityMetadataSource和MyAccessDecisionVoter注册到FilterSecurityInterceptor中 需要注意的是，FilterSecurityInterceptor并不可以通过@Bean的方式来声明，该对象是在WebSecurityConfigurerAdapter的初始化方法中默认创建的 但是Spring Security为我们提供了ObjectPostProcessor，用于解决上述问题，具体用法如下12345678910111213http .authorizeRequests() .antMatchers("/", "/home", "/403").permitAll() .anyRequest().authenticated() .withObjectPostProcessor(new ObjectPostProcessor&lt;FilterSecurityInterceptor&gt;() &#123; @Override public &lt;O extends FilterSecurityInterceptor&gt; O postProcess( O fsi) &#123; fsi.setSecurityMetadataSource(new MySecurityMetadataSource(securityService)); fsi.setAccessDecisionManager(new AffirmativeBased(getDecisionVoters())); return fsi; &#125; &#125;) 完整DemoGithub 👉 https://github.com/TavenYin/security-example/tree/master/dynamic-permissions]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Spring Cloud 分布式配置中心]]></title>
    <url>%2Fpost%2Fbca00fad.html</url>
    <content type="text"><![CDATA[Spring Cloud Config 能做什么？我们可以将分布式系统的配置文件托管在Git仓库或者数据库中，Config Server 负责管理配置文件，以Restful的形式提供给其他服务，可以在任何其他语言的应用程序中使用，不依赖Spring Cloud全家桶。 本节目标使用Spring Cloud 基于Git仓库搭建分布式配置中心 版本Spring Cloud Greenwich.SR2Spring Boot 2.1.7.RELEASE Git仓库在你喜欢的Git平台上建立一个仓库，创建一个目录，并建立几个配置文件（建议网络较慢的同学选择Gitee） 搭建Config Serverpom.xml1234567891011121314151617181920212223&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; ...&lt;/dependencies&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Greenwich.SR2&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; application.properties12345678910server.port = 9001spring.application.name = config-server#表示配置中心所在仓库的位置spring.cloud.config.server.git.uri = https://gitee.com/yintianwen7/cloud-config.git#仓库路径下的的相对搜索位置，可以配置多个spring.cloud.config.server.git.search-paths = demo#git的用户名spring.cloud.config.server.git.username = yourusername#git的密码spring.cloud.config.server.git.password = yourpassword ConfigServerApplication.java123456789@EnableConfigServer@SpringBootApplicationpublic class ConfigServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigServerApplication.class, args); &#125;&#125; 至此，ConfigServer 搭建完成，我们可以访问ConfigServer 查看 cloud-config/demo 下的配置文件 访问方式如下: /{application}/{profile}[/{label}] /{application}-{profile}.yml /{label}/{application}-{profile}.yml /{application}-{profile}.properties /{label}/{application}-{profile}.properties 这里 label 为git 分支，默认master 例如访问 application-dev.yml，直接请求 localhost:9001/application/dev即可 可以尝试修改一下Git仓库中的配置文件，再访问Config Server，这时你会发现Config Server中的数据是实时的。 搭建 Client 应用访问Config Serverpom.xml1234567891011121314151617181920212223&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; ...&lt;/dependencies&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Greenwich.SR2&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; bootstrap.properties，注意这里是 bootstrap.properties，不然你会发现你请求的URI一直是localhost:888812345spring.profiles.active=simplespring.application.name=applicationspring.cloud.config.label=master# 拼接规则: uri/name/profile/labelspring.cloud.config.uri=http://localhost:9001 CloudConfig.java 配置文件的映射实体，同理也可以@Value 或者 Environment 对象读取属性12345678@ConfigurationProperties("cloud.config")public class CloudConfig &#123; private String a; private String b; private String c; // 省略 get set&#125; ClientApplication.java123456789101112131415161718@RestController@SpringBootApplication@EnableConfigurationProperties(CloudConfig.class)public class ClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ClientApplication.class, args); &#125; @Autowired private CloudConfig cloudConfig; // 刷新配置时 POST /actuator/refresh @GetMapping("config") public Object config() &#123; return cloudConfig; &#125;&#125; 启动 Client，查看日志，可以看到请求配置中心的URL 请求 localhost:8080/config ，验证配置文件是否读取成功 刷新客户端配置Client 中读取的配置文件，并不是实时的，我们可以通过修改Git仓库中的文件来验证这点。那么如何刷新配置呢？ pom.xml 引入1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; bootstrap.properties 追加1management.endpoints.web.exposure.include=* 对需要刷新的Bean，添加注解@RefreshScope1234@RefreshScope@ConfigurationProperties("cloud.config")public class CloudConfig &#123;&#125; POST /actuator/refresh，即可刷新配置 Config Server 加密解密允许数据以加密形式存储在Git仓库，配置中心负责对加密的数据进行解密，然后提供给客户端应用。由于篇幅问题，这里不讲了，感兴趣的小伙伴参考 Spring Cloud构建微服务架构：分布式配置中心（加密解密） 配置中心高可用方案1：使用传统负载均衡器 方案2：将client、config server 注册到Spring Cloud注册中心，通过注册中心访问配置中心，具体代码参考👇 Demo本文demo👉：Github， Gitee 参考http://blog.didispace.com/spring-cloud-starter-dalston-3-2/https://segmentfault.com/a/1190000012908853]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
        <tag>分布式配置中心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ 集群搭建]]></title>
    <url>%2Fpost%2F4c62e394.html</url>
    <content type="text"><![CDATA[本文记录RabbitMQ集群搭建过程中遇到的问题 环境 vmware12 虚拟机，CentOS7本文以两台CentOS 为例，IP 192.168.32.128，192.168.32.129 磁盘节点和内存节点集群中，每一个RabbitMQ实例都是一个节点，而节点分为磁盘节点和内存节点 内存节点：将Rabbit中的元数据（Queue, Exchange, binding, vhost等）存储在内存中，持久化的 Message 依旧保存在磁盘中，内存节点的性能只能体现在资源管理上，消息的发送和接收和磁盘节点没有区别 磁盘节点：元数据存储在磁盘中，一个Rabbit集群要求至少有一个磁盘节点，因为内存节点中不存储元数据，所以每次内存节点启动，都会从其他节点中同步元数据 另外如果唯一磁盘的磁盘节点崩溃了，不能进行如下操作： 不能创建队列 不能创建交换器 不能创建绑定 不能添加用户 不能更改权限 不能添加和删除集群几点 RabbitMQ集群的几种类型 单一模式：仅有一个rabbit实例 普通模式：默认集群模式，每个节点各自维护自己的数据，两个节点仅存有相同的元数据。例如RabbitA 和 RabbitB，A中存在 QueueA，消费者可以从RabbitB实例中，读取QueueA的消息，这时RabbitB会从A中读取消息，返回给消费者。但是如果RabbitA 宕机，这时就无法获取QueueA的数据了 镜像模式：Rabbit 会将数据同步到其他节点中，这固然提高了可用性，但是随之而来的问题是，系统的性能会降低。节点之间消息的传递会占用带宽，而每个节点存储的数据量会变大 普通模式我们先来搭建普通模式的集群，在RabbitMQ中，每个节点的名必须是唯一的，默认以 rabbit@hostname为节点name 而每台虚拟机中默认的host是localhost，这就导致了每个节点的name都是 rabbit@localhost 1. 修改hosts修改两台主机的hosts文件，以下是129的配置，我们把128定义为F，129定义为G 123456cat /etc/hosts127.0.0.1 G::1 G192.168.32.129 G192.168.32.128 F 保证F和G可以ping通 2. 安装RabbitMQ可以参考 https://www.linuxprobe.com/install-rabbitmq-on-centos-7.html 当时安装完一直发现无法访问 Rabbit的web控制台，除了开启插件，和添加一个用户之外，还需要放开防火墙的端口12345678firewall-cmd --add-port=4369/tcp --permanentfirewall-cmd --add-port=25672/tcp --permanentfirewall-cmd --add-port=5671-5672/tcp --permanentfirewall-cmd --add-port=15672/tcp --permanentfirewall-cmd --add-port=61613-61614/tcp --permanentfirewall-cmd --add-port=1883/tcp --permanentfirewall-cmd --add-port=8883/tcp --permanentfirewall-cmd --reload # 重载配置 3. 同步Erlang cookieErlang VM将尝试在RabbitMQ服务器启动时创建一个随机生成的值（Erlang Cookie）。集群环境中，所有节点的Erlang Cookie必须一致。 .erlang.cookie 通常在 $HOME下或者/var/lib/rabbitmq下1234567# 找到其中一台主机的 .erlang.cookie# 修改权限chmod 777 /var/lib/rabbitmq/.erlang.cookie# 拷贝到另一台主机scp /var/lib/rabbitmq/.erlang.cookie G:/var/lib/rabbitmq/# 再把权限修改回来chmod 400 /var/lib/rabbitmq/.erlang.cookie 4. 检查节点名 如果两个节点都是 rabbit@localhost，这是无法建立集群的 我当时就是遇到了这个问题，需要修改节点name，参考 https://ubuntuqa.com/article/6619.html 我采取的方法：1234567vi /etc/rabbitmq/rabbitmq-env.conf# 添加如下配置NODENAME=rabbit@G# 保存后，重启rabbitmq 生效service rabbitmq-server restart 5. 组成集群上述步骤都成功后，我们可以组成集群了 这个时候我们先启动rabbitmq@F，然后rabbit@G执行以下操作，加入F，组成集群1234rabbitmqctl stop_app # 停止rabbitmq服务rabbitmqctl reset # 清空节点状态rabbitmqctl join_cluster rabbit@F # 加入F，组成集群rabbitmqctl start_app 集群中，任意节点停机后，执行 rabbitmqctl start_app 即可再次加入集群 查看集群状态 rabbitmqctl cluster_status 镜像模式任意rabbit节点输入命令，将所有队列，同步到所有节点中1rabbitmqctl set_policy ha-all "^" '&#123;"ha-mode":"all"&#125;' 12345678910111213rabbitmqctl set_policy [-p Vhost] Name Pattern Definition [Priority]-p Vhost： 可选参数，针对指定vhost下的queue进行设置Name: policy的名称Pattern: queue的匹配模式(正则表达式)Definition：镜像定义，包括三个部分ha-mode, ha-params, ha-sync-mode ha-mode:指明镜像队列的模式，有效值为 all/exactly/nodes all：表示在集群中所有的节点上进行镜像 exactly：表示在指定个数的节点上进行镜像，节点的个数由ha-params指定 nodes：表示在指定的节点上进行镜像，节点名称通过ha-params指定 ha-params：ha-mode模式需要用到的参数 ha-sync-mode：进行队列中消息的同步方式，有效值为automatic和manualpriority：可选参数，policy的优先级 关于镜像模式策略的更多请参考官方文档：https://www.rabbitmq.com/ha.html 集群的负载均衡为什么有了集群还需要负载均衡？ 这里我纠结了好几天，原因是，在我的理解里 集群 == 负载均衡，这样的理解是有问题的。正解：分布式集群保证的是高可用，而并不是负载均衡。 rabbitmq文档中，也建议我们使用TCP负载均衡器，这样也不需要在我们应用程序里管理集群的地址 Connecting to Clusters from ClientsA client can connect as normal to any node within a cluster. If that node should fail, and the rest of the cluster survives, then the client should notice the closed connection, and should be able to reconnect to some surviving member of the cluster. Generally, it’s not advisable to bake in node hostnames or IP addresses into client applications: this introduces inflexibility and will require client applications to be edited, recompiled and redeployed should the configuration of the cluster change or the number of nodes in the cluster change. Instead, we recommend a more abstracted approach: this could be a dynamic DNS service which has a very short TTL configuration, or a plain TCP load balancer, or some sort of mobile IP achieved with pacemaker or similar technologies. In general, this aspect of managing the connection to nodes within a cluster is beyond the scope of RabbitMQ itself, and we recommend the use of other technologies designed specifically to solve these problems. 网上比较多的方案是使用 HAProxy 作为负载均衡器，这里大家可以参考一下这一篇 HAProxy从零开始到掌握 具体安装配置的细节本文就不说了 这里我又添加了一台 130 的虚拟机来跑 HAProxy 贴一下我的 haproxy.cfg，仅供参考 1234567891011121314151617181920212223242526272829globaldaemonpidfile /home/ha/haproxy/conf/haproxy.pidlog 127.0.0.1 local2defaultsmode tcpmaxconn 10000timeout connect 5stimeout client 100stimeout server 100sfrontend http-in bind *:5670 maxconn 30000 default_backend default_serversbackend default_servers balance roundrobin server F 192.168.32.128:5672 check inter 2000 rise 2 fall 3 weight 1 server G 192.168.32.129:5672 check inter 2000 rise 2 fall 3 weight 1listen stats bind *:1936 mode http stats refresh 30s #每30秒更新监控数据 stats uri /stats #访问监控页面的uri stats realm HAProxy\ Stats #监控页面的认证提示 stats auth admin:admin 做完负载均衡之后，可以跑一下程序，查看一下负载均衡的效果，这里我的10个 Connection 按照权重 1:1 分配在了两台 Rabbit 上 参考 https://www.cnblogs.com/knowledgesea/p/6535766.htmlhttps://www.rabbitmq.com/clustering.html]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
        <tag>集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ 理解 Exchange]]></title>
    <url>%2Fpost%2Fea1358e6.html</url>
    <content type="text"><![CDATA[本节目标 了解 AMQP 模型 了解 Exchange 建议大家在学习MQ之前，先了解一下 生产者消费者模型，可以参考我之前的一篇 Java 理解生产者-消费者设计模式 AMQP 简介RabbitMQ 是实现了 AMQP 协议的消息中间件，所以说下面讲到这些概念不仅限于RabbitMQ，所有实现AMQP 协议的消息中间件都具备这些 AMQP 模型图如下 Publisher（发布者）和 Consumer（消费者）可以理解为是我们的应用程序 Exchange（交换机），我们的Publisher 会将消息先推送到Exchange ，Exchange 类似邮局，他会将消息再路由到 Queue（队列）中，Queue会将消息推送给Consumer Exchange 和 Queue 之间通过 Binding （绑定）将两者关联到一起，一个Exchange 可以绑定多个Queue，一个Queue 可以绑定多个Exchange Exchange刚刚我们说Exchange类似邮局。邮局的话，当然会有很多的邮递方式，所以我们的Exchange有多种类型，每种类型都有自己的策略 AMQP 提供四种Exchange类型 name 默认预设名 Direct exchange (Empty string) and amq.direct Fanout exchange amq.fanout Topic exchange amq.topic Headers exchange amq.match (and amq.headers in RabbitMQ) Direct ExchangeRabbit 文档中给出的定义翻译过来是这样的 Direct Exchange基于RoutingKey 将消息传递到队列 通俗点讲是啥意思呢，直接看代码吧 12// 第一个参数为Exchange，第二个参数就是RoutingKey rabbitTemplate.convertAndSend("user_exchange", "user_routingKey", message); 我们再来看绑定的时候，也会关联一个RoutingKey12// 绑定的时候 也会关联一个RoutingKeyBindingBuilder.bind(queue()).to(userExchange()).with("user_routingKey") 千言万语都在图里 Default ExchangeDefault Exchange 其实是AMQP中预先声明的，并且它属于 Direct Exchange，通常来说每个Exchange都会有自己的名，Default Exchange 的名是 &quot;&quot;. 他有一个特殊的属性，当你手动创建一个队列时，MQ会自动将这个队列绑定到Default Exchange 上，绑定时 RoutingKey 与队列名称相同 说的好像挺绕的，来看一下代码吧12345// 声明一个 Queue，这个时候我们不主动Binding的话，test_queue 会和Default Exchange 绑定，此时 routingKey = test_queueQueue queue = new Queue("test_queue");// 向默认交换机发布消息rabbitTemplate.convertAndSend("", "test_queue", message); Fanout ExchangeFanout Exchange 很简单，适合发布订阅场景。他不使用RoutingKey，他将消息路由到所有与其绑定的队列 Topic ExchangeTopic Exchange 又是基于RoutingKey来路由转发的，但是有些不同，上图 在使用 Topic 的时候，routingKey 可以是 *.orange.*,lazy.# 这种表达式 * (星) 代表一个词. # (hash) 代表0或多个词. 同理，当发布消息时的 routingKey 与其匹配时，会路由到相应的队列 Headers Exchange基于Header中的属性来匹配路由，由于不是很常用，这里就不说了 参考 https://www.rabbitmq.com/tutorials/amqp-concepts.html]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
        <tag>Exchange</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 源码解析 同步事务的实现原理]]></title>
    <url>%2Fpost%2Fff158ca0.html</url>
    <content type="text"><![CDATA[在最开始学习Spring AOP的时候，那时候只知道事务是利用AOP管理的，但是并没有翻看过源码，后来发现Spring不光可以管理关系型数据的事务。甚至可以同步管理Redis等其他数据库的事务，这是怎么做到的呢？本节我们一起探索一下Spring的事务 版本SpringBoot 2.1.6.RELEASE 分析首先如果我们自己写一个AOP来管理事务的话，会怎么做？ 执行代理方法前开启事务 执行代理的方法 执行代理方法后回滚或者提交事务 如何实现Redis事务的同步提交和回滚？ Spring中关于事务的管理要比我们想象的复杂的多，本文只关注同步事务的实现，下面我们来看源码 TransactionAspectSupport在翻看源码后找到这个类，事务切面支持，这个类实现了事务管理的核心逻辑，我们来看一下核心代码 我们通常在项目中会使用默认的DataSourceTransactionManager事务管理器 + 注解式事务（也就是声明式事务）CallbackPreferringPlatformTransactionManager 是Spring提供的回调式事务管理器（用于编程式事务），这里我们不讨论。 可以看到声明式事务的处理逻辑，和我们上述分析的基本一致，那么他是在什么时候同步处理了其他数据的事务呢？继续深入源码 源码跟踪我们以completeTransactionAfterThrowing() 这个方法为例（上图红框框），跟踪下去 可以看到，当我们异常需要回滚时，会调用 TransactionManager.rollback()，我们继续来跟踪这个方法 这里代码逻辑还是比较多的，我们只关注红框里的代码块，doRollback中只做了数据源的回滚，那么像Redis事务的提交是在哪里实现的？ 我们来看一下 triggerAfterCompletion() 方法，答案马上就知道了 同步事务 TransactionSynchronizationManager 中会从ThreadLocal 中获取出当前线程中 “同步事务”接口集合 List，然后接下来的操作就是遍历所有的TransactionSynchronization，执行synchronization.afterCompletion(completionStatus); 我们可以在TransactionSynchronization接口下找到一个Redis的实现 代码逻辑很简单，事务提交则Redis事务执行，否则取消Redis事务 同步事务的注册 我们RedisTemplate开启事务后，会执行到上图的debug处，RedisConnection 开启事务，并将当前Connection注册到TransactionSynchronizationManager 中，这样在triggerAfterCompletion 就可以同步的管理我们Redis的事务了]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>Spring</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解消息队列 - 使用场景简介]]></title>
    <url>%2Fpost%2F81e2e5c7.html</url>
    <content type="text"><![CDATA[在之前的 Java 理解生产者-消费者设计模式 一文中，我们学习了生产者-消费者模式，生产者和消费者之间通过一个缓冲队列进行通讯，即做到了异步消息吞吐，又使得程序解耦 可以说，使用 MQ（消息队列） 可以在分布式环境中实现生产者-消费者模式 适合MQ使用的场景 消息的发送者和消费者需要解耦的情况 异步处理 多个消费者（消息的关注者不止一个） 消费者的处理结果不返回给发送者 （以RabbitMQ为例，虽然可以做到RPC调用，但是这种调用会带来更多的麻烦） 削峰填谷 下面我跟详细分析一下各种使用场景 解耦 其实很好理解，因为生产者和消费者通过MQ进行通讯，可以说两者没有什么直接的瓜葛，必然解耦 异步 &amp;&amp; 多个消费者生产者只负责把消息发布，并在意谁处理消息，可能会有多个消费者处理该消息 例如用户注册，我们会给用户发送注册邮件和短信（可以把邮件和短信理解为多个消费者服务） 可以看到在使用了MQ，即可以通过异步处理的方式，降低响应时间，又可以降低业务之间的耦合度 消费者的处理结果不返回给发送者线程池和MQ都可以看成 生产者-消费者模式的实现，而这两者也都可以获得消费者的返回值。但是如果你这样做的话，你的生产者线程会阻塞（等待消费者的返回），没法做到完全异步的处理，有些违背了生产者-消费者模式的初衷。而且还要考虑等待超时后的处理。但是不能说这种用法是不对的，具体业务具体分析 削峰填谷何为削峰填谷，可以理解为流量控制。例如在抢票，秒杀等业务场景时，可能会突然有大量的请求涌入。但是我们的服务器资源有限，并不能处理这么多的请求，此时可以使用 MQ进行流量控制 因为，请求都缓存在了MQ中，下游服务可以根据自己的速度进行处理。 参考 https://www.kancloud.cn/cosmicyang/rabbitmq/824050]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>消息队列</tag>
        <tag>生产者消费者</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 理解 ThreadPoolExecutor 实现原理]]></title>
    <url>%2Fpost%2Fa5233273.html</url>
    <content type="text"><![CDATA[使用线程池（ThreadPoolExecutor）的好处是减少在创建和销毁线程上所花的时间以及系统资源的开销，解决资源不足的问题。如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题。 – 阿里Java开发手册 版本JDK 1.8 本节目标 理解线程池核心参数 理解线程池工作原理 理解线程池核心方法 线程池的核心参数和构造方法ctl12345678910111213141516171819202122232425262728293031323334353637// 线程池核心变量，包含线程池的运行状态和有效线程数，利用二进制的位掩码实现private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bits// 线程池状态private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// Packing and unpacking ctl// 获取当前线程池运行状态private static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;// 获取当前线程池有效线程数private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;// 打包ctl变量private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125;/* * Bit field accessors that don't require unpacking ctl. * These depend on the bit layout and on workerCount being never negative. */private static boolean runStateLessThan(int c, int s) &#123; return c &lt; s;&#125;private static boolean runStateAtLeast(int c, int s) &#123; return c &gt;= s;&#125;private static boolean isRunning(int c) &#123; return c &lt; SHUTDOWN;&#125; JDK7 以后，线程池的状态和有效线程数通过 ctl 这个变量表示（使用二进制的位掩码来实现，这里我们不深究），理解上述几个方法作用即可，不影响下面的源码阅读 关于线程池的五种状态 RUNNING：接受新任务并处理队列中的任务 SHUTDOWN ：不接受新任务，但处理队列中的任务 STOP ：不接受新任务，不处理队列中的任务，并中断正在进行的任务（中断并不是强制的，只是修改了Thread的状态，是否中断取决于Runnable 的实现逻辑） TIDYING ：所有任务都已终止，workerCount为0时，线程池会过度到该状态，并即将调用 terminate() TERMINATED ：terminated() 调用完成；线程池中止 线程池状态的转换 RUNNING =&gt; SHUTDOWN ：调用 shutdown() RUNNING / SHUTDOWN =&gt; STOP ：调用 shutdownNow() （该方法会返回队列中未执行的任务） SHUTDOWN =&gt; TIDYING： 当线程池和队列都为空时 STOP =&gt; TIDYING：当线程池为空时 TIDYING =&gt; TERMINATED：当 terminated() 调用完成时 构造方法线程池最终都是调用如下构造方法123456789public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; // 省略&#125; 核心参数我们来看一下线程池中的核心参数都是什么作用123456789101112131415161718192021222324private final BlockingQueue&lt;Runnable&gt; workQueue; // 阻塞队列，用于缓存任务private final ReentrantLock mainLock = new ReentrantLock(); // 线程池主锁private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;(); // 工作线程集合private final Condition termination = mainLock.newCondition(); // awaitTermination() 方法的等待条件private int largestPoolSize; // 记录最大线程池大小private long completedTaskCount; //用来记录线程池中曾经出现过的最大线程数private volatile ThreadFactory threadFactory; // 线程工厂，用于创建线程private volatile RejectedExecutionHandler handler; // 任务拒绝时的策略private volatile long keepAliveTime; // 线程存活时间 // 当线程数超过核心池数时，或允许核心池线程超时，该参数会起作用。否则一直会等待新的任务private volatile boolean allowCoreThreadTimeOut; // 是否允许核心池线程超时private volatile int corePoolSize; // 核心线程池数量private volatile int maximumPoolSize; // 最大线程池数量 workQueue这个队列的作用，和之前的 Java 理解生产者-消费者设计模式 中讲到的缓冲队列，作用很相似，或者说线程池就是生产者消费者模式的一种实现。 关于 handler ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy：当前任务自己决定 corePoolSize 和 maximumPoolSize如果你对这两个参数有疑问，看完下面的栗子你会清晰很多 下面我们来举个栗子来更好的理解一下线程池 理解线程池工作原理假如有一个工厂，工厂里面有10个工人，每个工人同时只能做一件任务。 因此只要当10个工人中有工人是空闲的，来了任务就分配给空闲的工人做； 当10个工人都有任务在做时，如果还来了任务，就把任务进行排队等待； 每个工人做完自己的任务后，会去任务队列中领取新的任务； 如果说新任务数目增长的速度远远大于工人做任务的速度（任务累积过多时），那么此时工厂主管可能会想补救措施，比如重新招4个临时工人进来； 然后就将任务也分配给这4个临时工人做； 如果说着14个工人做任务的速度还是不够，此时工厂主管可能就要考虑不再接收新的任务或者抛弃前面的一些任务了。 当这14个工人当中有人空闲时，而新任务增长的速度又比较缓慢，工厂主管可能就考虑辞掉4个临时工了，只保持原来的10个工人，毕竟请额外的工人是要花钱的。 开始工厂的10个工人，就是 corePoolSize (核心池数量)； 当10个人都在工作时 (核心池达到 corePoolSize)，任务排队等待时，会缓存到 workQueue 中； 当任务累积过多时(达到 workQueue 最大值时)，找临时工； 14个临时工，就是 maximumPoolSize (数量)； 如果此时工作速度还是不够，线程池这时会考虑拒绝任务，具体由拒绝策略决定 理解线程池核心方法execute()线程池中所有执行任务的方法有关的方法，都会调用 execute()。如果你理解了上述的小例子，再来看这个会清晰很多123456789101112131415161718192021222324252627282930313233343536373839public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); /* * Proceed in 3 steps: * * 1. If fewer than corePoolSize threads are running, try to * start a new thread with the given command as its first * task. The call to addWorker atomically checks runState and * workerCount, and so prevents false alarms that would add * threads when it shouldn't, by returning false. * * 2. If a task can be successfully queued, then we still need * to double-check whether we should have added a thread * (because existing ones died since last checking) or that * the pool shut down since entry into this method. So we * recheck state and if necessary roll back the enqueuing if * stopped, or start a new thread if there are none. * * 3. If we cannot queue task, then we try to add a new * thread. If it fails, we know we are shut down or saturated * and so reject the task. */ int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; else if (!addWorker(command, false)) reject(command);&#125; 分析execute() step 1 1）首先检查当前有效线程数 是否小于 核心池数量if (workerCountOf(c) &lt; corePoolSize) 2）如果满足上述条件，则尝试向核心池添加一个工作线程 （addWorker() 第二个参数决定了是添加核心池，还是最大池）if (addWorker(command, true)) 3）如果成功则退出方法，否则将执行 step2 step 2 1）如果当前线程池处于运行状态 &amp;&amp; 尝试向缓冲队列添加任务if (isRunning(c) &amp;&amp; workQueue.offer(command)) 2）如果线程池正在运行并且缓冲队列添加任务成功，进行 double check（再次检查） 3）如果此时线程池非运行状态 =&gt; 移除队列 =&gt; 拒绝当前任务，退出方法（这么做是为了，当线程池不可用时及时回滚） 12if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); 4）如果当前有效线程数为0，则创建一个无任务的工作线程（此时这个线程会去队列中获取任务） step 3 1）当无法无法向核心池和队列中添加任务时，线程池会再尝试向最大池中添加一个工作线程，如果失败则拒绝该任务 12else if (!addWorker(command, false)) reject(command); 图解execute()根据上述的步骤画了如下的这个图，希望能帮助大家更好的理解 addWorker()在分析execute() 方法时，我们已经知道了 addWorker() 的作用了，可以向核心池或者最大池添加一个工作线程。我们来看一下这个方法都做了什么 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; 这个方法代码看似很复杂，没关系，我们一步一步来分析 step 1先看第一部分 12345678910111213141516171819202122232425retry:for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125;&#125; 这一部分代码，主要是判断，是否可以添加一个工作线程。 在execute()中已经判断过if (workerCountOf(c) &lt; corePoolSize)了，为什么还要再判断？ 因为在多线程环境中，当上下文切换到这里的时候，可能线程池已经关闭了，或者其他线程提交了任务，导致workerCountOf(c) &gt; corePoolSize 1）首先进入第一个无限for循环，获取ctl对象，获取当前线程的运行状态，然后判断12345if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; 这个判断的意义为，当线程池运行状态 &gt;= SHUTDOWN 时，向添加一个工作线程必须同时满足 rs == SHUTDOWN firstTask == null ! workQueue.isEmpty()三个条件，否则添加线程失败 所以当线程状态为SHUTDOWN时，线程池允许添加一个无任务的工作线程去执行队列中的任务。 2）进入第二个无限for循环 123456789101112for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop&#125; 获取当前有效线程数，if 有效线程数 &gt;= 容量 || 有效线程数 &gt;= 核心池数量/最大池数量，则return false; 添加线程失败 如果有效线程数在合理范围之内，尝试使用 CAS 自增有效线程数 （CAS 是Java中的乐观锁，不了解的小伙伴可以Google一下），乐观锁自增成功，代表当前无其他线程竞争，相当于获取到锁了 如果自增成功，break retry; 跳出这两个循环，执行下面的代码 自增失败，检查线程池状态，如果线程池状态发生变化，回到第一个for 继续执行；否则继续在第二个for 中； step 2下面这部分就比较简单了 1234567891011121314151617181920212223242526272829303132333435363738boolean workerStarted = false;boolean workerAdded = false;Worker w = null;try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125;&#125; finally &#123; if (! workerStarted) addWorkerFailed(w);&#125;return workerStarted; 1）创建工作线程对象Worker； 2）加锁，判断当前线程池状态是否允许启动线程；如果可以，将线程加入workers（这个变量在需要遍历所有工作线程时会用到），记录最大值，启动线程； 3）如果线程启动失败，执行addWorkerFailed（从workers中移除该对象，有效线程数减一，尝试中止线程池） WorkerWorker对象是线程池中的内部类，线程的复用、线程超时都是在这实现的 123456789private final class Worker extends AbstractQueuedSynchronizer implements Runnable&#123; // 这里我们只关心Run()，省略了其他源码，感兴趣的同学可以自己看一下源码 public void run() &#123; runWorker(this); &#125;&#125; Worker 实现了 Runnable，我们这里只关心 Worker 的run方法中做了什么，关于 AbstractQueuedSynchronizer 有关的不在本文讨论 下面我们分析一下runWorker()1234567891011121314151617181920212223242526272829303132333435363738394041424344final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; // 通过该变量判断是用户任务抛出异常结束，还是线程池自然结束 completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125;&#125; 1）123 while (task != null || (task = getTask()) != null) &#123;// ...&#125; 不对地通过getTask() 从队列中获取任务，可以间接通过getTask()的返回值控制线程的结束 2）123456789// If pool is stopping, ensure thread is interrupted;// if not, ensure thread is not interrupted. This// requires a recheck in second case to deal with// shutdownNow race while clearing interruptif ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); 接下来这个判断，其实我是没有太理解的，暂且认为是保证当线程池STOP时，线程一定会被打断 3）执行Runnable12345678910111213141516171819try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125;&#125; finally &#123; task = null; w.completedTasks++; w.unlock();&#125; beforeExecute(wt, task); 和 afterExecute(task, thrown); 默认是没有实现的，我们可以自己扩展 4）最后是当跳出while循环后（getTask() == null或者用户任务抛出异常），会去执行processWorkerExit(w, completedAbruptly);线程退出工作（该方法会根据线程池状态，尝试中止线程池。然后会考虑是结束当前线程，还是再新建一个工作线程，这里就不细说了） 我们再来看一下 getTask() 方法12345678910111213141516171819202122232425262728293031323334353637private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; int wc = workerCountOf(c); // Are workers subject to culling? boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; &#125; catch (InterruptedException retry) &#123; timedOut = false; &#125; &#125;&#125; 1） 第一段不做解释，满足该条件时，return null; 退出线程12345// Check if queue empty only if necessary.if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null;&#125; 2） 下面这段很有意思12345678910111213141516int wc = workerCountOf(c);// Are workers subject to culling?// 是否允许线程超时// 当我们设置了允许核心池超时 或者 有效线程数 &gt; 核心池数量的时候// 线程池会考虑为我们清除掉一些线程boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize;// (有效线程数 &gt; 最大线程池数量 || (允许超时 &amp;&amp; 超时) ) // &amp;&amp; (有效线程数 &gt; 1 || 或者队列为空时)if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) // timedOut 表示当前线程超时，下文会说到 &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue;&#125; 我在第一次看这段代码的时候，傻傻的以为 timedOut 不是永远为false吗，我以为JDK源码怎么写出这么个Bug。别忘了当前的getTask()方法也是在一个无限循环里 3）12345678910try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true;&#125; catch (InterruptedException retry) &#123; timedOut = false;&#125; 根据 timed，决定调用使用poll() 或者 take()。 poll 在队列为空时会等待指定时间，如果这期间没有获取到元素，则return null take 则在队列为空时会一直等待，直至队列中被添加新的任务，或者被打断；这两个方法都会被shutdown() 或者 shutdownNow的 thread.interrupt()打断；如果被打断则回到第一步 至此 execute() 方法所涉及的逻辑我们差不多分析完了 备注线程池使用1234567891011121314public class Test &#123; public static void main(String[] args) &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 10, 60, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(5)); executor.execute(() -&gt; &#123; // 业务逻辑 &#125;); executor.shutdown(); &#125;&#125; 合理配置线程池的大小一般需要根据任务的类型来配置线程池大小： 如果是CPU密集型任务，参考值可以设为 N+1 （N 为CPU核心数） 如果是IO密集型任务，参考值可以设置为2*N 当然，这只是一个参考值，具体的设置还需要根据实际情况进行调整，比如可以先将线程池大小设置为参考值，再观察任务运行情况和系统负载、资源利用率来进行适当调整。 参考 JDK1.8 https://www.cnblogs.com/dolphin0520/p/3932921.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>ThreadPoolExecutor</tag>
        <tag>线程池</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 源码解析 @RequestBody @ResponseBody 的来龙去脉]]></title>
    <url>%2Fpost%2Fb75109d6.html</url>
    <content type="text"><![CDATA[@RequestBody 和 @ResponseBody 是实际开发中很常用的两个注解，通常用来解析和响应JSON，用起来十分的方便，这两个注解的背后是如何实现的？ 源码版本SpringBoot 2.1.3.RELEASE RequestResponseBodyMethodProcessor Resolves method arguments annotated with @RequestBody and handles return values from methods annotated with @ResponseBody by reading and writing to the body of the request or response with an HttpMessageConverter.An @RequestBody method argument is also validated if it is annotated with @javax.validation.Valid. In case of validation failure, MethodArgumentNotValidException is raised and results in an HTTP 400 response status code if DefaultHandlerExceptionResolver is configured. 简单来说，这个类用来解析@RequestBody的参数和处理 @ResponseBody返回值，通过 HttpMessageConverter 这个接口来实现。 如果@RequestBody标记的参数包含@Valid，还会对这个参数进行校验。 继承关系 HandlerMethodArgumentResolver 和 HandlerMethodReturnValueHandler 分别是Spring的参数处理器和返回值处理器 HandlerMethodArgumentResolver 12345678public interface HandlerMethodArgumentResolver &#123; boolean supportsParameter(MethodParameter parameter); Object resolveArgument(MethodParameter parameter, @Nullable ModelAndViewContainer mavContainer, NativeWebRequest webRequest, @Nullable WebDataBinderFactory binderFactory) throws Exception;&#125; Spring的参数解析器接口，supportsParameter() 方法用于判断解析器是否支持当前Controller方法的参数，resolveArgument() 则是将Request解析为Controller方法对应的参数Bean HandlerMethodReturnValueHandler 12345678public interface HandlerMethodReturnValueHandler &#123; boolean supportsReturnType(MethodParameter returnType); void handleReturnValue(@Nullable Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception;&#125; 同理这个接口将Controller方法返回的对象，封装为Response 我们在实际开发时，也可以实现这两个接口自定义自己的参数解析和响应处理，RequestResponseBodyMethodProcessor 实现了这两个接口，既做了参数解析器也做了响应处理器。 RequestResponseBodyMethodProcessor 源码分析我们来看一下 RequestResponseBodyMethodProcessor 是如何工作的，以解析参数为例 resolveArgument 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Overridepublic boolean supportsParameter(MethodParameter parameter) &#123; // 支持标记@RequestBody的参数 return parameter.hasParameterAnnotation(RequestBody.class);&#125;@Overridepublic Object resolveArgument(MethodParameter parameter, @Nullable ModelAndViewContainer mavContainer, NativeWebRequest webRequest, @Nullable WebDataBinderFactory binderFactory) throws Exception &#123; parameter = parameter.nestedIfOptional(); // 通过HttpMessageConverters 将请求体, 封装为@RequestBody所标记的XXBean Object arg = readWithMessageConverters(webRequest, parameter, parameter.getNestedGenericParameterType()); String name = Conventions.getVariableNameForParameter(parameter); if (binderFactory != null) &#123; WebDataBinder binder = binderFactory.createBinder(webRequest, arg, name); if (arg != null) &#123; // 如果存在@Valid 对参数进行校验 validateIfApplicable(binder, parameter); if (binder.getBindingResult().hasErrors() &amp;&amp; isBindExceptionRequired(binder, parameter)) &#123; throw new MethodArgumentNotValidException(parameter, binder.getBindingResult()); &#125; &#125; if (mavContainer != null) &#123; mavContainer.addAttribute(BindingResult.MODEL_KEY_PREFIX + name, binder.getBindingResult()); &#125; &#125; return adaptArgumentIfNecessary(arg, parameter);&#125;@Overrideprotected &lt;T&gt; Object readWithMessageConverters(NativeWebRequest webRequest, MethodParameter parameter, Type paramType) throws IOException, HttpMediaTypeNotSupportedException, HttpMessageNotReadableException &#123; HttpServletRequest servletRequest = webRequest.getNativeRequest(HttpServletRequest.class); Assert.state(servletRequest != null, "No HttpServletRequest"); ServletServerHttpRequest inputMessage = new ServletServerHttpRequest(servletRequest); Object arg = readWithMessageConverters(inputMessage, parameter, paramType); if (arg == null &amp;&amp; checkRequired(parameter)) &#123; throw new HttpMessageNotReadableException("Required request body is missing: " + parameter.getExecutable().toGenericString(), inputMessage); &#125; return arg;&#125; 作为参数解析器，RequestResponseBodyMethodProcessor 支持所有标记@RequestBody的参数。在resolveArgument()方法中，通过调用readWithMessageConverters() 将 Request 转为对应 arg。我们来看一下 readWithMessageConverters() 到底做了什么 readWithMessageConverters 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980protected &lt;T&gt; Object readWithMessageConverters(HttpInputMessage inputMessage, MethodParameter parameter, Type targetType) throws IOException, HttpMediaTypeNotSupportedException, HttpMessageNotReadableException &#123; // 当前请求的contentType MediaType contentType; boolean noContentType = false; try &#123; contentType = inputMessage.getHeaders().getContentType(); &#125; catch (InvalidMediaTypeException ex) &#123; throw new HttpMediaTypeNotSupportedException(ex.getMessage()); &#125; if (contentType == null) &#123; noContentType = true; contentType = MediaType.APPLICATION_OCTET_STREAM; &#125; // Controller参数的Class Class&lt;?&gt; contextClass = parameter.getContainingClass(); Class&lt;T&gt; targetClass = (targetType instanceof Class ? (Class&lt;T&gt;) targetType : null); if (targetClass == null) &#123; ResolvableType resolvableType = ResolvableType.forMethodParameter(parameter); targetClass = (Class&lt;T&gt;) resolvableType.resolve(); &#125; // 当前请求方式 HttpMethod httpMethod = (inputMessage instanceof HttpRequest ? ((HttpRequest) inputMessage).getMethod() : null); Object body = NO_VALUE; EmptyBodyCheckingHttpInputMessage message; try &#123; message = new EmptyBodyCheckingHttpInputMessage(inputMessage); // 遍历所有的HttpMessageConverter， for (HttpMessageConverter&lt;?&gt; converter : this.messageConverters) &#123; Class&lt;HttpMessageConverter&lt;?&gt;&gt; converterType = (Class&lt;HttpMessageConverter&lt;?&gt;&gt;) converter.getClass(); GenericHttpMessageConverter&lt;?&gt; genericConverter = (converter instanceof GenericHttpMessageConverter ? (GenericHttpMessageConverter&lt;?&gt;) converter : null); // 如果当前的HttpMessageConverter可以解析对应的 class和contentType if (genericConverter != null ? genericConverter.canRead(targetType, contextClass, contentType) : (targetClass != null &amp;&amp; converter.canRead(targetClass, contentType))) &#123; if (message.hasBody()) &#123; HttpInputMessage msgToUse = getAdvice().beforeBodyRead(message, parameter, targetType, converterType); // 将Http报文转换为对应的class body = (genericConverter != null ? genericConverter.read(targetType, contextClass, msgToUse) : ((HttpMessageConverter&lt;T&gt;) converter).read(targetClass, msgToUse)); body = getAdvice().afterBodyRead(body, msgToUse, parameter, targetType, converterType); &#125; else &#123; body = getAdvice().handleEmptyBody(null, message, parameter, targetType, converterType); &#125; break; &#125; &#125; &#125; catch (IOException ex) &#123; throw new HttpMessageNotReadableException("I/O error while reading input message", ex, inputMessage); &#125; if (body == NO_VALUE) &#123; if (httpMethod == null || !SUPPORTED_METHODS.contains(httpMethod) || (noContentType &amp;&amp; !message.hasBody())) &#123; return null; &#125; throw new HttpMediaTypeNotSupportedException(contentType, this.allSupportedMediaTypes); &#125; MediaType selectedContentType = contentType; Object theBody = body; LogFormatUtils.traceDebug(logger, traceOn -&gt; &#123; String formatted = LogFormatUtils.formatValue(theBody, !traceOn); return "Read \"" + selectedContentType + "\" to [" + formatted + "]"; &#125;); return body;&#125; 上述代码核心逻辑就是遍历当前解析中配置的所有 HttpMessageConverter，如果某个Converter可以解析当前的 contentType，就把转换工作交给他去进行。 之前做过将默认解析替换为fastjson，当时就是添加一个FastJson实现的HttpMessageConverter，但是那时候并不理解这么做是为了什么，现在才恍然大悟… handleReturnValue RequestResponseBodyMethodProcessor 的Response处理逻辑和解析逻辑类似，找到一个支持的HttpMessageConverter，把响应工作交给他，感兴趣的童鞋可以自己找下源码。 RequestResponseBodyMethodProcessor 是怎么被调用的上面讲了 RequestResponseBodyMethodProcessor 做了参数解析和响应处理的工作，那么他在Spring框架中是怎么被调用的，我们来看一下 如图，RequestMappingHandlerAdapter 的resolvers（Request解析器）、handlers（Response处理器）还有 ExceptionHandlerExceptionResolver 的handlers 调用了 RequestResponseBodyMethodProcessor RequestMappingHandlerAdapter我们只分析一下 RequestMappingHandlerAdapter ，该类对所有标记 @RequestMapping的注解进行解析和响应 在WebMvcConfigurationSupport中，配置了该Bean，将其加入到Spring容器中，我们自定义的参数解析、响应解析、和HttpMessageConvert 通过上图的方法set到 RequestMappingHandlerAdapter 中。 123456789101112131415161718192021222324252627282930@Beanpublic RequestMappingHandlerAdapter requestMappingHandlerAdapter() &#123; RequestMappingHandlerAdapter adapter = createRequestMappingHandlerAdapter(); adapter.setContentNegotiationManager(mvcContentNegotiationManager()); // 获取所有HttpMessageConverter，包括我们自定义的配置 adapter.setMessageConverters(getMessageConverters()); adapter.setWebBindingInitializer(getConfigurableWebBindingInitializer()); // 自定义的参数解析器 adapter.setCustomArgumentResolvers(getArgumentResolvers()); // 自定义的响应处理器 adapter.setCustomReturnValueHandlers(getReturnValueHandlers()); if (jackson2Present) &#123; adapter.setRequestBodyAdvice(Collections.singletonList(new JsonViewRequestBodyAdvice())); adapter.setResponseBodyAdvice(Collections.singletonList(new JsonViewResponseBodyAdvice())); &#125; AsyncSupportConfigurer configurer = new AsyncSupportConfigurer(); configureAsyncSupport(configurer); if (configurer.getTaskExecutor() != null) &#123; adapter.setTaskExecutor(configurer.getTaskExecutor()); &#125; if (configurer.getTimeout() != null) &#123; adapter.setAsyncRequestTimeout(configurer.getTimeout()); &#125; adapter.setCallableInterceptors(configurer.getCallableInterceptors()); adapter.setDeferredResultInterceptors(configurer.getDeferredResultInterceptors()); return adapter;&#125; 继续说 RequestMappingHandlerAdapter ，getDefaultArgumentResolvers() 封装了SpringBoot中的默认参数解析器，其中就有我们的本节所讲的 RequestResponseBodyMethodProcessor ，在afterPropertiesSet() 方法中调用了该方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667@Overridepublic void afterPropertiesSet() &#123; // Do this first, it may add ResponseBody advice beans initControllerAdviceCache(); if (this.argumentResolvers == null) &#123; List&lt;HandlerMethodArgumentResolver&gt; resolvers = getDefaultArgumentResolvers(); this.argumentResolvers = new HandlerMethodArgumentResolverComposite().addResolvers(resolvers); &#125; if (this.initBinderArgumentResolvers == null) &#123; List&lt;HandlerMethodArgumentResolver&gt; resolvers = getDefaultInitBinderArgumentResolvers(); this.initBinderArgumentResolvers = new HandlerMethodArgumentResolverComposite().addResolvers(resolvers); &#125; if (this.returnValueHandlers == null) &#123; List&lt;HandlerMethodReturnValueHandler&gt; handlers = getDefaultReturnValueHandlers(); this.returnValueHandlers = new HandlerMethodReturnValueHandlerComposite().addHandlers(handlers); &#125;&#125;private List&lt;HandlerMethodArgumentResolver&gt; getDefaultArgumentResolvers() &#123; List&lt;HandlerMethodArgumentResolver&gt; resolvers = new ArrayList&lt;&gt;(); // Annotation-based argument resolution resolvers.add(new RequestParamMethodArgumentResolver(getBeanFactory(), false)); resolvers.add(new RequestParamMapMethodArgumentResolver()); resolvers.add(new PathVariableMethodArgumentResolver()); resolvers.add(new PathVariableMapMethodArgumentResolver()); resolvers.add(new MatrixVariableMethodArgumentResolver()); resolvers.add(new MatrixVariableMapMethodArgumentResolver()); resolvers.add(new ServletModelAttributeMethodProcessor(false)); // 添加RequestResponseBodyMethodProcessor resolvers.add(new RequestResponseBodyMethodProcessor(getMessageConverters(), this.requestResponseBodyAdvice)); // 省略，详见源码... return resolvers;&#125;private List&lt;HandlerMethodReturnValueHandler&gt; getDefaultReturnValueHandlers() &#123; List&lt;HandlerMethodReturnValueHandler&gt; handlers = new ArrayList&lt;&gt;(); // Single-purpose return value types handlers.add(new ModelAndViewMethodReturnValueHandler()); handlers.add(new ModelMethodProcessor()); handlers.add(new ViewMethodReturnValueHandler()); handlers.add(new ResponseBodyEmitterReturnValueHandler(getMessageConverters(), this.reactiveAdapterRegistry, this.taskExecutor, this.contentNegotiationManager)); handlers.add(new StreamingResponseBodyReturnValueHandler()); handlers.add(new HttpEntityMethodProcessor(getMessageConverters(), this.contentNegotiationManager, this.requestResponseBodyAdvice)); handlers.add(new HttpHeadersReturnValueHandler()); handlers.add(new CallableMethodReturnValueHandler()); handlers.add(new DeferredResultMethodReturnValueHandler()); handlers.add(new AsyncTaskMethodReturnValueHandler(this.beanFactory)); // Annotation-based return value types handlers.add(new ModelAttributeMethodProcessor(false)); // 添加RequestResponseBodyMethodProcessor handlers.add(new RequestResponseBodyMethodProcessor(getMessageConverters(), this.contentNegotiationManager, this.requestResponseBodyAdvice)); // 省略，详见源码... return handlers;&#125; RequestResponseBodyMethodProcessor 何时被调用上面铺垫了这么多，终于来了 RequestMappingHandlerAdapter 的 invokeHandlerMethod 中 构建了 invocableMethod 对象并将所有的解析器和处理器封装到该对象，通过invocableMethod.invokeAndHandle() 进行对请求的解析，对controller的调用，以及响应的处理 invocableMethod.invokeAndHandle() 中是怎么样实现的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public void invokeAndHandle(ServletWebRequest webRequest, ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123; // 参数解析，并反射调用controller方法，获取方法返回值 Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs); // 下面就是对Response的处理 setResponseStatus(webRequest); if (returnValue == null) &#123; if (isRequestNotModified(webRequest) || getResponseStatus() != null || mavContainer.isRequestHandled()) &#123; mavContainer.setRequestHandled(true); return; &#125; &#125; else if (StringUtils.hasText(getResponseStatusReason())) &#123; mavContainer.setRequestHandled(true); return; &#125; mavContainer.setRequestHandled(false); Assert.state(this.returnValueHandlers != null, "No return value handlers"); try &#123; this.returnValueHandlers.handleReturnValue( returnValue, getReturnValueType(returnValue), mavContainer, webRequest); &#125; catch (Exception ex) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(formatErrorForReturnValue(returnValue), ex); &#125; throw ex; &#125;&#125;public Object invokeForRequest(NativeWebRequest request, @Nullable ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123; // 调用参数解析器获取调用controller 所需的参数 Object[] args = getMethodArgumentValues(request, mavContainer, providedArgs); if (logger.isTraceEnabled()) &#123; logger.trace("Arguments: " + Arrays.toString(args)); &#125; // 反射调用 controller return doInvoke(args);&#125;protected Object[] getMethodArgumentValues(NativeWebRequest request, @Nullable ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123; if (ObjectUtils.isEmpty(getMethodParameters())) &#123; return EMPTY_ARGS; &#125; MethodParameter[] parameters = getMethodParameters(); Object[] args = new Object[parameters.length]; // 遍历解析参数 for (int i = 0; i &lt; parameters.length; i++) &#123; MethodParameter parameter = parameters[i]; parameter.initParameterNameDiscovery(this.parameterNameDiscoverer); args[i] = findProvidedArgument(parameter, providedArgs); if (args[i] != null) &#123; continue; &#125; // 这里的 resolvers 是一个封装了所有参数解析器的包装类，遍历所有解析器，如果不能找到支持当前参数的，抛出异常 // 如果找到当前参数对应的解析器，则缓存起来，在下面的 resolvers.resolveArgument 时，直接使用 if (!this.resolvers.supportsParameter(parameter)) &#123; throw new IllegalStateException(formatArgumentError(parameter, "No suitable resolver")); &#125; try &#123; // 调用参数解析器 args[i] = this.resolvers.resolveArgument(parameter, mavContainer, request, this.dataBinderFactory); &#125; catch (Exception ex) &#123; // Leave stack trace for later, exception may actually be resolved and handled.. if (logger.isDebugEnabled()) &#123; String error = ex.getMessage(); if (error != null &amp;&amp; !error.contains(parameter.getExecutable().toGenericString())) &#123; logger.debug(formatArgumentError(parameter, error)); &#125; &#125; throw ex; &#125; &#125; return args;&#125; invokeAndHandle() 里做了三件事 将请求中解析为Controller中指定的参数 用解析好的参数反射调用 Controller 方法 处理响应 一次Http请求经历了什么回过头来再看，这时候我们发一个请求，在 RequestMappingHandlerAdapter 的 invokeHandlerMethod()中 debug一下，看一下线程栈是什么样的 简单画一张图来表示一下 END 欢迎各位给出意见和指正]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发编程-volatile]]></title>
    <url>%2Fpost%2Feb0b8344.html</url>
    <content type="text"><![CDATA[原文：Java并发编程：volatile关键字解析 原文讲解了很多东西作为铺垫，在原文内容有些删减和更改。如读者觉得不妥，或者理解不到位，建议阅读原文。 内存模型相关概念大家都知道，计算机在执行程序时，每条指令都是在CPU中执行的，而执行指令过程中，势必涉及到数据的读取和写入。由于程序运行过程中的临时数据是存放在主存（物理内存）当中的，这时就存在一个问题，由于CPU执行速度很快，而从内存读取数据和向内存写入数据的过程跟CPU执行指令的速度比起来要慢的多，因此如果任何时候对数据的操作都要通过和内存的交互来进行，会大大降低指令执行的速度。因此在CPU里面就有了高速缓存。 也就是，当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。举个简单的例子，比如下面的这段代码： 1i = i + 1; 当线程执行这个语句时，会先从主存当中读取i的值，然后复制一份到高速缓存当中，然后CPU执行指令对i进行加1操作，然后将数据写入高速缓存，最后将高速缓存中i最新的值刷新到主存当中。 这个代码在单线程中运行是没有任何问题的，但是在多线程中运行就会有问题了。在多核CPU中，每条线程可能运行于不同的CPU中，因此每个线程运行时有自己的高速缓存（对单核CPU来说，其实也会出现这种问题，只不过是以线程调度的形式来分别执行的）。本文我们以多核CPU为例。 比如同时有2个线程执行这段代码，假如初始时i的值为0，那么我们希望两个线程执行完之后i的值变为2。但是事实会是这样吗？ 可能存在下面一种情况：初始时，两个线程分别读取i的值存入各自所在的CPU的高速缓存当中，然后线程1进行加1操作，然后把i的最新值1写入到内存。此时线程2的高速缓存当中i的值还是0，进行加1操作之后，i的值为1，然后线程2把i的值写入内存。 最终结果i的值是1，而不是2。这就是著名的缓存一致性问题。通常称这种被多个线程访问的变量为共享变量。 也就是说，如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致的问题。 为了解决缓存不一致性问题，通常来说有以下2种解决方法： 1）通过在总线加LOCK#锁的方式 2）通过缓存一致性协议 这2种方式都是硬件层面上提供的方式。 在早期的CPU当中，是通过在总线上加LOCK#锁的形式来解决缓存不一致的问题。因为CPU和其他部件进行通信都是通过总线来进行的，如果对总线加LOCK#锁的话，也就是说阻塞了其他CPU对其他部件访问（如内存），从而使得只能有一个CPU能使用这个变量的内存。比如上面例子中 如果一个线程在执行 i = i +1，如果在执行这段代码的过程中，在总线上发出了LCOK#锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。 但是上面的方式会有一个问题，由于在锁住总线期间，其他CPU无法访问内存，导致效率低下。 所以就出现了缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。 什么是Java 内存模型在Java虚拟机规范中试图定义一种Java内存模型（Java Memory Model，JMM）来屏蔽各个硬件平台和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。那么Java内存模型规定了哪些东西呢，它定义了程序中变量的访问规则，往大一点说是定义了程序执行的次序。注意，为了获得较好的执行性能，Java内存模型并没有限制执行引擎使用处理器的寄存器或者高速缓存来提升指令执行速度，也没有限制编译器对指令进行重排序。也就是说，在java内存模型中，也会存在缓存一致性问题和指令重排序的问题。 Java内存模型规定所有的变量都是存在主存当中（类似于前面说的物理内存），每个线程都有自己的工作内存（类似于前面的高速缓存）。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存。 多线程中可能发生的问题内存不可见12345678//线程1boolean stop = false;while(!stop)&#123; doSomething();&#125; //线程2stop = true; 线程2通过stop = true 尝试停止线程1，线程1一定会中断吗？不一定 因为上文说过了，每个线程都会有自己的工作内存，线程1运行时，在将stop变量拷贝到自己的工作内存中。线程2这时候修改了stop，然后没来得及同步到主存，这时候线程2去做别的工作了，线程1不知道线程2修改了变量，一直循环下去。 重排序 什么是指令重排序？ 一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。 例如下面代码1234int a = 10; //语句1int r = 2; //语句2a = a + 3; //语句3r = a*a; //语句4 可能的顺序 但是绝不可能出现 语句2 -&gt; 语句1 -&gt; 语句4 -&gt; 语句3 上述顺序改变数据之间的依赖关系，处理器会考虑语句之间的依赖关系 重排序造成的影响 123456789//线程1:context = loadContext(); //语句1inited = true; //语句2 //线程2:while(!inited )&#123; sleep()&#125;doSomethingwithconfig(context); 线程1的代码没有依赖关系，如果顺序变为了 语句2 -&gt; 语句1，线程2可能就会拿到一个没有初始化的 context对象从而导致程序异常，而这种异常是很难排查的 volatile作用volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作 一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 1）保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 2）禁止进行指令重排序。 1. 内存可见性变量使用了volatile之后，所有读取操作全部发生在主存中，也就避免了上述的内存不可见的问题了 2. 防止重排序volatile关键字禁止指令重排序有两层意思： 1）当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行； 2）在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。 volidate 应用场景1.状态标记123456789volatile boolean flag = false; while(!flag)&#123; doSomething();&#125; public void setFlag() &#123; flag = true;&#125; 12345678910volatile boolean inited = false;//线程1:context = loadContext(); inited = true; //线程2:while(!inited )&#123;sleep()&#125;doSomethingwithconfig(context); 2.双重检查锁 （double check）1234567891011121314151617class Singleton &#123; private volatile static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if(instance==null) &#123; synchronized (Singleton.class) &#123; if(instance==null) instance = new Singleton(); &#125; &#125; return instance; &#125;&#125; 关于双重检查锁volatile 的两个作用 可见性 防止new Singleton()时重排序 创建对象可以简单分解为三步 1.分配内存空间 2.初始化对象 3.将对象指向刚分配的内存空间 处理器在优化性能时，可能会将第二步和第三步进行重排序，这样会导致第二个线程获取到未初始化完成的对象，导致程序异常。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>Java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 并发登录人数控制]]></title>
    <url>%2Fpost%2F5f58dc01.html</url>
    <content type="text"><![CDATA[通常系统都会限制同一个账号的登录人数，多人登录要么限制后者登录，要么踢出前者，Spring Security 提供了这样的功能，本文讲解一下在没有使用Security的时候如何手动实现这个功能 本文借鉴了 https://jinnianshilongnian.iteye.com/blog/2039760, 如果你是使用 Shiro + Session 的模式，推荐阅读此文 demo 技术选型 SpringBoot JWT Filter Redis + Redisson JWT（token）存储在Redis中，类似 JSessionId-Session的关系，用户登录后每次请求在Header中携带jwt 如果你是使用session的话，也完全可以借鉴本文的思路，只是代码上需要加些改动 两种实现思路比较时间戳维护一个 username: jwtToken 这样的一个 key-value 在Reids中, Filter逻辑如下 123456789101112131415161718192021222324252627282930313233343536373839404142public class CompareKickOutFilter extends KickOutFilter &#123; @Autowired private UserService userService; @Override public boolean isAccessAllowed(HttpServletRequest request, HttpServletResponse response) &#123; String token = request.getHeader("Authorization"); String username = JWTUtil.getUsername(token); String userKey = PREFIX + username; RBucket&lt;String&gt; bucket = redissonClient.getBucket(userKey); String redisToken = bucket.get(); if (token.equals(redisToken)) &#123; return true; &#125; else if (StringUtils.isBlank(redisToken)) &#123; bucket.set(token); &#125; else &#123; Long redisTokenUnixTime = JWTUtil.getClaim(redisToken, "createTime").asLong(); Long tokenUnixTime = JWTUtil.getClaim(token, "createTime").asLong(); // token &gt; redisToken 则覆盖 if (tokenUnixTime.compareTo(redisTokenUnixTime) &gt; 0) &#123; bucket.set(token); &#125; else &#123; // 注销当前token userService.logout(token); sendJsonResponse(response, 4001, "您的账号已在其他设备登录"); return false; &#125; &#125; return true; &#125;&#125; 队列踢出 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class QueueKickOutFilter extends KickOutFilter &#123; /** * 踢出之前登录的/之后登录的用户 默认踢出之前登录的用户 */ private boolean kickoutAfter = false; /** * 同一个帐号最大会话数 默认1 */ private int maxSession = 1; public void setKickoutAfter(boolean kickoutAfter) &#123; this.kickoutAfter = kickoutAfter; &#125; public void setMaxSession(int maxSession) &#123; this.maxSession = maxSession; &#125; @Override public boolean isAccessAllowed(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; String token = request.getHeader("Authorization"); UserBO currentSession = CurrentUser.get(); Assert.notNull(currentSession, "currentSession cannot null"); String username = currentSession.getUsername(); String userKey = PREFIX + "deque_" + username; String lockKey = PREFIX_LOCK + username; RLock lock = redissonClient.getLock(lockKey); lock.lock(2, TimeUnit.SECONDS); try &#123; RDeque&lt;String&gt; deque = redissonClient.getDeque(userKey); // 如果队列里没有此token，且用户没有被踢出；放入队列 if (!deque.contains(token) &amp;&amp; currentSession.isKickout() == false) &#123; deque.push(token); &#125; // 如果队列里的sessionId数超出最大会话数，开始踢人 while (deque.size() &gt; maxSession) &#123; String kickoutSessionId; if (kickoutAfter) &#123; // 如果踢出后者 kickoutSessionId = deque.removeFirst(); &#125; else &#123; // 否则踢出前者 kickoutSessionId = deque.removeLast(); &#125; try &#123; RBucket&lt;UserBO&gt; bucket = redissonClient.getBucket(kickoutSessionId); UserBO kickoutSession = bucket.get(); if (kickoutSession != null) &#123; // 设置会话的kickout属性表示踢出了 kickoutSession.setKickout(true); bucket.set(kickoutSession); &#125; &#125; catch (Exception e) &#123; &#125; &#125; // 如果被踢出了，直接退出，重定向到踢出后的地址 if (currentSession.isKickout()) &#123; // 会话被踢出了 try &#123; // 注销 userService.logout(token); sendJsonResponse(response, 4001, "您的账号已在其他设备登录"); &#125; catch (Exception e) &#123; &#125; return false; &#125; &#125; finally &#123; if (lock.isHeldByCurrentThread()) &#123; lock.unlock(); LOGGER.info(Thread.currentThread().getName() + " unlock"); &#125; else &#123; LOGGER.info(Thread.currentThread().getName() + " already automatically release lock"); &#125; &#125; return true; &#125;&#125; 比较两种方法 第一种方法逻辑简单粗暴, 只维护一个key-value 不需要使用锁，非要说缺点的话没有第二种方法灵活。 第二种方法我很喜欢，代码很优雅灵活，但是逻辑相对麻烦一些，而且为了保证线程安全地操作队列，要使用分布式锁。目前我们项目中使用的是第一种方法 演示下载地址: https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/login-control 运行项目，访问localhost:8887 demo中没有存储用户信息，随意输入用户名密码，用户名相同则被踢出 访问 localhost:8887/index.html 弹出用户信息, 代表当前用户有效 另一个浏览器登录相同用户名，回到第一个浏览器刷新页面，提示被踢出 application.properties中选择开启哪种过滤器模式，默认是比较时间戳踢出，开启队列踢出 queue-filter.enabled=true]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>Redis</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java理解生产者-消费者设计模式]]></title>
    <url>%2Fpost%2F5b860a98.html</url>
    <content type="text"><![CDATA[在实际的软件开发过程中，经常会碰到如下场景：某个模块负责产生数据，这些数据由另一个模块来负责处理（此处的模块是广义的，可以是类、函数、线程、进程等）。产生数据的模块，就形象地称为生产者；而处理数据的模块，就称为消费者； 生产者和消费者之间通过缓冲区(通常是一个阻塞队列)实现通讯, 生产者将产生的数据放入缓冲区，消费者从缓冲区中获取数据。 举个栗子去食堂吃饭，食堂的叔叔阿姨会先将饭做好，放到食堂窗口，同学们会去食堂打饭。 生产者(食堂的叔叔阿姨) -&gt; 生产数据(做饭) -&gt; 缓冲区(食堂窗口) -&gt; 消费数据(打饭) -&gt; 消费者(同学) 生产者消费者实现思路 生产者和消费者的任务很明确，生产者只管生产数据，然后添加到缓冲队列。而消费者只管从缓冲队列中获取数据 可以说生产者消费者都很无脑，而缓冲队列则要忙一些，他起到了一个平衡生产者和消费者的作用。 如果生产者生产速度过快，消费者消费的很慢，并且缓冲队列达到了最大长度时。缓冲队列会阻塞生产者，让生产者停止生产，等待消费者消费了数据后，再唤醒生产者 同理，当消费者消费速度过快时，队列为空时。缓冲队列则会阻塞消费者，待生产者向队列添加数据后，再唤醒消费者 实现通过上述的分析后，我们来用最基本的Java代码实现一下 我们先来定义一下Consumer 和Producer ，他们的逻辑比较简单，这里我们只循环十次模拟一下生产消费的场景。Buffer 为缓冲区，我们待会再看1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 消费者 */class Consumer extends Thread &#123; private Buffer buffer; private int number; public Consumer(Buffer b, int number) &#123; buffer = b; this.number = number; &#125; public void run() &#123; int value; for (int i = 0; i &lt; 10; i++) &#123; // 从缓冲区中获取数据 value = buffer.get(); try &#123; // 模拟消费数据 sleep(1000); &#125; catch (InterruptedException e) &#123; &#125; System.out.println("消费者 #" + this.number + " got: " + value); &#125; &#125;&#125;/** * 生产者 */class Producer extends Thread &#123; private Buffer buffer; private int number; public Producer(Buffer b, int number) &#123; buffer = b; this.number = number; &#125; public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; // 模拟生产数据 sleep(500); &#125; catch (InterruptedException e) &#123; &#125; // 将数据放入缓冲区 buffer.put(i); System.out.println("生产者 #" + this.number + " put: " + i); &#125; &#125;&#125; 可以看到 Consumer 和Producer 没有什么逻辑，只是对缓冲区的读写操作，下面我们来重点看一下 Buffer的实现 12345678910111213141516171819202122232425262728293031/** * 缓冲区 */class Buffer &#123; private List&lt;Integer&gt; data = new ArrayList&lt;&gt;(); private static final int MAX = 10; private static final int MIN = 0; public synchronized int get() &#123; while (MIN == data.size()) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; &#125; &#125; Integer i = data.remove(0); notifyAll(); return i; &#125; public synchronized void put(int value) &#123; while (MAX == data.size()) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; &#125; &#125; data.add(value); notifyAll(); &#125;&#125; 分析put(): 生产者向缓冲区写入数据的操作，当MAX == data.size()时，就是我们刚刚所说的生产者速度过快，消费者速度过慢的情况，这个时候身为 “缓冲区” 要平衡一下，调用 Object.wait()，让当前生产者线程进入挂起状态，等待消费者消费数据后将其唤醒 当MAX &gt; data.size()时，向ArrayList中添加数据，并尝试唤醒正在等待的消费者（这一步是必须的） get(): 消费者向缓冲区读数据的操作，和上述逻辑相反，当MIN == data.size()时，这时消费者速度太快，生产者太慢，队列中已经没有数据了。”缓冲区” 再一次站了出来，通过wait()，让当前消费者线程进入挂起状态，等待生产者生产数据后将其唤醒 当MIN &lt; data.size()时，取出ArrayList中第一条数据，并尝试唤醒正在等待的生产者 上述案例完整代码 ProducerConsumer.java 上述使用最基本的Java代码实现生产者消费者模式，实际开发中我们可能会使用BlockingQueue、ReentrantLock、ThreadPoolExecutor这些更成熟的轮子，但是一通百通 关于上述案例的思考 为什么缓冲区的判断条件是 while(condition) 而不是 if(condition)？答：防止线程被错误的唤醒举例：当有两个消费者线程wait() 时，此时生产者在队列里放入了一条数据，并调用notifyAll(), 两个消费者线程被唤醒，第一个消费者成功取出队列中数据，而第二个消费者此时就是被错误的唤醒了，程序抛出异常，所以此处使用 while(condition)循环检查 Java中要求wait()方法为什么要放在同步块中？答：防止出现Lost Wake-Up举例：如果队列没有同步限制，消费者和生产者并发执行，很可能出现这种情况，消费者这时候检查了条件正准备wait(),这时候上下文切换到了生产者，生产者咔咔一顿操作向队列中添加了数据，并唤醒了消费者，而此时消费者并没有wait()，这个通知就丢掉了，然后消费者wait() 就这样睡去了… 为什么缓冲区一定要使用阻塞队列实现？同理就是为了防止出现Lost Wake-Up 为什么要使用生产者消费者模式顺序执行不就可以了吗？生产者消费者到底有什么意义？ 并发 （异步）生产者直接调用消费者，两者是同步（阻塞）的，如果消费者吞吐数据很慢，这时候生产者白白浪费大好时光。而使用这种模式之后，生产者将数据丢到缓冲区，继续生产，完全不依赖消费者，程序执行效率会大大提高。 解耦生产者和消费者之间不直接依赖，通过缓冲区通讯，将两个类之间的耦合度降到最低。 参考https://blog.csdn.net/u011109589/article/details/80519863]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot 使用 AOP 防止重复提交]]></title>
    <url>%2Fpost%2F8398a16a.html</url>
    <content type="text"><![CDATA[在传统的web项目中，防止重复提交，通常做法是：后端生成一个唯一的提交令牌（uuid），并存储在服务端。页面提交请求携带这个提交令牌，后端验证并在第一次验证后删除该令牌，保证提交请求的唯一性。 上述的思路其实没有问题的，但是需要前后端都稍加改动，如果在业务开发完在加这个的话，改动量未免有些大了，本节的实现方案无需前端配合，纯后端处理。 思路 自定义注解 @NoRepeatSubmit 标记所有Controller中的提交请求 通过AOP 对所有标记了 @NoRepeatSubmit 的方法拦截 在业务方法执行前，获取当前用户的 token（或者JSessionId）+ 当前请求地址，作为一个唯一 KEY，去获取 Redis 分布式锁（如果此时并发获取，只有一个线程会成功获取锁） 业务方法执行后，释放锁 关于Redis 分布式锁 不了解的同学戳这里 ==&gt; Redis分布式锁的正确实现方式 使用Redis 是为了在负载均衡部署，如果是单机的部署的项目可以使用一个线程安全的本地Cache 替代 Redis Code这里只贴出 AOP 类和测试类，完整代码见 ==&gt; Gitee12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@Aspect@Componentpublic class RepeatSubmitAspect &#123; private final static Logger LOGGER = LoggerFactory.getLogger(RepeatSubmitAspect.class); @Autowired private RedisLock redisLock; @Pointcut("@annotation(noRepeatSubmit)") public void pointCut(NoRepeatSubmit noRepeatSubmit) &#123; &#125; @Around("pointCut(noRepeatSubmit)") public Object around(ProceedingJoinPoint pjp, NoRepeatSubmit noRepeatSubmit) throws Throwable &#123; int lockSeconds = noRepeatSubmit.lockTime(); HttpServletRequest request = RequestUtils.getRequest(); Assert.notNull(request, "request can not null"); // 此处可以用token或者JSessionId String token = request.getHeader("Authorization"); String path = request.getServletPath(); String key = getKey(token, path); String clientId = getClientId(); boolean isSuccess = redisLock.tryLock(key, clientId, lockSeconds); if (isSuccess) &#123; LOGGER.info("tryLock success, key = [&#123;&#125;], clientId = [&#123;&#125;]", key, clientId); // 获取锁成功, 执行进程 Object result; try &#123; result = pjp.proceed(); &#125; finally &#123; // 解锁 redisLock.releaseLock(key, clientId); LOGGER.info("releaseLock success, key = [&#123;&#125;], clientId = [&#123;&#125;]", key, clientId); &#125; return result; &#125; else &#123; // 获取锁失败，认为是重复提交的请求 LOGGER.info("tryLock fail, key = [&#123;&#125;]", key); return new ResultBean(ResultBean.FAIL, "重复请求，请稍后再试", null); &#125; &#125; private String getKey(String token, String path) &#123; return token + path; &#125; private String getClientId() &#123; return UUID.randomUUID().toString(); &#125;&#125; 多线程测试测试代码如下，模拟十个请求并发同时提交1234567891011121314151617181920212223242526272829303132333435363738394041424344@Componentpublic class RunTest implements ApplicationRunner &#123; private static final Logger LOGGER = LoggerFactory.getLogger(RunTest.class); @Autowired private RestTemplate restTemplate; @Override public void run(ApplicationArguments args) throws Exception &#123; System.out.println("执行多线程测试"); String url="http://localhost:8000/submit"; CountDownLatch countDownLatch = new CountDownLatch(1); ExecutorService executorService = Executors.newFixedThreadPool(10); for(int i=0; i&lt;10; i++)&#123; String userId = "userId" + i; HttpEntity request = buildRequest(userId); executorService.submit(() -&gt; &#123; try &#123; countDownLatch.await(); System.out.println("Thread:"+Thread.currentThread().getName()+", time:"+System.currentTimeMillis()); ResponseEntity&lt;String&gt; response = restTemplate.postForEntity(url, request, String.class); System.out.println("Thread:"+Thread.currentThread().getName() + "," + response.getBody()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.countDown(); &#125; private HttpEntity buildRequest(String userId) &#123; HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_JSON); headers.set("Authorization", "yourToken"); Map&lt;String, Object&gt; body = new HashMap&lt;&gt;(); body.put("userId", userId); return new HttpEntity&lt;&gt;(body, headers); &#125;&#125; 成功防止重复提交，控制台日志如下，可以看到十个线程的启动时间几乎同时发起，只有一个请求提交成功了 本节demo戳这里 ==&gt; Giteebuild项目之后，启动本地redis，运行项目自动执行测试方法 参考https://www.jianshu.com/p/09c6b05b670a]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>SpringBoot</tag>
        <tag>AOP</tag>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-并发控制----读锁、写锁、乐观锁]]></title>
    <url>%2Fpost%2Fe708def4.html</url>
    <content type="text"><![CDATA[并发是一个让人很头疼的问题，通常我们会在服务端或者数据库端做处理，保证在并发下数据的准确性，今天我们简要的讨论一下MySQL中如何通过锁解决并发问题 读锁 也叫共享锁 （shared lock） 如何使用SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE 详解即事务A 使用共享锁 获取了某条（或者某些）记录时，事务B 可以读取这些记录，可以继续添加共享锁，但是不能修改或删除这些记录（当事务B 对这些数据修改或删除时会进入阻塞状态，直至锁等待超时或者事务A提交） 使用场景读取结果集的最新版本，同时防止其他事务产生更新该结果集主要用在需要数据依存关系时确认某行记录是否存在，并确保没有人对这个记录进行UPDATE或者DELETE操作 注意事项当使用读锁时，避免产生如下操作 123456事务1BEGIN;select * from sys_user where id = 1 LOCK IN SHARE MODE; (步骤1)update sys_user set username = &quot;taven&quot; where id = 1; (步骤3，发生阻塞)COMMIT; 123456事务2BEGIN;select * from sys_user where id = 1 LOCK IN SHARE MODE; (步骤2)update sys_user set username = &quot;taven&quot; where id = 1; (步骤4，死锁)COMMIT; 分析根据我们之前对读锁定义可知，当有事务拿到一个结果集的读锁时，其他事务想要更新该结果集，需要拿到读锁的事务提交（释放锁）。而上述情况两个事务分别拿到了读锁，而且都有update 操作，两个事务互相等待造成死锁（都在等待对方释放读锁） 写锁 也叫排它锁（exclusive lock） 如何使用SELECT * FROM table_name WHERE ... FOR UPDATE 详解一个写锁会阻塞其他的读锁和写锁即事务A 对某些记录添加写锁时，事务B 无法向这些记录添加写锁或者读锁（不添加锁的读取是可以的），事务B 也无法执行对 锁住的数据 update delete 使用场景读取结果集的最新版本，同时防止其他事务产生读取或者更新该结果集。例如：并发下对商品库存的操作 注意事项在使用读锁、写锁时都需要注意，读锁、写锁属于行级锁。即事务1 对商品A 获取写锁，和事务2 对商品B 获取写锁互相不会阻塞的。需要我们注意的是我们的SQL要合理使用索引，当我们的SQL 全表扫描的时候，行级锁会变成表锁。使用EXPLAIN查看 SQL是否使用了索引，扫描了多少行 乐观锁 上述介绍的是行级锁，可以最大程度地支持并发处理（同时也带来了最大的锁开销）乐观锁是一种逻辑锁，通过数据的版本号（vesion）的机制来实现，极大降低了数据库的性能开销。 我们为表添加一个字段 version，读取数据时将此版本号一同读出，之后更新时，对此版本号+1，同时将提交数据的version 与数据库中对应记录的当前version 进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据 123update t_goods set status=2,version=version+1where id=#&#123;id&#125; and version &lt; #&#123;version&#125;; // 更新前将version自增 或者123update t_goods set status=2,version=version+1where id=#&#123;id&#125; and version = #&#123;version&#125;; // 更新前version 不自增]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>并发</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-理解事务隔离级别]]></title>
    <url>%2Fpost%2Feca98c0.html</url>
    <content type="text"><![CDATA[在SQL标准中定义了四种隔离级别，每一种级别都规定了一个事务中所做的修改，哪些在事务内和事务间是可见的，哪些是不可见，较低的隔离级别通常可以执行更高的并发，系统的开销也更低 事务的ACID 原子性（atomicity）一个事务必须视为一个不可分割的最小工作单元，整个事务的所有操作要么全部提交成功，要么全部回滚 一致性（consistency）数据库总是从一个一致性状态转换到另一个一致性状态。 隔离性（isolation）通常来说，一个事务所作的修改在最终提交以前，对其他事务是不可见的，在讨论隔离级别的时候，会理解为什么说是 “通常来说” 是不可见的 持久性（durability）一旦事务提交，则所做的修改就会永久的保存到数据库中 隔离级别1.READ UNCOMMITTED （未提交读） 即在该级别，事务中的对数据的修改，即使没有提交，对其他事务也是可见的。这种情况被称为脏读（Dirty Read），这个级别会导致很多问题，而且性能相比其他级别不会好太多，实际很少使用。 2.READ COMMITTED （提交读） 大多数据库的默认隔离级别（但MySQL不是），该级别解决了脏读的问题。但是事务中会读到其他事务已提交的数据，无法保证读一致性，会造成不可重复读的问题。如下事务的两次读取是不一致的 3.REPEATABLE READ （可重复读） 该隔离级别，解决了脏读，不可重复读。InnoDB 使用 MVCC（多版本并发控制下文会讲到） 保证了事务中的读一致性。但还是会出现一个幻读的情况 如下图（左为事务A，右为事务B）事务执行这样的逻辑：查找是否有某条记录，如果不存在则新增。 事务A 查询是否存在 id=&quot;1&quot;这条数据，发现不存在，准备执行insert 此时事务B insert了这条数据，并提交了事务。 事务A 执行insert 发生主键冲突，再次执行了 select * from where id = &quot;1&quot;，发现依旧不存在当前结果集 RR 级别下如何防止幻读12# 用 X锁SELECT i` FROM `users` WHERE `id` = &quot;1&quot; FOR UPDATE; 如果 id = 1 的记录存在则会被加行（X）锁，如果不存在，则会加 next-lock key / gap 锁（范围行锁），即记录存在与否，mysql 都会对记录应该对应的索引加锁，其他事务是无法再获得做操作的。 4.SERIALIZABLE （串行化）在该隔离级别下，读取的每一行都会被添加锁（个人理解是读锁和gap锁），导致大量的超时和锁争用问题，实际应用中很少使用，只有在非常需要确保数据一致性且可以接受没有并发的情况下，才考虑该级别。 在 SERIALIZABLE 级别下再运行一下 上述的demo 可以看到步骤2的 insert 被阻塞了，上述“幻读”的情况 MVCCMVCC 是什么 MVVC (Multi-Version Concurrency Control, 多版本并发控制)，在InnoDB引擎下，MVCC是为了实现事务的隔离性，通过版本号，避免同一数据在不同事务间的竞争，你可以把它当成基于多版本号的一种乐观锁，读不加锁，读写不冲突MVCC 实现机制 InnoDB在每行数据都增加两个隐藏字段，一个记录创建的版本号，一个记录删除的版本号。 在MVVC 中，为了保证数据操作在多线程过程中，保证事务隔离的机制，降低锁竞争的压力，保证较高的并发量。在每开启一个事务时，会生成一个事务的版本号，被操作的数据会生成一条新的数据行（临时），但是在提交前对其他事务是不可见的，对于数据的更新（包括增删改）操作成功，会将这个版本号更新到数据的行中，事务提交成功，将新的版本号更新到此数据行中，这样保证了每个事务操作的数据，都是互不影响的，也不存在锁的问题。 MVVC下的CRUD （REPEATABLE READ下） SELECT： InnoDB 查询的每行数据必须满足以下两点 1、InnoDB必须找到一个行的版本，它至少要和事务的版本一样老(即它的版本号不大于事务的版本号)。这保证了不管是事务开始之前，或者事务创建时，或者修改了这行数据的时候，这行数据是存在的。 2、这行数据的删除版本必须是未定义的或者比事务版本要大。这可以保证在事务开始之前这行数据没有被删除。符合这两个条件的行可能会被当作查询结果而返回。 INSERT： InnoDB为这个新行记录当前的系统版本号。 DELETE： InnoDB将当前的系统版本号设置为这一行的删除ID。 UPDATE： InnoDB会写一个这行数据的新拷贝，这个拷贝的版本为当前的系统版本号。它同时也会将这个版本号写到旧行的删除版本里。 参考 MySQL之MVVC简介 mysql 幻读的详解、实例及解决办法 《高性能MySQL 第三版》]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-10w+数据-insert-优化]]></title>
    <url>%2Fpost%2F5c987bd6.html</url>
    <content type="text"><![CDATA[由于业务原因，遇到了如题所述的业务问题，事务执行时间在30s~50s 不等，效果非常不理想 方案1. jdbc批处理5w+ 数据测试，分别使用了mybatis insert()()（拼接xml）, mybatis的批处理和 jdbc的批处理。可以看到在jdbc执行时间方面是差不多的，但是在方法执行时间上，批处理要稍微快了一些，但是还是不理想 方案2. 优化MySQL 参数修改 my.ini innodb_buffer_pool_size : InnoDB, unlike MyISAM, uses a buffer pool to cache both indexes androw data. The bigger you set this the less disk I/O is needed toaccess data in tables. On a dedicated database server you may set thisparameter up to 80% of the machine physical memory size. Do not set ittoo large, though, because competition of the physical memory maycause paging in the operating system. Note that on 32bit systems youmight be limited to 2-3.5G of user level memory per process, so do notset it too high. Innodb的缓冲池会缓存数据和索引，设置的越大访问表中的数据所需的磁盘I/O就越少。 修改innodb_buffer_pool_size = 512M测试一下效率，这速度简直感人！ innodb_log_buffer_size : The size of the buffer InnoDB uses for buffering log data. As soon asit is full, InnoDB will have to flush it to disk. As it is flushed once per second anyway, it does not make sense to have it very large(even with long transactions). 表示InnoDB写入到磁盘上的日志文件时使用的缓冲区的字节数，默认值为8M。当缓冲区充满时，InnoDB将刷新数据到磁盘。由于它每秒刷新一次，所以将它设置得非常大是没有意义的 (即使是长事务)。 innodb_log_file_size : Size of each log file in a log group. You should set the combined sizeof log files to about 25%-100% of your buffer pool size to avoidunneeded buffer pool flush activity on log file overwrite. However,note that a larger logfile size will increase the time needed for therecovery process. 该值越大，缓冲池中必要的检查点刷新活动就会越少，节省磁盘I/ O。但是越大的日志文件，mysql的崩溃恢复就越慢 设置上述两个参数innodb_log_file_size=64M innodb_log_buffer_size=16M，效率提升的并不明显。 总结 数据量大时，批处理在方法执行时间上要比 mybatis xml拼接快一点 （批处理只编译一条SQL，而拼接的方式SQL会很长） 性能瓶颈优化还是要从数据库下手，目前来看MySQL 大数据量时很依赖 innodb_buffer_pool_size （缓冲池）参考 https://my.oschina.net/realfighter/blog/368225]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java打包成exe 在没有JRE环境的电脑上运行]]></title>
    <url>%2Fpost%2F33eaa3e0.html</url>
    <content type="text"><![CDATA[公司业务需求原因，需要给用户提供一个桌面应用程序。由于时间关系，没有考虑.net，使用了江湖上失传已久的Java Swing 准备工作 将你的Java项目打包为 可执行Jar 使用 exe4j 生成.exe 新建一个文件夹，将你的 JRE（与JDK同级别的那个JRE），可执行jar 都复制过来，下图除红圈以外的Jar 为项目的依赖Jar 包 打开 exe4j ，Next 选择 JAR IN EXE 选择刚刚我们新建的文件夹 （包含JRE，和项目可执行JAR） 这里是设置文件名，图标，是否仅允许一个应用实例等等，需要注意的是，64位的 jdk是需要设置一下的，如图 这里是将项目的，可执行JAR，还有依赖JAR 全部添加进来，注意一定要使用相对路径，Main class from 选择一个项目的启动类。 设置 Search sequence 清除默认配置，设置JRE，使用相对路径 后几步默认配置，下一步即可。 备注 需要注意的是，JAR和JRE 一定要使用相对路径（绝对路径无法保证你的路径和用户电脑一致） exe4j，没有激活之前生成的exe，启动之前会跳个对话框，激活即可 生成的exe 不依赖之前的JAR，只依赖JRE 参考：https://www.cnblogs.com/lsy-blogs/p/7668425.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>swing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Spring Cache + Redis 作为缓存]]></title>
    <url>%2Fpost%2Fef257646.html</url>
    <content type="text"><![CDATA[本文介绍如何使用 spring-cache，以及集成 Redis 作为缓存实现。表格过长，推荐读者使用电脑阅读 准备工作Redis windows 安装 如何配置maven完整依赖详见 ==&gt; Gitee123456789101112131415161718&lt;!-- 使用spring cache --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- redis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 为了解决 ClassNotFoundException: org.apache.commons.poolimpl.GenericObjectPoolConfig --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;0&lt;/version&gt;&lt;/dependency&gt; application.properties1234567891011121314151617181920212223# Redis数据库索引（默认为0）spring.redis.database=0 # Redis服务器地址spring.redis.host=localhost# Redis服务器连接端口spring.redis.port=6379 # Redis服务器连接密码（默认为空）#spring.redis.password=yourpwd# 连接池最大连接数（使用负值表示没有限制）spring.redis.lettuce.pool.max-active=8 # 连接池最大阻塞等待时间 spring.redis.lettuce.pool.max-wait=-1ms# 连接池中的最大空闲连接spring.redis.lettuce.pool.max-idle=8 # 连接池中的最小空闲连接spring.redis.lettuce.pool.min-idle=0 # 连接超时时间（毫秒）spring.redis.timeout=5000ms#配置缓存相关cache.default.expire-time=200cache.user.expire-time=180cache.user.name=test @EnableCaching标记注解 @EnableCaching，开启缓存，并配置Redis缓存管理器，需要初始化一个缓存空间。在缓存的时候，也需要标记使用哪一个缓存空间123456789101112131415161718192021222324252627282930313233343536373839404142434445@Configuration@EnableCachingpublic class RedisConfig &#123; @Value("$&#123;cache.default.expire-time&#125;") private int defaultExpireTime; @Value("$&#123;cache.user.expire-time&#125;") private int userCacheExpireTime; @Value("$&#123;cache.user.name&#125;") private String userCacheName; /** * 缓存管理器 * * @param lettuceConnectionFactory * @return */ @Bean public CacheManager cacheManager(RedisConnectionFactory lettuceConnectionFactory) &#123; RedisCacheConfiguration defaultCacheConfig = RedisCacheConfiguration.defaultCacheConfig(); // 设置缓存管理器管理的缓存的默认过期时间 defaultCacheConfig = defaultCacheConfig.entryTtl(Duration.ofSeconds(defaultExpireTime)) // 设置 key为string序列化 .serializeKeysWith(RedisSerializationContext.SerializationPair.fromSerializer(new StringRedisSerializer())) // 设置value为json序列化 .serializeValuesWith(RedisSerializationContext.SerializationPair.fromSerializer(new GenericJackson2JsonRedisSerializer())) // 不缓存空值 .disableCachingNullValues(); Set&lt;String&gt; cacheNames = new HashSet&lt;&gt;(); cacheNames.add(userCacheName); // 对每个缓存空间应用不同的配置 Map&lt;String, RedisCacheConfiguration&gt; configMap = new HashMap&lt;&gt;(); configMap.put(userCacheName, defaultCacheConfig.entryTtl(Duration.ofSeconds(userCacheExpireTime))); RedisCacheManager cacheManager = RedisCacheManager.builder(lettuceConnectionFactory) .cacheDefaults(defaultCacheConfig) .initialCacheNames(cacheNames) .withInitialCacheConfigurations(configMap) .build(); return cacheManager; &#125;&#125; 到此配置工作已经结束了 Spring Cache 使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Service@CacheConfig(cacheNames="user")// cacheName 是一定要指定的属性，可以通过 @CacheConfig 声明该类的通用配置public class UserService &#123; /** * 将结果缓存，当参数相同时，不会执行方法，从缓存中取 * * @param id * @return */ @Cacheable(key = "#id") public User findUserById(Integer id) &#123; System.out.println("===&gt; findUserById(id), id = " + id); return new User(id, "taven"); &#125; /** * 将结果缓存，并且该方法不管缓存是否存在，每次都会执行 * * @param user * @return */ @CachePut(key = "#user.id") public User update(User user) &#123; System.out.println("===&gt; update(user), user = " + user); return user; &#125; /** * 移除缓存，根据指定key * * @param user */ @CacheEvict(key = "#user.id") public void deleteById(User user) &#123; System.out.println("===&gt; deleteById(), user = " + user); &#125; /** * 移除当前 cacheName下所有缓存 * */ @CacheEvict(allEntries = true) public void deleteAll() &#123; System.out.println("===&gt; deleteAll()"); &#125;&#125; 注解 作用 @Cacheable | 将方法的结果缓存起来，下一次方法执行参数相同时，将不执行方法，返回缓存中的结果@CacheEvict | 移除指定缓存@CachePut | 标记该注解的方法总会执行，根据注解的配置将结果缓存@Caching | 可以指定相同类型的多个缓存注解，例如根据不同的条件@CacheConfig | 类级别注解，可以设置一些共通的配置，@CacheConfig(cacheNames=&quot;user&quot;), 代表该类下的方法均使用这个cacheNames 下面详细讲一下每个注解的作用和可选项。 Spring Cache 注解@EnableCaching 做了什么 @EnableCaching 注释触发后置处理器, 检查每一个Spring bean 的 public 方法是否存在缓存注解。如果找到这样的一个注释, 自动创建一个代理拦截方法调用和处理相应的缓存行为。 常用缓存注解简述@Cacheable将方法的结果缓存，必须要指定一个 cacheName（缓存空间）12@Cacheable(&quot;books&quot;)public Book findBook(ISBN isbn) &#123;...&#125; 默认 cache key缓存的本质还是以 key-value 的形式存储的，默认情况下我们不指定key的时候 ，使用 SimpleKeyGenerator 作为key的生成策略 如果没有给出参数，则返回SimpleKey.EMPTY。 如果只给出一个Param，则返回该实例。 如果给出了更多的Param，则返回包含所有参数的SimpleKey。 注意：当使用默认策略时，我们的参数需要有 有效的hashCode()和equals()方法 自定义 cache key12345678@Cacheable(cacheNames=&quot;books&quot;, key=&quot;#isbn&quot;)public Book findBook(ISBN isbn, boolean checkWarehouse, boolean includeUsed)@Cacheable(cacheNames=&quot;books&quot;, key=&quot;#isbn.rawNumber&quot;)public Book findBook(ISBN isbn, boolean checkWarehouse, boolean includeUsed)@Cacheable(cacheNames=&quot;books&quot;, key=&quot;T(someType).hash(#isbn)&quot;)public Book findBook(ISBN isbn, boolean checkWarehouse, boolean includeUsed) 如上，配合Spring EL 使用，下文会详细介绍 Spring EL 对 Cache 的支持 指定对象 指定对象中的属性 某个类的某个静态方法 自定义 keyGenerator12@Cacheable(cacheNames=&quot;books&quot;, keyGenerator=&quot;myKeyGenerator&quot;)public Book findBook(ISBN isbn, boolean checkWarehouse, boolean includeUsed) 实现 KeyGenerator接口可以自定义 cache key 的生成策略 自定义 cacheManager12@Cacheable(cacheNames=&quot;books&quot;, cacheManager=&quot;anotherCacheManager&quot;) public Book findBook(ISBN isbn) &#123;...&#125; 当我们的项目包含多个缓存管理器时，可以指定具体的缓存管理器，作为缓存解析 同步缓存在多线程环境中，可能会出现相同的参数的请求并发调用方法的操作，默认情况下，spring cache 不会锁定任何东西，相同的值可能会被计算几次，这就违背了缓存的目的 对于这些特殊情况，可以使用sync属性。此时只有一个线程在处于计算，而其他线程则被阻塞，直到在缓存中更新条目为止。12@Cacheable(cacheNames=&quot;foos&quot;, sync=true) public Foo executeExpensiveOperation(String id) &#123;...&#125; 条件缓存 condition: 什么情况缓存，condition = true 时缓存，反之不缓存 unless: 什么情况不缓存，unless = true 时不缓存，反之缓存12345@Cacheable(cacheNames=&quot;book&quot;, condition=&quot;#name.length() &lt; 32&quot;) public Book findBook(String name)@Cacheable(cacheNames=&quot;book&quot;, condition=&quot;#name.length() &lt; 32&quot;, unless=&quot;#result?.hardback&quot;)public Optional&lt;Book&gt; findBook(String name) Spring EL 对 Cache 的支持 Name Location Description Example methodName Root object 被调用的方法的名称 #root.methodName method Root object 被调用的方法 #root.method.name target Root object 当前调用方法的对象 #root.target targetClass Root object 当前调用方法的类 #root.targetClass args Root object 当前方法的参数 #root.args[0] caches Root object 当前方法的缓存集合 #root.caches[0].name Argument name Evaluation context 当前方法的参数名称 #iban or #a0 (you can also use #p0 or #p&lt;#arg&gt; notation as an alias). result Evaluation context 方法返回的结果(要缓存的值)。只有在 unless 、@CachePut(用于计算键)或@CacheEvict(beforeInvocation=false)中才可用.对于支持的包装器(例如Optional)，#result引用的是实际对象，而不是包装器 #result @CachePut这个注解和 @Cacheable 有点类似，都会将结果缓存，但是标记 @CachePut 的方法每次都会执行，目的在于更新缓存，所以两个注解的使用场景完全不同。@Cacheable 支持的所有配置选项，同样适用于@CachePut 12@CachePut(cacheNames=&quot;book&quot;, key=&quot;#isbn&quot;)public Book updateBook(ISBN isbn, BookDescriptor descriptor) 需要注意的是，不要在一个方法上同时使用@Cacheable 和 @CachePut @CacheEvict用于移除缓存 可以移除指定key 声明 allEntries=true移除该CacheName下所有缓存 声明beforeInvocation=true 在方法执行之前清除缓存，无论方法执行是否成功12345@CacheEvict(cacheNames=&quot;book&quot;, key=&quot;#isbn&quot;)public Book updateBook(ISBN isbn, BookDescriptor descriptor)@CacheEvict(cacheNames=&quot;books&quot;, allEntries=true) public void loadBooks(InputStream batch) @Caching可以让你在一个方法上嵌套多个相同的Cache 注解（@Cacheable, @CachePut, @CacheEvict），分别指定不同的条件12@Caching(evict = &#123; @CacheEvict(&quot;primary&quot;), @CacheEvict(cacheNames=&quot;secondary&quot;, key=&quot;#p0&quot;) &#125;)public Book importBooks(String deposit, Date date) @CacheConfig类级别注解，用于配置一些共同的选项（当方法注解声明的时候会被覆盖），例如 CacheName。 支持的选项如下：123456789101112@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface CacheConfig &#123; String[] cacheNames() default &#123;&#125;; String keyGenerator() default &quot;&quot;; String cacheManager() default &quot;&quot;; String cacheResolver() default &quot;&quot;;&#125; 参考： https://docs.spring.io/spring/docs/current/spring-framework-reference/integration.html#cache-annotations-cacheable https://spring.io/guides/gs/caching/ 本文demo： https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/springboot-redis]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>SpringBoot</tag>
        <tag>Cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Shiro 一》SpringBoot+Shiro 构建Web工程]]></title>
    <url>%2Fpost%2F9d0d6a3a.html</url>
    <content type="text"><![CDATA[Shiro 一款简单易用，功能强大的安全框架，帮助我们安全高效的构建企业级应用。之前几个项目都用到过 Shiro，最近抽空梳理了一下，分享一些经验。 本文demo：戳这里本文demo选型：thymeleaf, springboot 2, shiro, ehcachePS:如果我不拖延的话，估计还是会有后续的 :） Shiro 能做什么 认证：登录用户的认证 权限：基于角色和权限的访问权限（url权限），以及颗粒化权限控制（按钮权限） 加密技术：Shiro的crypto包中包含了一系列的易于理解和使用的加密、哈希（aka摘要）辅助类 session管理：可在web容器以及 EJB容器中使用 session，可扩展 （例如我们可以通过重写 sessionDao 将 session 存储到数据库中） RememberMe：基于cookie的记住我服务 Shiro 常用组件介绍 Subject：Subject其实代表的就是当前正在执行操作的用户，只不过因为“User”一般指代人，但是一个“Subject”可以是人，也可以是任何的第三方系统，服务账号等任何其他正在和当前系统交互的第三方软件系统。 所有的Subject实例都被绑定到一个SecurityManager，如果你和一个Subject交互，所有的交互动作都会被转换成Subject与SecurityManager的交互 SecurityManager：Shiro的核心，他主要用于协调Shiro内部各种安全组件，不过我们一般不用太关心SecurityManager，对于应用程序开发者来说，主要还是使用Subject的API来处理各种安全验证逻辑 Realm：这是用于连接Shiro和客户系统的用户数据的桥梁。一旦Shiro真正需要访问各种安全相关的数据（比如使用用户账户来做用户身份验证以及权限验证）时，他总是通过调用系统配置的各种Realm来读取数据 关于Shiro 的其余核心组件参考 Shiro 官网 或者 Shiro的架构 本文不做过多的阐述 Shiro 是如何工作的简单来讲的话，在Spring项目中 Shiro 会将他的所有组件注册到 SecurityManager中 再通过将 SecurityManager 注册到 ShiroFilterFactoryBean（这个类实现了Spring 的BeanPostProcessor会预先加载） 中， 最后以 filter 的形式注册到Spring容器（实现了Spring的FactoryBean，构造一个 filter 注册到 Spring 容器中），实现用户权限的管理。 Shiro 如何集成 shiro 所需依赖，完整见demo源码 12345678910111213141516171819202122&lt;!--shiro--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-ehcache&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 基于thymeleaf的shiro扩展 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.theborakompanioni&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-extras-shiro&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt; ShiroConfig 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117@Configurationpublic class ShiroConfig &#123; private static final Logger log = LoggerFactory.getLogger(ShiroConfig.class); @Bean public ShiroFilterFactoryBean shiroFilterFactoryBean(DefaultWebSecurityManager securityManager) &#123; ShiroFilterFactoryBean shiroFilter = new ShiroFilterFactoryBean(); shiroFilter.setSecurityManager(securityManager); Map&lt;String, String&gt; chainDefinition = new LinkedHashMap&lt;&gt;(); // 静态资源与登录请求不拦截 chainDefinition.put("/js/**", "anon"); chainDefinition.put("/css/**", "anon"); chainDefinition.put("/img/**", "anon"); chainDefinition.put("/layui/**", "anon"); chainDefinition.put("/login", "anon"); chainDefinition.put("/login.html", "anon"); // 用户为授权通过认证 &amp;&amp; 包含'admin'角色 chainDefinition.put("/admin/**", "authc, roles[super_admin]"); // 用户为授权通过认证或者RememberMe &amp;&amp; 包含'document:read'权限 chainDefinition.put("/docs/**", "user, perms[document:read]"); // 用户访问所有请求 授权通过 || RememberMe chainDefinition.put("/**", "user"); shiroFilter.setFilterChainDefinitionMap(chainDefinition); // 当 用户身份失效时重定向到 loginUrl shiroFilter.setLoginUrl("/login.html"); // 用户登录后默认重定向请求 shiroFilter.setSuccessUrl("/index.html"); return shiroFilter; &#125; @Bean public Realm realm() &#123; ShiroRealm realm = new ShiroRealm(); realm.setCredentialsMatcher(credentialsMatcher()); realm.setCacheManager(ehCacheManager()); return realm; &#125; @Bean public CacheManager ehCacheManager() &#123; EhCacheManager cacheManager = new EhCacheManager(); cacheManager.setCacheManagerConfigFile("classpath:ehcache.xml"); return cacheManager; &#125; @Bean public CredentialsMatcher credentialsMatcher() &#123; AuthCredentialsMatcher credentialsMatcher = new AuthCredentialsMatcher(ehCacheManager()); credentialsMatcher.setHashAlgorithmName(AuthCredentialsMatcher.HASH_ALGORITHM_NAME); credentialsMatcher.setHashIterations(AuthCredentialsMatcher.HASH_ITERATIONS); credentialsMatcher.setStoredCredentialsHexEncoded(true); return credentialsMatcher; &#125; @Bean public DefaultWebSecurityManager securityManager() &#123; log.debug("--------------shiro已经加载----------------"); DefaultWebSecurityManager manager = new DefaultWebSecurityManager(); manager.setCacheManager(ehCacheManager()); manager.setRealm(realm()); manager.setRememberMeManager(rememberMeManager()); return manager; &#125; @Bean public RememberMeManager rememberMeManager() &#123; CookieRememberMeManager cookieRememberMeManager = new CookieRememberMeManager(); //rememberMe cookie加密的密钥 建议每个项目都不一样 默认AES算法 密钥长度(128 256 512 位) cookieRememberMeManager.setCipherKey(Base64.decode("2AvVhdsgUs0FSA3SDFAdag==")); cookieRememberMeManager.setCookie(rememberMeCookie()); return cookieRememberMeManager; &#125; @Bean public SimpleCookie rememberMeCookie()&#123; //这个参数是cookie的名称，对应前端的checkbox的name = rememberMe SimpleCookie simpleCookie = new SimpleCookie("rememberMe"); //&lt;!-- 记住我cookie生效时间30天 ,单位秒;--&gt; simpleCookie.setMaxAge(259200); return simpleCookie; &#125; /** * Shiro生命周期处理器: * 用于在实现了Initializable接口的Shiro bean初始化时调用Initializable接口回调(例如:UserRealm) * 在实现了Destroyable接口的Shiro bean销毁时调用 Destroyable接口回调(例如:DefaultSecurityManager) */ @Bean public LifecycleBeanPostProcessor lifecycleBeanPostProcessor() &#123; return new LifecycleBeanPostProcessor(); &#125; /** * 启用shrio授权注解拦截方式，AOP式方法级权限检查 */ @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor(DefaultWebSecurityManager securityManager) &#123; AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor = new AuthorizationAttributeSourceAdvisor(); authorizationAttributeSourceAdvisor.setSecurityManager(securityManager); return authorizationAttributeSourceAdvisor; &#125; /** * thymeleaf的shiro扩展 * * @return */ @Bean public ShiroDialect shiroDialect() &#123; return new ShiroDialect(); &#125;&#125; 以上基本是Spring项目集成 Shiro 的通用配置，下面针对上述的几个Bean 聊一聊1. ShiroFilterFactoryBean：用于定义 请求的拦截规则, Shiro为我们默认提供了一些选项，常用如下 anon: 请求不拦截 authc: 要求用户必须认证通过 user: 要求用户为记住我状态 roles[xxx]: 要求用户必须满足 xxx 角色 perms[xxx]: 要求用户必须满足 xxx 权限其实上述每一个都对应了一个 Shiro 过滤器 Filter Name Class anon org.apache.shiro.web.filter.authc.AnonymousFilter authc org.apache.shiro.web.filter.authc.FormAuthenticationFilter authcBasic org.apache.shiro.web.filter.authc.BasicHttpAuthenticationFilter logout org.apache.shiro.web.filter.authc.LogoutFilter noSessionCreation org.apache.shiro.web.filter.session.NoSessionCreationFilter perms | org.apache.shiro.web.filter.authz.PermissionsAuthorizationFilterport| org.apache.shiro.web.filter.authz.PortFilterrest| org.apache.shiro.web.filter.authz.HttpMethodPermissionFilterroles| org.apache.shiro.web.filter.authz.RolesAuthorizationFilterssl| org.apache.shiro.web.filter.authz.SslFilteruser| org.apache.shiro.web.filter.authc.UserFilter 我们也可以自定义 过滤器来实现拦截 2. Realm：上面提到过Realm是用于连接Shiro和客户系统的用户数据的桥梁, 我们通过实现AuthorizingRealm 来提供用户认证和授权两个API 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class ShiroRealm extends AuthorizingRealm &#123; private static final Logger log = LoggerFactory.getLogger(AuthorizingRealm.class); @Autowired @Lazy // 这里lazy 是有必要的, shiro组件会预先加载，导致依赖的bean 没有生成代理对象（AOP失效） private UserService userService; /** * 认证 * * @param authenticationToken * @return * @throws AuthenticationException */ @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException &#123; String username = (String) authenticationToken.getPrincipal(); if (log.isDebugEnabled()) &#123; log.debug(String.format("user:%s executing doGetAuthenticationInfo", username)); &#125; User user = userService.getUserByUsername(username); if (user == null) &#123; throw new UnknownAccountException(); &#125; if (Constant.IS_LOCK.equals(user.getIsLock())) &#123; throw new LockedAccountException(); &#125; // ShiroUser 作为实际的 principal ShiroUser shiroUser = new ShiroUser(); BeanUtils.copyProperties(user, shiroUser); // SimpleAuthenticationInfo(Object principal, Object credentials, String realmName) // principal 会被封装到 subject 中 // shiro 默认会把我们的 credentials (也就是password) 和 token 中的作对比，所以我们可以不用做密码校验 ByteSource salt = ByteSource.Util.bytes(user.getUsername()); SimpleAuthenticationInfo info = new SimpleAuthenticationInfo(shiroUser, user.getPassword(), salt, getName()); if (log.isDebugEnabled()) &#123; log.debug(String.format("user:%s executed doGetAuthenticationInfo", username)); &#125; return info; &#125; /** * 授权 * * @param principalCollection * @return */ @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) &#123; ShiroUser shiroUser = (ShiroUser) principalCollection.getPrimaryPrincipal(); if (log.isDebugEnabled()) &#123; log.debug(String.format("user:%s executing doGetAuthorizationInfo", shiroUser.getUsername())); &#125; AuthorizationDTO authorizationDTO = userService.getRolesAndPermissions(shiroUser.getId()); SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); info.addRoles(authorizationDTO.getRoleCodeSet()); info.addStringPermissions(authorizationDTO.getPermissionCodeSet()); if (log.isDebugEnabled()) &#123; log.debug(String.format("user:%s executed doGetAuthorizationInfo", shiroUser.getUsername())); &#125; return info; &#125;&#125; doGetAuthenticationInfo : 认证方法，在执行 subject.login(token);后，Shiro认证器会读取 Realm 中的该方法获取 AuthenticationInfo对象（认证信息），包含principal（我们存储在shiro subject中的对象），credentials （密码）。 doGetAuthorizationInfo: 授权方法，在需要校验用户访问权限的时候，Shiro授权器会读取 Realm 中的该方法获取 AuthorizationInfo对象（授权信息）读取DB后，可以通过 addRoles(roleCollection) 和 addStringPermissions(permCollection) 设置当前用户的角色和权限。Shiro 在拿到这个权限信息后，会去找缓存管理器，以当前 subject 的 principal 作为key 缓存起来。 3. CredentialsMatcher: 密码匹配器，用于匹配 doGetAuthenticationInfo 方法返回的 credentials 和 subject.login(token);时的 token 中的 password是否一致。常用的实现有 SimpleCredentialsMatcher（默认是该实现）、HashedCredentialsMatcher （该实现可以进行加密匹配） 4. DefaultWebSecurityManager：如上述，用于协调Shiro内部各种安全组件，我们需要将我们扩展的bean 注册到 SecurityManager 中 5. RememberMeManager：开启该组件后使用记住我服务， token 中 rememberMe 为 true 时，登录成功之后会创建RememberMe cookie。 其余参考上文代码注释 关于 thymeleaf-extras-shiroShiro 默认支持在 jsp 中使用 shiro标签。但是想在 thymeleaf 中使用 Shiro 标签呢？ 使用 thymeleaf-extras-shiro 完美解决 thymeleaf 颗粒化权限控制1234567891011你好, &lt;span th:text=&quot;$&#123;principal&#125;&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;p shiro:hasRole=&quot;super_admin&quot;&gt;当前角色超级管理员&lt;/p&gt;&lt;button shiro:hasPermission=&quot;&apos;sys:user:add&apos;&quot;&gt;添加&lt;/button&gt;&lt;button shiro:hasPermission=&quot;&apos;sys:user:update&apos;&quot;&gt;编辑&lt;/button&gt;&lt;button shiro:hasPermission=&quot;&apos;sys:user:lock&apos;&quot;&gt;冻结&lt;/button&gt;&lt;div shiro:hasAllPermissions=&quot;&apos;sys:user:add, sys:user:update, sys:user:lock&apos;&quot;&gt; &lt;span&gt;满足所有权限时显示&lt;/span&gt;&lt;/div&gt;&lt;div shiro:hasAnyPermissions=&quot;&apos;sys:user:add, sys:user:update, sys:user:lock&apos;&quot;&gt; &lt;span&gt;满足一个权限即可显示&lt;/span&gt;&lt;/div&gt; 更多用法参考Github 文档：https://github.com/theborakompanioni/thymeleaf-extras-shiro 本文demohttps://gitee.com/yintianwen7/taven-springboot-learning/tree/master/springboot-shiro如果你发现我的文章或者demo中存在问题，请联系我]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>Shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-又一次事务不生效的排查]]></title>
    <url>%2Fpost%2F836297d7.html</url>
    <content type="text"><![CDATA[上一篇关于事务的 解决 spring service 调用当前类方法事务不生效 正文今天在工作中突然发现标记了 @Transactional 的方法，事务并没有生效。在检查了业务层是否 try catch异常，MySQL存储引擎之后。心态已近崩溃的边缘：)这个时候突然发现了一个神奇的现象！两个@Service 都标记了 @Transactional，userService 并没有生成代理对象，也就导致了事务不生效。 继续排查，最后锁定了凶手 — BeanPostProcessor 先看一下官方描述1234567891011121314/** * Factory hook that allows for custom modification of new bean instances, * e.g. checking for marker interfaces or wrapping them with proxies. * * &lt;p&gt;ApplicationContexts can autodetect BeanPostProcessor beans in their * bean definitions and apply them to any beans subsequently created. * Plain bean factories allow for programmatic registration of post-processors, * applying to all beans created through this factory. * * &lt;p&gt;Typically, post-processors that populate beans via marker interfaces * or the like will implement &#123;@link #postProcessBeforeInitialization&#125;, * while post-processors that wrap beans with proxies will normally * implement &#123;@link #postProcessAfterInitialization&#125;. * 简单的来说，BeanPostProcessor是Spring 给我们提供的一个扩展接口1234567public interface BeanPostProcessor &#123; // bean实例化方法调用前被调用 Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; // bean实例化方法调用后被调用 Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException;&#125; 分析 ApplicationContexts 检测到 BeanPostProcessor 之后会将他应用于随后创建的所有 bean，所以 BeanPostProcessor 会在其他Bean的之前加载，但是随之引发的问题的就是 BeanPostProcessor 实现类所引用的Bean 没有被代理，只是被托管到 IOC 容器中。 我的项目里引用了 Shiro，而 Shiro 的所有组件最终会被封装到 ShiroFilterFactoryBean （该类实现了 BeanPostProcessor）中，而 Shiro 的 Realm 中又依赖了我们的 Service，该 Service 预先加载，导致没有被代理 解决方案1234567public class AuthRealm extends AuthorizingRealm &#123; @Autowired @Lazy // 延迟加载 private UserService userService;&#125; 总结这里给大家简单介绍引起事务不生效的几个原因 try catch 捕获 Service 运行时异常，因为 Spring 默认在捕获到 RuntimeException 时回滚 MySQL存储引擎 InnoDB 是支持事务的，而 MyISAM 不支持 由于各种原因没有使用或者 Spring 没有生成代理对象]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>AOP</tag>
        <tag>Shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-insert-or-update]]></title>
    <url>%2Fpost%2F73b0b9b2.html</url>
    <content type="text"><![CDATA[我们经常会遇到类似的业务场景，插入一条数据如果他不存在则执行 insert ，当这条记录存在的时候，我们去 update 他的一些属性（或者什么都不做）。 解决方案： 使用 ON DUPLICATE KEY UPDATE在 主键 或者 唯一约束 重复时，执行更新操作。 使用 REPLACE INTO在 主键 或者 唯一约束 重复时，先 delete 再 insert。 ON DUPLICATE KEY UPDATE 创建表，建立唯一约束，准备一条数据123456789101112CREATE TABLE `stu_class_ref` ( `id` varchar(30) NOT NULL, `stu_id` varchar(30) DEFAULT NULL, `class_id` varchar(30) DEFAULT NULL, `note` varchar(100) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `stu_id` (`stu_id`,`class_id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;INSERT INTO `test`.`stu_class_ref`(`id`, `stu_id`, `class_id`, `note`) VALUES (&apos;001&apos;, &apos;zhangsan&apos;, &apos;yuwen&apos;, NULL); 使用 ON DUPLICATE KEY UPDATE 123456INSERT INTO `test`.`stu_class_ref`(`id`, `stu_id`, `class_id`, `note`) VALUES (UUID_SHORT(), &apos;zhangsan&apos;, &apos;yuwen&apos;, &apos;我喜欢语文:)&apos;)ON DUPLICATE KEY UPDATE note = &apos;我喜欢语文:)&apos;&gt; Affected rows: 2&gt; 时间: 0.042s Affected rows: 2，MySQL 检查插入的行是否会产生重复键错误，如果会则执行update 如果想要引用 VALUES 中的值，参考如下123456INSERT INTO `test`.`stu_class_ref`(`id`, `stu_id`, `class_id`, `note`) VALUES (UUID_SHORT(), &apos;zhangsan&apos;, &apos;yuwen&apos;, NULL)ON DUPLICATE KEY UPDATE note = VALUES(class_id)&gt; Affected rows: 2&gt; 时间: 0.006s REPLACE INTO MySQL 中 还有一个黑科技语法 REPLACE INTO1234REPLACE INTO `stu_class_ref`(`id`, `stu_id`, `class_id`, `note`) VALUES (UUID_SHORT(), &apos;zhangsan&apos;, &apos;yuwen&apos;, NULL)&gt; Affected rows: 2&gt; 时间: 0.004s REPLACE INTO 就比较简单粗暴了，他会先执行delete 操作，然后insert ON DUPLICATE KEY UPDATE 与 REPLACE INTO 再来创建一张表, 创建三个唯一约束, 插入三条数据123456789101112131415161718CREATE TABLE `interesting` ( `id` varchar(30) NOT NULL, `uni_a` varchar(30) DEFAULT NULL, `uni_b` varchar(30) DEFAULT NULL, `uni_c` varchar(30) DEFAULT NULL, `version` int(11) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `uni_a` (`uni_a`) USING BTREE, UNIQUE KEY `uni_b` (`uni_b`) USING BTREE, UNIQUE KEY `uni_c` (`uni_c`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;INSERT INTO `test`.`interesting`(`id`, `uni_a`, `uni_b`, `uni_c`, `version`) VALUES (&apos;1&apos;, &apos;a&apos;, &apos;a&apos;, &apos;a&apos;, NULL);INSERT INTO `test`.`interesting`(`id`, `uni_a`, `uni_b`, `uni_c`, `version`) VALUES (&apos;2&apos;, &apos;b&apos;, &apos;b&apos;, &apos;b&apos;, NULL);INSERT INTO `test`.`interesting`(`id`, `uni_a`, `uni_b`, `uni_c`, `version`) VALUES (&apos;3&apos;, &apos;c&apos;, &apos;c&apos;, &apos;c&apos;, NULL); 执行 ON DUPLICATE KEY UPDATE12345INSERT INTO `interesting`(`id`, `uni_a`, `uni_b`, `uni_c`, `version`) VALUES (UUID_SHORT(), &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, NULL)ON DUPLICATE KEY UPDATE version = 666&gt; Affected rows: 2&gt; 时间: 0.049s Affected rows: 2 但是其实三条主键都有冲突了 再看一下 REPLACE INTO1234REPLACE INTO `interesting`(`id`, `uni_a`, `uni_b`, `uni_c`, `version`) VALUES (UUID_SHORT(), &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, NULL)&gt; Affected rows: 4&gt; 时间: 0.026s Affected rows: 4 REPLACE INTO 将三条有冲突的全部delete 然后 insert ####总结： ON DUPLICATE KEY UPDATE 只会对所匹配的第一行进行update, REPLACE INTO 会对所有匹配行进行delete, insert 所以应避免对有多个唯一索引的表使用]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 使用 hibernate validator]]></title>
    <url>%2Fpost%2F81ef5d67.html</url>
    <content type="text"><![CDATA[本文将全面的介绍如何使用 validator 进行数据校验 本文源码：https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/springboot-validate 准备工作我们只需要引入 spring-boot-starter-web包即可使用 常用注解 注解 释义 @Valid 被注释的元素是一个对象，需要检查此对象的所有字段值 @Null 被注释的元素必须为 null @NotNull 被注释的元素必须不为 null @NotEmpty 被注释的字符串的必须非空, 即不为null, “” @NotBlank 被注释的字符串的必须非空, 即不为null, “”, “ “ @AssertTrue 被注释的元素必须为 true @AssertFalse 被注释的元素必须为 false @Min(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @Size(max, min) 被注释的元素的大小必须在指定的范围内 @Digits (integer, fraction) 被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past 被注释的元素必须是一个过去的日期 @Future 被注释的元素必须是一个将来的日期 @Pattern(value) 被注释的元素必须符合指定的正则表达式 @Email 被注释的元素必须是电子邮箱地址 @Length(min=, max=) 被注释的字符串的大小必须在指定的范围内 @Range(min=, max=) 被注释的元素必须在合适的范围 简单的实体校验123456789101112131415161718public class CardDTO &#123; @NotBlank private String cardId; @Size(min = 10, max = 10) @NotNull private String cardNum; // 卡号 @Past @NotNull private Date createDate; @Range(max = 3) private String cardType; // 省略get set&#125; 123456789@RestControllerpublic class UserController &#123; @PostMapping(&quot;simple&quot;) public Object simple(@RequestBody @Valid CardDTO cardDTO) &#123; return cardDTO; &#125;&#125; 实体属性上添加校验注解 controller 方法 参数前 使用@Valid 即可 复杂的实体校验嵌套实体校验123456789101112131415public class UserDTO &#123; @NotBlank private String userId; @NotBlank private String username; private String password; @Valid private List&lt;CardDTO&gt; cardList; //省略 get set&#125; controller 写法 同上，只是在 UserDTO cardList 属性上标记@Valid 注解 即可。 List 校验 我们需要像 嵌套校验 时一样，对List&lt;CardDTO&gt; 做一层封装 123456789101112131415public class ValidList&lt;E&gt; implements List&lt;E&gt; &#123; @Valid private List&lt;E&gt; list = new ArrayList&lt;&gt;(); public List&lt;E&gt; getList() &#123; return list; &#125; public void setList(List&lt;E&gt; list) &#123; this.list = list; &#125; // 省略了 实现方法&#125; 重写实现方法完全使用 this.list.xxx()Gitee:spring 会将数据封装到我们定义的 list 属性中，又将属性声明了 @Valid 使得 hibernate validator 可以为我们做校验！ 使用 @Validated 分组校验12345public interface Insert &#123;&#125;public interface Update &#123;&#125; 定义两个接口 12345678910111213public class GroupCardDTO &#123; @NotBlank(groups = &#123;Update.class&#125;) private String id; @NotBlank(groups = &#123;Insert.class&#125;) private String cardNum; @NotNull(groups = &#123;Insert.class, Update.class&#125;) private Integer cardType; //省略 get set&#125; 实体标记的注解中添加 group 属性 1234@PostMapping(&quot;insert_card&quot;)public Object insert_card(@RequestBody @Validated(Insert.class) GroupCardDTO card)&#123; return card;&#125; 使用 @Validated(xxx.class) 标记参数，完成分组校验！ 自定义注解校验当 validator 提供的注解无法满足我们的业务需求，可以通过自定义的方式来实现校验。 需求：校验某字符串必须为大写或者小写1234public enum CaseMode &#123; UPPER, LOWER&#125; 定义一个枚举类 12345678910111213141516171819import javax.validation.Constraint;import javax.validation.Payload;import java.lang.annotation.*;@Target( &#123; ElementType.FIELD &#125;)@Retention(RetentionPolicy.RUNTIME)@Constraint(validatedBy = CheckCaseValidator.class)@Documentedpublic @interface CheckCase &#123; String message() default ""; Class&lt;?&gt;[] groups() default &#123;&#125;; Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;; CaseMode value() default CaseMode.LOWER;&#125; 定义注解 @Constraint 指定我们的校验逻辑实现类 12345678910111213141516171819202122232425262728293031import javax.validation.ConstraintValidator;import javax.validation.ConstraintValidatorContext;public class CheckCaseValidator implements ConstraintValidator&lt;CheckCase, String&gt; &#123; private CaseMode caseMode; @Override public void initialize(CheckCase constraintAnnotation) &#123; this.caseMode = constraintAnnotation.value(); &#125; @Override public boolean isValid(String value, ConstraintValidatorContext context) &#123; if (value == null || "".equals(value.trim())) &#123; return false; &#125; switch (this.caseMode) &#123; case LOWER: return value.equals(value.toLowerCase()); case UPPER: return value.equals(value.toUpperCase()); default: return false; &#125; &#125;&#125; initialize() 初始化时执行，可以用来获取注解中的属性 isValid() 实现我们的校验逻辑 备注 我们自定义的注解依然支持 @Validated group 分组 手动使用 validator 校验123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import org.hibernate.validator.internal.engine.path.PathImpl;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Set;import javax.validation.ConstraintViolation;import javax.validation.Validation;import javax.validation.Validator;import javax.validation.ValidatorFactory;public class ValidateUtil &#123; /** * 校验实体类 * * @param t * @return */ public static &lt;T&gt; List&lt;Map&gt; validate(T t) &#123; //定义返回错误List List&lt;Map&gt; errList = new ArrayList&lt;&gt;(); Map&lt;String, String&gt; errorMap; ValidatorFactory factory = Validation.buildDefaultValidatorFactory(); Validator validator = factory.getValidator(); Set&lt;ConstraintViolation&lt;T&gt;&gt; errorSet = validator.validate(t); for (ConstraintViolation&lt;T&gt; c : errorSet) &#123; errorMap = new HashMap&lt;&gt;(); errorMap.put("field", c.getPropertyPath().toString()); //获取发生错误的字段 errorMap.put("msg", c.getMessage()); //获取校验信息 errList.add(errorMap); &#125; return errList; &#125; /** * 使用 ValidList 校验List, 返回对应索引和错误消息 * * @param t * @param &lt;T&gt; * @return */ public static &lt;T&gt; List&lt;Map&gt; validateList(T t) &#123; //定义返回错误List List&lt;Map&gt; errList = new ArrayList&lt;&gt;(); Map&lt;String, Object&gt; errorMap; ValidatorFactory factory = Validation.buildDefaultValidatorFactory(); Validator validator = factory.getValidator(); Set&lt;ConstraintViolation&lt;T&gt;&gt; errorSet = validator.validate(t); for (ConstraintViolation&lt;T&gt; c : errorSet) &#123; errorMap = new HashMap&lt;&gt;(); int index = ((PathImpl) c.getPropertyPath()).getLeafNode().getIndex(); errorMap.put("index", index); // 当前索引 errorMap.put("field", c.getPropertyPath().toString()); //获取发生错误的字段 errorMap.put("msg", c.getMessage()); //获取校验信息 errList.add(errorMap); &#125; return errList; &#125;&#125; 本节源码https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/springboot-validate]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JWT 鉴权]]></title>
    <url>%2Fpost%2F606cdcbb.html</url>
    <content type="text"><![CDATA[JWT 是什么 JSON Web Token（JWT）是一个开放式标准（RFC 7519），它定义了一种紧凑且自包含的方式，用于在各方之间以JSON对象安全传输信息。这些信息可以通过数字签名进行验证和信任。 JWT 的组成JWT 的格式为 xxx.yyy.zzz。包含 Header（头部），Payload（负载），Signature（签名）三部分。 Header通常会声明使用的加密算法和token类型 1234&#123; &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot;&#125; PayloadPayload 包含 Claims，Claim是一些实体（一般都是用户）的状态和额外的数据组成。JWT 为我们预定义了一些 Claims，例如：iss (issuer), exp (expiration time), sub (subject), aud (audience), and others.但是并不要求我们强制使用，我们也可以根据需求自定义 Claim 12345&#123; &quot;sub&quot;: &quot;1234567890&quot;, &quot;name&quot;: &quot;Taven&quot;, &quot;admin&quot;: true&#125; Signature 1234HMACSHA256( base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload), secret) 这段伪代码已经很好的讲解了，签名是如何计算的。服务端提供一个secret，将header、payload 进行base64编码，然后使用header中声明的算法计算签名。 JWT 与 session 对比 性能：JWT 轻量级，而session 会占用大量服务端内存； 部署：使用session的系统 负载均衡需要考虑 session 共享，而JWT 不需要； 安全：安全性的话，小弟不敢多说，我觉得五五开吧，有人说 JWT 泄露的话，直接就可以登录了。但是使用 session 如果请求被拦截了都是一样的； 场景：APP 中由于没有cookie吧，多是使用token。而 web 应用的话，使用 token 和 session 都是可以的。 实现思路简单一句话就是，登录之后服务端给客户端颁发JWT，客户端将 JWT 放在请求头中，服务端 filter 校验http header中的JWT。 查阅了一些资料后发现一些分歧 方案1： 有的朋友认为服务端不需要存储 JWT，只在 filter 校验的时候解析无误，即认为 JWT 是可用的。在需要重置 JWT 的时候（例如注销），客户端主动删除JWT。这种做法引发的问题，例如在注销了之后，token 依然可用，由于服务端没有存储 token，无法去校验。于是乎有了方案2 方案2：服务端数据库中存储 JWT，filter 每次校验 token 与数据库中是否一致，这种做法虽然稳妥，但是在性能上一定是不如方案1的。 demohttps://gitee.com/yintianwen7/taven-springboot-learning/tree/master/spring-boot-jwt]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>JWT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot Atomikos 多数据源分布式事务]]></title>
    <url>%2Fpost%2F2727b3ac.html</url>
    <content type="text"><![CDATA[之前的 《spring 动态切换、添加数据源实现以及源码浅析》 中介绍了如何使用 spring 提供的 AbstractRoutingDataSource 配置多数据源，有了多数据源自然要管理事务的一致性。上篇文章中提到过配置多数据源的两种方式 使用AbstractRoutingDataSource 配置多个 SqlSessionFactory 前言粗略阅读了一下spring的源码，由于 spring 事务的机制，在开启事务之前spring 会去创建当前数据源的 事务object，直到事务提交，spring 都不会在乎你是否切换了数据源。这就导致了，使用 AbstractRouting DataSource 方式开启事务时，切换数据源不生效。关于如何解决这个问题，感兴趣的朋友可以去阅读一下：https://www.jianshu.com/p/61e8961c6154 本文只讨论上述第二种方式结合 atomikos 管理多数据源事务。 Atomikos来自：http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-jta.html Atomikos is a popular open source transaction manager which can be embedded into your Spring Boot application. You can use thespring-boot-starter-jta-atomikos Starter to pull in the appropriate Atomikos libraries. Spring Boot auto-configures Atomikos and ensures that appropriate depends-on settings are applied to your Spring beans for correct startup and shutdown ordering. By default, Atomikos transaction logs are written to a transaction-logs directory in your application’s home directory (the directory in which your application jar file resides). You can customize the location of this directory by setting a spring.jta.log-dir property in your application.properties file. Properties starting with spring.jta.atomikos.properties can also be used to customize the Atomikos UserTransactionServiceImp. See the AtomikosProperties Javadoc for complete details. 引入spring-boot-starter-jta-atomikos，spring boot 为我们自动配置Atomikos，我们可以通过 spring.jta.xxx 修改默认配置。 Talk is cheap. Show me the code demo源码：https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/spring-atomikos 添加 maven 依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jta-atomikos&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt; application.properties 1234567891011121314151617181920212223242526272829303132333435363738394041424344#spring.jta.log-dir=classpath:tx-logsspring.jta.transaction-manager-id=txManagerspring.datasource.druid.system-db.name=system-dbspring.datasource.druid.system-db.url=jdbc:mysql://localhost:3306/test1?useSSL=falsespring.datasource.druid.system-db.username=rootspring.datasource.druid.system-db.password=taven753spring.datasource.druid.system-db.initialSize=5spring.datasource.druid.system-db.minIdle=5spring.datasource.druid.system-db.maxActive=20spring.datasource.druid.system-db.maxWait=60000spring.datasource.druid.system-db.timeBetweenEvictionRunsMillis=60000spring.datasource.druid.system-db.minEvictableIdleTimeMillis=30000spring.datasource.druid.system-db.validationQuery=SELECT 1spring.datasource.druid.system-db.validationQueryTimeout=10000spring.datasource.druid.system-db.testWhileIdle=truespring.datasource.druid.system-db.testOnBorrow=falsespring.datasource.druid.system-db.testOnReturn=falsespring.datasource.druid.system-db.poolPreparedStatements=truespring.datasource.druid.system-db.maxPoolPreparedStatementPerConnectionSize=20spring.datasource.druid.system-db.filters=stat,wallspring.datasource.druid.system-db.connectionProperties=druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000spring.datasource.druid.system-db.useGlobalDataSourceStat=truespring.datasource.druid.business-db.name=business-dbspring.datasource.druid.business-db.url=jdbc:mysql://localhost:3306/test2?useSSL=falsespring.datasource.druid.business-db.username=rootspring.datasource.druid.business-db.password=taven753spring.datasource.druid.business-db.initialSize=5spring.datasource.druid.business-db.minIdle=5spring.datasource.druid.business-db.maxActive=20spring.datasource.druid.business-db.maxWait=60000spring.datasource.druid.business-db.timeBetweenEvictionRunsMillis=60000spring.datasource.druid.business-db.minEvictableIdleTimeMillis=30000spring.datasource.druid.business-db.validationQuery=SELECT 1spring.datasource.druid.business-db.validationQueryTimeout=10000spring.datasource.druid.business-db.testWhileIdle=truespring.datasource.druid.business-db.testOnBorrow=falsespring.datasource.druid.business-db.testOnReturn=falsespring.datasource.druid.business-db.poolPreparedStatements=truespring.datasource.druid.business-db.maxPoolPreparedStatementPerConnectionSize=20spring.datasource.druid.business-db.filters=stat,wallspring.datasource.druid.business-db.connectionProperties=druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000spring.datasource.druid.business-db.useGlobalDataSourceStat=true system 数据源的配置类 12345678910111213141516171819202122232425262728293031323334353637383940414243import javax.sql.DataSource;import org.apache.ibatis.session.SqlSessionFactory;import org.mybatis.spring.SqlSessionFactoryBean;import org.mybatis.spring.annotation.MapperScan;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.jta.atomikos.AtomikosDataSourceBean;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Primary;import com.gitee.taven.config.prop.SystemProperties;import com.gitee.taven.utils.PojoUtil;@Configuration@MapperScan(basePackages = SystemDataSourceConfig.PACKAGE, sqlSessionFactoryRef = "systemSqlSessionFactory")public class SystemDataSourceConfig &#123; static final String PACKAGE = "com.gitee.taven.mapper.system"; @Autowired private SystemProperties systemProperties; @Bean(name = "systemDataSource") @Primary public DataSource systemDataSource() &#123; AtomikosDataSourceBean ds = new AtomikosDataSourceBean(); ds.setXaProperties(PojoUtil.obj2Properties(systemProperties)); ds.setXaDataSourceClassName("com.alibaba.druid.pool.xa.DruidXADataSource"); ds.setUniqueResourceName("systemDataSource"); ds.setPoolSize(5); return ds; &#125; @Bean @Primary public SqlSessionFactory systemSqlSessionFactory() throws Exception &#123; SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); sqlSessionFactoryBean.setDataSource(systemDataSource()); return sqlSessionFactoryBean.getObject(); &#125; &#125; business 数据源的配置类同上 省略了 mybatis 代码，通过service 测试事务，抛出异常后，事务会回滚 1234567891011121314151617181920212223@Servicepublic class UserService &#123; @Autowired private UsersMapper usersMapper; @Autowired private UserInformationsMapper userInformationsMapper; @Transactional public void testJTA() &#123; Users u = new Users(); u.setUsername("hmj"); u.setPassword("hmjbest"); usersMapper.insertSelective(u); UserInformations ui = new UserInformations(); ui.setUserid(666l); ui.setEmail("dsb"); userInformationsMapper.insertSelective(ui); // int i = 10/0; &#125; &#125; demo源码：https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/spring-atomikos]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>多数据源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 多线程异步调用---提高程序执行效率]]></title>
    <url>%2Fpost%2F574b9274.html</url>
    <content type="text"><![CDATA[原文：https://spring.io/guides/gs/async-method/ 为什么要用异步？当需要调用多个服务时，使用传统的同步调用来执行时，是这样的 调用服务A 等待服务A的响应 调用服务B 等待服务B的响应 调用服务C 等待服务C的响应 根据从服务A、服务B和服务C返回的数据完成业务逻辑，然后结束 如果每个服务需要3秒的响应时间，这样顺序执行下来，可能需要9秒以上才能完成业务逻辑，但是如果我们使用异步调用 调用服务A 调用服务B 调用服务C 然后等待从服务A、B和C的响应 根据从服务A、服务B和服务C返回的数据完成业务逻辑，然后结束 理论上 3秒左右即可完成同样的业务逻辑 Talk is cheap. Show me the code123456789101112131415161718192021222324252627public class User &#123; private String name; private String blog; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getBlog() &#123; return blog; &#125; public void setBlog(String blog) &#123; this.blog = blog; &#125; @Override public String toString() &#123; return "User [name=" + name + ", blog=" + blog + "]"; &#125;&#125; 12345678910111213141516171819202122232425262728293031import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.boot.web.client.RestTemplateBuilder;import org.springframework.scheduling.annotation.Async;import org.springframework.stereotype.Service;import org.springframework.web.client.RestTemplate;import java.util.concurrent.CompletableFuture;@Servicepublic class GitHubLookupService &#123; private static final Logger logger = LoggerFactory.getLogger(GitHubLookupService.class); private final RestTemplate restTemplate; public GitHubLookupService(RestTemplateBuilder restTemplateBuilder) &#123; this.restTemplate = restTemplateBuilder.build(); &#125; @Async public CompletableFuture&lt;User&gt; findUser(String user) throws InterruptedException &#123; logger.info("Looking up " + user); String url = String.format("https://api.github.com/users/%s", user); User results = restTemplate.getForObject(url, User.class); // Artificial delay of 3s for demonstration purposes Thread.sleep(3000L); return CompletableFuture.completedFuture(results); &#125;&#125; The findUser method is flagged with Spring’s @Async annotation, indicating it will run on a separate thread. The method’s return type is CompletableFuture&lt;User&gt; instead of User, a requirement for any asynchronous service. findUser 方法被标记为Spring的 @Async 注解，表示它将在一个单独的线程上运行。该方法的返回类型是 CompleetableFuture&lt;user&gt; 而不是 User，这是任何异步服务的要求。 1234567891011121314151617181920212223242526272829import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.Bean;import org.springframework.scheduling.annotation.EnableAsync;import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;import java.util.concurrent.Executor;@SpringBootApplication@EnableAsyncpublic class App &#123; public static void main(String[] args) &#123; // close the application context to shut down the custom ExecutorService SpringApplication.run(App.class, args).close(); &#125; @Bean public Executor asyncExecutor() &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(2); executor.setMaxPoolSize(2); executor.setQueueCapacity(500); executor.setThreadNamePrefix("GithubLookup-"); executor.initialize(); return executor; &#125;&#125; The @EnableAsync annotation switches on Spring’s ability to run @Async methods in a background thread pool. This class also customizes the used Executor. In our case, we want to limit the number of concurrent threads to 2 and limit the size of the queue to 500. There are many more things you can tune. By default, a SimpleAsyncTaskExecutor is used. @EnableAsync 注解开启Spring在后台线程池中运行 @Async 方法的能力。该类也可以自定义使用的 Executor。在我们的示例中，我们希望将并发线程的数量限制为2，并将队列的大小限制为500。有很多你可以配置的东西)。默认情况下，使用SimpleAsyncTaskExecutor。 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.util.concurrent.CompletableFuture;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.boot.CommandLineRunner;import org.springframework.stereotype.Component;import com.gitee.taven.entity.User;import com.gitee.taven.service.GitHubLookupService;@Componentpublic class AppRunner implements CommandLineRunner &#123; private static final Logger logger = LoggerFactory.getLogger(AppRunner.class); private final GitHubLookupService gitHubLookupService; public AppRunner(GitHubLookupService gitHubLookupService) &#123; this.gitHubLookupService = gitHubLookupService; &#125; @Override public void run(String... args) throws Exception &#123; // Start the clock long start = System.currentTimeMillis(); // Kick of multiple, asynchronous lookups CompletableFuture&lt;User&gt; page1 = gitHubLookupService.findUser("PivotalSoftware"); CompletableFuture&lt;User&gt; page2 = gitHubLookupService.findUser("CloudFoundry"); CompletableFuture&lt;User&gt; page3 = gitHubLookupService.findUser("Spring-Projects"); // Wait until they are all done CompletableFuture.allOf(page1,page2,page3).join(); // Print results, including elapsed time float exc = (float)(System.currentTimeMillis() - start)/1000; logger.info("Elapsed time: " + exc + " seconds"); logger.info("--&gt; " + page1.get()); logger.info("--&gt; " + page2.get()); logger.info("--&gt; " + page3.get()); &#125; &#125; 通过实现 CommandLineRunner 调用 service 服务，我们设置了 Thread.sleep(3000L); 运行demo，4.73s 结束战斗！ 本文demohttps://gitee.com/yintianwen7/taven-springboot-learning/tree/master/spring-async-demo]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>多线程</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 动态切换、添加数据源实现以及源码浅析]]></title>
    <url>%2Fpost%2F9ffabefe.html</url>
    <content type="text"><![CDATA[公司项目需求，由于要兼容老系统的数据库结构，需要搭建一个 可以动态切换、添加数据源的后端服务。 分析参考了过去的项目，通过配置多个SqlSessionFactory 来实现多数据源，这么做的话，未免过于笨重，而且无法实现动态添加数据源这个需求通过 spring AbstractRoutingDataSource 为我们抽象了一个 DynamicDataSource 解决这一问题 源码简单分析下 AbstractRoutingDataSource 的源码 targetDataSources 就是我们的多个数据源，在初始化的时候会调用afterPropertiesSet()，去解析我们的数据源 然后 put 到 resolvedDataSources 实现了 DataSource 的 getConnection(); 我们看看 determineTargetDataSource(); 做了什么 通过下面的 determineCurrentLookupKey();（这个方法需要我们实现） 返回一个key，然后从 resolvedDataSources （其实也就是 targetDataSources） 中 get 一个数据源，实现了每次调用 getConnection(); 打开连接 切换数据源，如果想动态添加的话 只需要重新 set targetDataSources 再调用 afterPropertiesSet() 即可 Talk is cheap. Show me the code我使用的springboot版本为 1.5.x，下面是核心代码完整代码：https://gitee.com/yintianwen7/spring-dynamic-datasource123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/** * 多数据源配置 * * @author Taven * */@Configuration@MapperScan("com.gitee.taven.mapper")public class DataSourceConfigurer &#123; /** * DataSource 自动配置并注册 * * @return data source */ @Bean("db0") @Primary @ConfigurationProperties(prefix = "datasource.db0") public DataSource dataSource0() &#123; return DruidDataSourceBuilder.create().build(); &#125; /** * DataSource 自动配置并注册 * * @return data source */ @Bean("db1") @ConfigurationProperties(prefix = "datasource.db1") public DataSource dataSource1() &#123; return DruidDataSourceBuilder.create().build(); &#125; /** * 注册动态数据源 * * @return */ @Bean("dynamicDataSource") public DataSource dynamicDataSource() &#123; DynamicRoutingDataSource dynamicRoutingDataSource = new DynamicRoutingDataSource(); Map&lt;Object, Object&gt; dataSourceMap = new HashMap&lt;&gt;(); dataSourceMap.put("dynamic_db0", dataSource0()); dataSourceMap.put("dynamic_db1", dataSource1()); dynamicRoutingDataSource.setDefaultTargetDataSource(dataSource0());// 设置默认数据源 dynamicRoutingDataSource.setTargetDataSources(dataSourceMap); return dynamicRoutingDataSource; &#125; /** * Sql session factory bean. * Here to config datasource for SqlSessionFactory * &lt;p&gt; * You need to add @&#123;@code @ConfigurationProperties(prefix = "mybatis")&#125;, if you are using *.xml file, * the &#123;@code 'mybatis.type-aliases-package'&#125; and &#123;@code 'mybatis.mapper-locations'&#125; should be set in * &#123;@code 'application.properties'&#125; file, or there will appear invalid bond statement exception * * @return the sql session factory bean */ @Bean @ConfigurationProperties(prefix = "mybatis") public SqlSessionFactoryBean sqlSessionFactoryBean() &#123; SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); // 必须将动态数据源添加到 sqlSessionFactoryBean sqlSessionFactoryBean.setDataSource(dynamicDataSource()); return sqlSessionFactoryBean; &#125; /** * 事务管理器 * * @return the platform transaction manager */ @Bean public PlatformTransactionManager transactionManager() &#123; return new DataSourceTransactionManager(dynamicDataSource()); &#125;&#125; 通过 ThreadLocal 获取线程安全的数据源 key12345678910111213141516171819202122232425262728293031323334353637package com.gitee.taven.config;public class DynamicDataSourceContextHolder &#123; private static final ThreadLocal&lt;String&gt; contextHolder = new ThreadLocal&lt;String&gt;() &#123; @Override protected String initialValue() &#123; return "dynamic_db0"; &#125; &#125;; /** * To switch DataSource * * @param key the key */ public static void setDataSourceKey(String key) &#123; contextHolder.set(key); &#125; /** * Get current DataSource * * @return data source key */ public static String getDataSourceKey() &#123; return contextHolder.get(); &#125; /** * To set DataSource as default */ public static void clearDataSourceKey() &#123; contextHolder.remove(); &#125;&#125; 动态 添加、切换数据源123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * 动态数据源 * * @author Taven * */public class DynamicRoutingDataSource extends AbstractRoutingDataSource &#123; private final Logger logger = LoggerFactory.getLogger(getClass()); private static Map&lt;Object, Object&gt; targetDataSources = new HashMap&lt;&gt;(); /** * 设置当前数据源 * * @return */ @Override protected Object determineCurrentLookupKey() &#123; logger.info("Current DataSource is [&#123;&#125;]", DynamicDataSourceContextHolder.getDataSourceKey()); return DynamicDataSourceContextHolder.getDataSourceKey(); &#125; @Override public void setTargetDataSources(Map&lt;Object, Object&gt; targetDataSources) &#123; super.setTargetDataSources(targetDataSources); DynamicRoutingDataSource.targetDataSources = targetDataSources; &#125; /** * 是否存在当前key的 DataSource * * @param key * @return 存在返回 true, 不存在返回 false */ public static boolean isExistDataSource(String key) &#123; return targetDataSources.containsKey(key); &#125; /** * 动态增加数据源 * * @param map 数据源属性 * @return */ public synchronized boolean addDataSource(Map&lt;String, String&gt; map) &#123; try &#123; Connection connection = null; // 排除连接不上的错误 try &#123; Class.forName(map.get(DruidDataSourceFactory.PROP_DRIVERCLASSNAME)); connection = DriverManager.getConnection( map.get(DruidDataSourceFactory.PROP_URL), map.get(DruidDataSourceFactory.PROP_USERNAME), map.get(DruidDataSourceFactory.PROP_PASSWORD)); System.out.println(connection.isClosed()); &#125; catch (Exception e) &#123; return false; &#125; finally &#123; if (connection != null &amp;&amp; !connection.isClosed()) connection.close(); &#125; String database = map.get("database");//获取要添加的数据库名 if (StringUtils.isBlank(database)) return false; if (DynamicRoutingDataSource.isExistDataSource(database)) return true; DruidDataSource druidDataSource = (DruidDataSource) DruidDataSourceFactory.createDataSource(map); druidDataSource.init(); Map&lt;Object, Object&gt; targetMap = DynamicRoutingDataSource.targetDataSources; targetMap.put(database, druidDataSource); // 当前 targetDataSources 与 父类 targetDataSources 为同一对象 所以不需要set// this.setTargetDataSources(targetMap); this.afterPropertiesSet(); logger.info("dataSource &#123;&#125; has been added", database); &#125; catch (Exception e) &#123; logger.error(e.getMessage()); return false; &#125; return true; &#125; &#125; 可以通过 AOP 或者 手动 DynamicDataSourceContextHolder.setDataSourceKey(String key) 切换数据源 需要注意的：当我们开启了事务之后，是无法在去切换数据源的 本文项目源码：https://gitee.com/yintianwen7/spring-dynamic-datasource参考文献：https://github.com/helloworlde/SpringBoot-DynamicDataSource]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>多数据源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Service 调用当前类方法事务不生效]]></title>
    <url>%2Fpost%2Ff2e584e4.html</url>
    <content type="text"><![CDATA[今天在测试框架的时候，我想在一个service类的方法中调用 当前类的另一个方法（该方法通过@Transactional开启事务），这时候发现被调用类的事务并没有生效。 1234567891011public boolean test1() &#123; // xxx 业务逻辑 return test2();&#125;@Transactionalpublic boolean test2() &#123; testMapper.insertSalary(&quot;test&quot;, UUID.randomUUID().toString()); int a = 10/0; return true;&#125; WHY? 搜索引擎一番查询之后，了解到问题的关键：@Transactional 是基于aop生的代理对象开启事务的PS:不了解代理模式的小伙伴，结尾有传送门 思路 spring 的事务是通过 aop 管理的 aop 会通过动态代理 为我们生成代理对象，aop 的功能（例如事务）都是在代理对象中实现的 aop 生成的代理类又在 spring 容器中，所以我们只要在 spring 容器中拿到当前这个bean 再去调用 test2() 就可以开启事务了。 解决123456789101112131415161718192021222324252627282930313233343536373839404142import org.springframework.beans.BeansException;import org.springframework.context.ApplicationContext;import org.springframework.context.ApplicationContextAware;import org.springframework.stereotype.Component;/** * Spring的ApplicationContext的持有者,可以用静态方法的方式获取spring容器中的bean * */@Componentpublic class SpringContextHolder implements ApplicationContextAware &#123; private static ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; SpringContextHolder.applicationContext = applicationContext; &#125; public static ApplicationContext getApplicationContext() &#123; assertApplicationContext(); return applicationContext; &#125; @SuppressWarnings("unchecked") public static &lt;T&gt; T getBean(String beanName) &#123; assertApplicationContext(); return (T) applicationContext.getBean(beanName); &#125; public static &lt;T&gt; T getBean(Class&lt;T&gt; requiredType) &#123; assertApplicationContext(); return applicationContext.getBean(requiredType); &#125; private static void assertApplicationContext() &#123; if (SpringContextHolder.applicationContext == null) &#123; throw new RuntimeException("applicaitonContext属性为null,请检查是否注入了SpringContextHolder!"); &#125; &#125;&#125; 123456789101112public boolean test1() &#123; // xxx 业务逻辑 // 在spring容器中 获取当前类的代理类 return SpringContextHolder.getBean(TestS.class).test2();&#125;@Transactionalpublic boolean test2() &#123; testMapper.insertSalary(&quot;test&quot;, UUID.randomUUID().toString()); int a = 10/0; return true;&#125; ok！搞定！ 传送门Spring AOP的实现原理Java 代理模式]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)Java 代理模式]]></title>
    <url>%2Fpost%2F4dd31fbe.html</url>
    <content type="text"><![CDATA[原文：https://www.cnblogs.com/cenyu/p/6289209.html 代理(Proxy)是一种设计模式，通过代理对象访问目标对象.这样做的好处是:可以在目标对象实现的基础上，扩展目标对象的功能。 举个例子来说明：假如说我现在想买一辆二手车，最便捷的方法一定是我去找中介，他们来给我做琐碎的事情，我只是负责选择自己喜欢的车，然后付钱就可以了。 代理对象是对目标对象的扩展,并会调用目标对象 静态代理12345public interface IUserDao &#123; void save(); &#125; 12345678public class UserDao implements IUserDao &#123; @Override public void save() &#123; System.out.println(&quot;----已经保存数据!----&quot;); &#125;&#125; 12345678910111213141516public class UserDaoProxy implements IUserDao &#123; //目标对象 private IUserDao target; public UserDaoProxy(IUserDao target)&#123; this.target=target; &#125; @Override public void save() &#123; System.out.println(&quot;开始事务...&quot;); target.save();//执行目标对象的方法 System.out.println(&quot;提交事务...&quot;); &#125;&#125; 12345678@Testpublic void staticProxy() &#123; //目标对象 UserDao target = new UserDao(); //代理对象，建立代理关系 UserDaoProxy proxy = new UserDaoProxy(target); proxy.save();//执行的是代理的方法&#125; 静态代理的缺点：接口新增方法时，类过多时，需要手动维护，过于繁琐，但是通过动态代理机制可以解决这一问题。 动态代理spring 中 AOP 通过 动态代理 在运行时为我们的代码增强，例如我们在开发的时候只需要做业务逻辑，AOP 通过动态代理可以为我们做事务，日志管理，权限校验等等。 jdk 动态代理我们不需要再去手动创建代理类（但是要求被代理类必须实现一个接口），只需要做一个动态代理工厂即可，jdk通过反射为我们在内存中动态创建代理对象，以及在方法执行的前后添加通知。1234567891011121314151617181920212223242526272829303132333435import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;/** * 创建动态代理对象 * */public class JdkProxyFactory&#123; //维护一个目标对象 private Object target; public JdkProxyFactory(Object target)&#123; this.target=target; &#125; //给目标对象生成代理对象 public Object getProxyInstance()&#123; return Proxy.newProxyInstance( target.getClass().getClassLoader(), target.getClass().getInterfaces(), new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("开始事务2"); //执行目标对象方法 Object returnValue = method.invoke(target, args); System.out.println("提交事务2"); return returnValue; &#125; &#125; ); &#125;&#125; 123456789101112131415@Testpublic void jdkProxy() &#123; // 目标对象 IUserDao target = new UserDao(); // 原始类型 System.out.println(&quot;原始类型&quot; + target.getClass()); // 给目标对象，创建代理对象 IUserDao proxy = (IUserDao) new JdkProxyFactory(target).getProxyInstance(); // class $Proxy0 内存中动态生成的代理对象 System.out.println(&quot;动态代理后对象类型:&quot; + proxy.getClass()); // 代理对象 执行方法 proxy.save();&#125; 执行测试方法12345原始类型class com.example.demo.original_code.UserDao动态代理后对象类型:class com.sun.proxy.$Proxy4开始事务2----已经保存数据!----提交事务2 cglib 动态代理静态代理 和 jdk代理 要求目标对象必须实现接口，而 cglib 动态代理无需实现接口。cglib 会动态为目标对象 创建一个子类对象。使用须知：1.被代理的类不能为 final2.被代理类的方法 不能为 final/static，否则无法被拦截12345678910111213141516171819202122232425262728293031323334353637383940414243import java.lang.reflect.Method;import org.springframework.cglib.proxy.Enhancer;import org.springframework.cglib.proxy.MethodInterceptor;import org.springframework.cglib.proxy.MethodProxy;/** * Cglib子类代理工厂 * 对UserDao在内存中动态构建一个子类对象 */public class CglibProxyFactory implements MethodInterceptor &#123; //维护目标对象 private Object target; public CglibProxyFactory(Object target) &#123; this.target = target; &#125; //给目标对象创建一个代理对象 public Object getProxyInstance()&#123; //1.工具类 Enhancer en = new Enhancer(); //2.设置父类 en.setSuperclass(target.getClass()); //3.设置回调函数 en.setCallback(this); //4.创建子类(代理对象) return en.create(); &#125; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; System.out.println("开始事务..."); //执行目标对象的方法 Object returnValue = method.invoke(target, args); System.out.println("提交事务..."); return returnValue; &#125;&#125; 1234567891011@Testpublic void cglibProxy() &#123; //目标对象 UserDao target = new UserDao(); //代理对象 UserDao proxy = (UserDao) new CglibProxyFactory(target).getProxyInstance(); //执行代理对象的方法 proxy.save();&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-查询树结构]]></title>
    <url>%2Fpost%2Fa998078e.html</url>
    <content type="text"><![CDATA[在 oracle 数据库中，通过 start with connect by prior 递归可以直接查出树结构，但是在 mysql 当中如何解决树查询问题呢？ #####思路：我们可以通过自定义函数，遍历找出某一节点的所有子节点 （或者某一节点的所有父节点）的字符串集合。然后通过 FIND_IN_SET 函数，这就查出了我们想要的树 #####实践：1）建表 以及 测试数据准备123456CREATE TABLE `tree` ( `id` int(11) NOT NULL, `pid` int(11) DEFAULT NULL, `name` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 12345678INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;1&apos;, NULL, &apos;一级&apos;);INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;2&apos;, &apos;1&apos;, &apos;二级1&apos;);INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;3&apos;, &apos;2&apos;, &apos;三级1&apos;);INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;4&apos;, &apos;3&apos;, &apos;四级1&apos;);INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;5&apos;, &apos;4&apos;, &apos;五级&apos;);INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;6&apos;, &apos;1&apos;, &apos;三级2&apos;);INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;7&apos;, &apos;1&apos;, &apos;二级2&apos;);INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;8&apos;, &apos;6&apos;, &apos;四级2&apos;); 2）查询 某节点下所有子节点12345678910111213CREATE FUNCTION `GET_CHILD_NODE`(rootId varchar(100)) RETURNS varchar(2000) BEGIN DECLARE str varchar(2000); DECLARE cid varchar(100); SET str = &apos;$&apos;; SET cid = rootId; WHILE cid is not null DO SET str = concat(str, &apos;,&apos;, cid); SELECT group_concat(id) INTO cid FROM tree where FIND_IN_SET(pid, cid); END WHILE; RETURN str; END 第一次进入函数 cid 为根节点，开始循环后，cid 为每次查询结果集 （也就是子节点），下一次会查询出所有子节点的子节点 … 以此类推，当没有子节点时退出循环，也就得到了所有的节点。12345678910mysql&gt; SELECT * from tree where FIND_IN_SET(id, GET_CHILD_NODE(2));+----+-----+-------+| id | pid | name |+----+-----+-------+| 2 | 1 | 二级1 || 3 | 2 | 三级1 || 4 | 3 | 四级1 || 5 | 4 | 五级 |+----+-----+-------+4 rows in set 再通过这些节点 筛选，ok ! 3）查询 某节点的所有父节点1234567891011121314151617CREATE FUNCTION `GET_PARENT_NODE`(rootId varchar(100)) RETURNS varchar(1000) BEGIN DECLARE fid varchar(100) default &apos;&apos;; DECLARE str varchar(1000) default rootId; WHILE rootId is not null do SET fid =(SELECT pid FROM tree WHERE id = rootId); IF fid is not null THEN SET str = concat(str, &apos;,&apos;, fid); SET rootId = fid; ELSE SET rootId = fid; END IF; END WHILE; return str; END 和上一个函数类似，不断的遍历去找 父id，然后拼接到字符串中，为空退出循环。1234567891011mysql&gt; select * from tree where FIND_IN_SET(id, GET_PARENT_NODE(5));+----+------+-------+| id | pid | name |+----+------+-------+| 1 | NULL | 一级 || 2 | 1 | 二级1 || 3 | 2 | 三级1 || 4 | 3 | 四级1 || 5 | 4 | 五级 |+----+------+-------+5 rows in set]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Dijkstra 算法实现原理]]></title>
    <url>%2Fpost%2F9186820e.html</url>
    <content type="text"><![CDATA[迪杰斯特拉(Dijkstra)算法是典型最短路径算法，用于计算一个节点到其他节点的最短路径。它的主要特点是以起始点为中心向外层层扩展(广度优先搜索思想)，直到扩展到终点为止。 （嗯，第一段是抄的，由于本人算法的基础比较薄弱，我会尽量用通俗易懂的语言来让大家理解本文） 参考博客:数据结构–Dijkstra算法最清楚的讲解 大概就是这样一个有权图，Dijkstra算法可以计算任意节点到其他节点的最短路径 算法思路 指定一个节点，例如我们要计算 ‘A’ 到其他节点的最短路径 引入两个集合（S、U），S集合包含已求出的最短路径的点（以及相应的最短长度），U集合包含未求出最短路径的点（以及A到该点的路径，注意 如上图所示，A-&gt;C由于没有直接相连 初始时为∞） 初始化两个集合，S集合初始时 只有当前要计算的节点，A-&gt;A = 0，U集合初始时为 A-&gt;B = 4, A-&gt;C = ∞, A-&gt;D = 2, A-&gt;E = ∞，敲黑板！！！接下来要进行核心两步骤了 从U集合中找出路径最短的点，加入S集合，例如 A-&gt;D = 2 更新U集合路径，if ( &#39;D 到 B,C,E 的距离&#39; + &#39;AD 距离&#39; &lt; &#39;A 到 B,C,E 的距离&#39; ) 则更新U 循环执行 4、5 两步骤，直至遍历结束，得到A 到其他节点的最短路径算法图解1.选定A节点并初始化，如上述步骤3所示 2.执行上述 4、5两步骤，找出U集合中路径最短的节点D 加入S集合，并根据条件 if ( &#39;D 到 B,C,E 的距离&#39; + &#39;AD 距离&#39; &lt; &#39;A 到 B,C,E 的距离&#39; ) 来更新U集合 3.这时候 A-&gt;B, A-&gt;C 都为3，没关系。其实这时候他俩都是最短距离，如果从算法逻辑来讲的话，会先取到B点。而这个时候 if 条件变成了 if ( &#39;B 到 C,E 的距离&#39; + &#39;AB 距离&#39; &lt; &#39;A 到 C,E 的距离&#39; ) ，如图所示这时候A-&gt;B距离 其实为 A-&gt;D-&gt;B 思路就是这样，往后就是大同小异了 算法结束 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class Dijkstra &#123; public static final int M = 10000; // 代表正无穷 public static void main(String[] args) &#123; // 二维数组每一行分别是 A、B、C、D、E 各点到其余点的距离, // A -&gt; A 距离为0, 常量M 为正无穷 int[][] weight1 = &#123; &#123;0,4,M,2,M&#125;, &#123;4,0,4,1,M&#125;, &#123;M,4,0,1,3&#125;, &#123;2,1,1,0,7&#125;, &#123;M,M,3,7,0&#125; &#125;; int start = 0; int[] shortPath = dijkstra(weight1, start); for (int i = 0; i &lt; shortPath.length; i++) System.out.println(&quot;从&quot; + start + &quot;出发到&quot; + i + &quot;的最短距离为：&quot; + shortPath[i]); &#125; public static int[] dijkstra(int[][] weight, int start) &#123; // 接受一个有向图的权重矩阵，和一个起点编号start（从0编号，顶点存在数组中） // 返回一个int[] 数组，表示从start到它的最短路径长度 int n = weight.length; // 顶点个数 int[] shortPath = new int[n]; // 保存start到其他各点的最短路径 String[] path = new String[n]; // 保存start到其他各点最短路径的字符串表示 for (int i = 0; i &lt; n; i++) path[i] = new String(start + &quot;--&gt;&quot; + i); int[] visited = new int[n]; // 标记当前该顶点的最短路径是否已经求出,1表示已求出 // 初始化，第一个顶点已经求出 shortPath[start] = 0; visited[start] = 1; for (int count = 1; count &lt; n; count++) &#123; // 要加入n-1个顶点 int k = -1; // 选出一个距离初始顶点start最近的未标记顶点 int dmin = Integer.MAX_VALUE; for (int i = 0; i &lt; n; i++) &#123; if (visited[i] == 0 &amp;&amp; weight[start][i] &lt; dmin) &#123; dmin = weight[start][i]; k = i; &#125; &#125; // 将新选出的顶点标记为已求出最短路径，且到start的最短路径就是dmin shortPath[k] = dmin; visited[k] = 1; // 以k为中间点，修正从start到未访问各点的距离 for (int i = 0; i &lt; n; i++) &#123; //如果 &apos;起始点到当前点距离&apos; + &apos;当前点到某点距离&apos; &lt; &apos;起始点到某点距离&apos;, 则更新 if (visited[i] == 0 &amp;&amp; weight[start][k] + weight[k][i] &lt; weight[start][i]) &#123; weight[start][i] = weight[start][k] + weight[k][i]; path[i] = path[k] + &quot;--&gt;&quot; + i; &#125; &#125; &#125; for (int i = 0; i &lt; n; i++) &#123; System.out.println(&quot;从&quot; + start + &quot;出发到&quot; + i + &quot;的最短路径为：&quot; + path[i]); &#125; System.out.println(&quot;=====================================&quot;); return shortPath; &#125; &#125;]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>Dijkstra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot jackson（Date类型入参、格式化，以及如何处理null）]]></title>
    <url>%2Fpost%2F23d9d33.html</url>
    <content type="text"><![CDATA[首先，我们要知道 springboot 默认使用 jackson 解析 json（当然这里也是可以配置使用其他 json 解析框架）。在不配置其他 json 解析的情况下，我们可以通过 spring boot 提供的注解和配置 来让 jackson 帮助我们提高开发效率 使用 @ResponseBody @RequestBody， Date 类型对象入参，返回json格式化解决方法如下: 1. application.properties中加入如下代码12spring.jackson.date-format=yyyy-MM-dd HH:mm:ssspring.jackson.time-zone=GMT+8 2. 如果个别实体需要使用其他格式的 pattern，在实体上加入注解即可1234567import org.springframework.format.annotation.DateTimeFormat;import com.fasterxml.jackson.annotation.JsonFormat;public class MrType &#123; @JsonFormat(timezone = "GMT+8",pattern = "yyyy-MM-dd") @DateTimeFormat(pattern="yyyy-MM-dd") private Date createdDate;&#125; 关于spring boot 时间类型支持我做了以下测试：1) application.properties 配置注释，不添加注解：spring 无法接收时间参数（400），json 输出 &quot;2018-03-29T09:45:31.513+0000&quot;2) application.properties 配置开启，不添加注解：仅支持 yyyy-MM-dd HH:mm:ss 的格式参数和 json 输出3) application.properties 配置开启，实体添加 @JsonFormat(pattern = &quot;yyyy-MM-dd&quot;)，实体可接受 yyyy-MM-dd HH:mm:ss 和 yyyy-MM-dd 格式的参数，json输出格式为 yyyy-MM-dd，由此可见@JsonFormat是限制Date 类型 json 输出的，但是为什么对接受的类型也造成了影响？有待考证4) application.properties 配置开启，实体添加 @DateTimeFormat(pattern = &quot;yyyy-MM-dd&quot;)，结果与第二条测试一样？貌似@DateTimeFormat 注解并没有生效？有待考证5) application.properties 配置开启，实体添加 @JsonFormat 和 @DateTimeFormat 结果与第三条一样 结论：实际项目中 application.properties设置通用时间格式，个别属性需要特殊处理时，添加@JsonFormat（@JsonFormat 自己好像就把这件事搞定了） 使用 @ResponseBody 时 忽略 json 中值为null的属性1. application.properties中加入如下代码1spring.jackson.default-property-inclusion=non-null 或者在类上声明@JsonInclude(JsonInclude.Include.NON_NULL) 1234567import java.io.Serializable;import com.fasterxml.jackson.annotation.JsonInclude;@JsonInclude(JsonInclude.Include.NON_NULL)//该注解配合jackson，序列化时忽略 null属性public class AjaxResult implements Serializable &#123;&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>jackson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-并发下生成不重复流水号]]></title>
    <url>%2Fpost%2Fef3d0933.html</url>
    <content type="text"><![CDATA[更新于 2018-12-23 22:21:44前言：一年前的写的，当时的做法并不能在并发下保证流水号的唯一性，因为当时并没有写多线程测试过… 思路 sCode sName sQz sValue order 订单 DD 18120100 首先每个业务的流水号对应表中的一条数据 每个要获取流水号的线程调用 一个用来生成流水号的 存储过程 根据sCode 找到 sValue。1234if (sValue == null) 初始化值 else sValue ++ 敲黑板！划重点！ 分析：上述的思路看似没什么问题，但是如果两个线程并发的执行会出现两个事务读取到相同的数据，同时都执行初始化或者自增。 方案：使用 MySQL 写锁（select…for update，也叫X锁，排它锁） 来解决这一问题 例如：事务A和事务B同时进行，事务A 拿到 当前这条数据的写锁，此时如果事务B 想访问这条数据需要等待事务A结束。并发时让两个事务对同一条数据的操作变成了串行化。 123456CREATE TABLE `sys_sno` ( `sCode` varchar(50) DEFAULT NULL COMMENT &apos;编码&apos;, `sName` varchar(100) DEFAULT NULL COMMENT &apos;名称&apos;, `sQz` varchar(50) DEFAULT NULL COMMENT &apos;前缀&apos;, `sValue` varchar(80) DEFAULT NULL COMMENT &apos;值&apos;) ENGINE=InnoDB DEFAULT CHARSET=utf8; 123456789101112131415161718192021222324252627282930313233343536373839CREATE DEFINER=`root`@`localhost` PROCEDURE `GetSerialNo`(IN tsCode VARCHAR(50),OUT result VARCHAR(200) )BEGIN DECLARE tsValue VARCHAR(50); DECLARE tdToday VARCHAR(20); DECLARE nowdate VARCHAR(20); DECLARE tsQZ VARCHAR(50); DECLARE t_error INTEGER DEFAULT 0; DECLARE CONTINUE HANDLER FOR SQLEXCEPTION SET t_error=1; START TRANSACTION; /* UPDATE sys_sno SET sValue=sValue WHERE sCode=tsCode; */ SELECT sValue INTO tsValue FROM sys_sno WHERE sCode=tsCode for UPDATE; SELECT sQz INTO tsQZ FROM sys_sno WHERE sCode=tsCode ; -- 因子表中没有记录，插入初始值 IF tsValue IS NULL THEN SELECT CONCAT(DATE_FORMAT(NOW(),&apos;%y%m&apos;),&apos;0001&apos;) INTO tsValue; UPDATE sys_sno SET sValue=tsValue WHERE sCode=tsCode ; SELECT CONCAT(tsQZ,tsValue) INTO result; ELSE SELECT SUBSTRING(tsValue,1,4) INTO tdToday; SELECT CONVERT(DATE_FORMAT(NOW(),&apos;%y%m&apos;),SIGNED) INTO nowdate; -- 判断年月是否需要更新 IF tdToday = nowdate THEN SET tsValue=CONVERT(tsValue,SIGNED) + 1; ELSE SELECT CONCAT(DATE_FORMAT(NOW(),&apos;%y%m&apos;) ,&apos;0001&apos;) INTO tsValue ; END IF; UPDATE sys_sno SET sValue =tsValue WHERE sCode=tsCode; SELECT CONCAT(tsQZ,tsValue) INTO result; END IF; IF t_error =1 THEN ROLLBACK; SET result = &apos;Error&apos;; ELSE COMMIT; END IF; SELECT result ; END; 测试代码完整测试代码：https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/uni-number123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import com.gitee.taven.uninumber.mapper.OrderMapper;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.HashMap;import java.util.Map;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;@Servicepublic class OrderService &#123; private static OrderMapper orderMapper; @Autowired public void setOrderMapper(OrderMapper orderMapper) &#123; this.orderMapper = orderMapper; &#125; private static final int TOTAL_THREADS = 100; public void multiThread() &#123; // 创建固定长度线程池 ExecutorService fixedThreadPool = Executors.newFixedThreadPool(50); for (int i = 0; i &lt; TOTAL_THREADS; i++) &#123; Thread thread = new OrderThread(); fixedThreadPool.execute(thread); &#125; &#125; public static class OrderThread extends Thread &#123; @Override public void run() &#123; try &#123; Map&lt;String, String&gt; parameterMap = initParameterMap(); orderMapper.createOrderNum(parameterMap); String number = parameterMap.get(&quot;result&quot;); System.out.println(Thread.currentThread().getName() + &quot; : &quot; +number); &#125; catch (Exception e) &#123; System.out.println(e); &#125; &#125; public Map&lt;String, String&gt; initParameterMap() &#123; Map&lt;String, String&gt; parameterMap = new HashMap&lt;&gt;(); parameterMap.put(&quot;tsCode&quot;, &quot;order&quot;); parameterMap.put(&quot;result&quot;, &quot;-1&quot;); return parameterMap; &#125; &#125;&#125; 12345678910111213141516// mapper 接口public interface OrderMapper &#123; int createOrderNum(Map&lt;String, String&gt; parameterMap);&#125;// xml &lt;update id=&quot;createOrderNum&quot; parameterMap=&quot;initMap&quot; statementType=&quot;CALLABLE&quot;&gt; CALL GetSerialNo(?,?) &lt;/update&gt; &lt;parameterMap type=&quot;java.util.Map&quot; id=&quot;initMap&quot;&gt; &lt;parameter property=&quot;tsCode&quot; mode=&quot;IN&quot; jdbcType=&quot;VARCHAR&quot;/&gt; &lt;parameter property=&quot;result&quot; mode=&quot;OUT&quot; jdbcType=&quot;VARCHAR&quot;/&gt; &lt;/parameterMap&gt; 备注 执行了一个百个线程之后 可以看一下数据，自增到了100 说明成功了 删掉 存储过程中的 for update，再执行 可以看出锁的作用]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>并发</tag>
      </tags>
  </entry>
</search>
