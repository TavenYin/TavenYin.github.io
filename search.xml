<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[聊聊 Redis 分布式锁]]></title>
    <url>%2Fpost%2F95275951.html</url>
    <content type="text"><![CDATA[前言Redis 是日常开发中经常使用到的中间件，以优秀的性能著称。但是 Redis 分布式锁可以说是饱受争议，很多人认为 Redis 并不适合作为分布式锁。它确实存在着一些问题，今天我准备聊一聊 Redis 分布式锁如何实现、有什么问题、该如何解决以及它的进阶版本红锁（Red Lock）解决了哪些问题，又带来了哪些新的问题 Redis 分布式锁的标准实现方式我们以一个 Redis 单实例为例 上锁原理Redis 通过 SET key value NX 命令实现锁的互斥机制。SETNX 含义为（SET if Not eXists），只有在 key 不存在时，才能 SET 成功 完整的上锁命令如下所示12# NX 代表 key 不存在才能 SET 成功，PX 指定 key 的过期时间SET resource_name client_id NX PX 30000 key 的过期时间，就是锁的持有时间（或者说释放时间） client_id 可以是任何随机的唯一值（例如 UUID），它存在的意义是用于解锁。只有 client_id 匹配时，才能解锁（保证只有持有锁的客户端才可以解锁，防止其他客户端错误的解锁） 解锁原理解锁本质上就是删除 key，或者 key 过期。 主动解锁通常使用 lua 脚本进行，因为我们希望原子性的完成判断和删除逻辑12345if redis.call("get",KEYS[1]) == ARGV[1] then return redis.call("del",KEYS[1])else return 0end Redis 分布式锁的问题通过上一章节，我们已经了解到了，如何利用 Redis 机制实现一个分布式锁，现在我们要更深入的讨论下 Redis 是否可靠 Redis 锁未设置过期时间，导致死锁场景：我使用 SET key client_id NX 持有了一个锁，但是我并没有设置过期时间。此时我的应用进程突然挂了，并没有来得及进行解锁操作。此时任何客户端都无法再持有这个锁了，必须得人工介入删除掉这个 key，业务才能继续流转 对于这个场景有什么好的解决办法呢 Redission 提供了一个很好的解决方案。Redission 提供了一个“看门狗”机制 如果用户在申请锁的时候没有指定锁的释放时间，此时 Redission 会为锁指定一个 30 秒的过期时间。在锁释放之前，再额外使用一个线程不断地延长 key 的过期时间。这样就能保证即使应用进程意外挂掉，也不会出现“死锁” Redission 使用了 Netty API 来实现“看门狗”，这里为了方便大家理解，我使用 Java API 做一个简单的示例以供参考 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class WatchDogExample &#123; /** * 定义线程池 */ static ScheduledExecutorService scheduled = new ScheduledThreadPoolExecutor(8, new ThreadFactory() &#123; private final AtomicInteger threadNumber = new AtomicInteger(1); @Override public Thread newThread(Runnable r) &#123; Thread thread = new Thread(r, "watchDog-" + threadNumber.getAndIncrement()); // 设置为守护线程，防止主线程意外挂掉，看门狗线程依旧在执行 thread.setDaemon(false); return thread; &#125; &#125;); public static void main(String[] args) &#123; // 模拟获得锁 tryLock() // SET NX 命令，过期时间为 30s System.out.println("SET KEY VALUE NX EX 30"); // 向线程池提交 KEY 续期任务 ScheduledFuture&lt;?&gt; future = scheduled.scheduleAtFixedRate(() -&gt; &#123; if (Thread.currentThread().isInterrupted()) &#123; return; &#125; // EXPIRE 命令续期key 30s System.out.println("EXPIRE KEY 30"); &#125;, 10, 10, TimeUnit.SECONDS); try &#123; // 模拟业务流程 Thread.sleep(20000); &#125; catch (InterruptedException e) &#123; &#125; // 解锁 unlock。停止任务调度 &amp; 删除key future.cancel(true); System.out.println("DEL KEY"); try &#123; // 保证 main 线程不停止。如果 main 线程停止，守护线程将结束 new CountDownLatch(1).await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; Redis 锁过期了，但是应用进程还在操作共享资源这种情况，我的理解是，如果你明确指定了锁的过期时间，那么你就必须保证在这个时间内完成对共享资源的操作。如果你无法保证，就使用 2.1 的方式，不指定过期时间 如何实现锁的等待其实 SETNX 实现的效果仅是一个 tryLock，以 Java 中常用的锁作为参考。通常来说我们如果无法立即获得锁，是可以选择等待锁释放后，继续尝试获取锁。那么 Redis 分布式锁可以实现这一效果吗？ 当然可以了，参考 Redission 的实现，我们可以利用 Redis 的发布订阅来实现锁的等待，思路如下 a. 尝试获取锁b. 获取锁失败，订阅锁释放消息，线程进入等待状态c. 其它客户端解锁，发布锁释放消息d. 接受到锁释放消息，回到步骤 a 感兴趣的同学，可以自行阅读下 Redission 源码 Redis 锁是不可重入锁？首先我们要明白，可重入锁有什么意义？ Java synchronized、ReentrantLock 都是可重入锁。可重入锁，即同一个线程可以多次获取同一个锁，其意义在于防止代码出现死锁。 假设我们的 Redis 锁已经实现了 2.3 中提到的锁等待功能，此时我设置了锁的最大等待时间为 -1（无限等待），锁的持有时间也是 -1。我在编码的时候，没有注意，多次获取了相同的锁。由于是无限等待，我的代码在第二次获取锁时，就会出现死锁，永远的卡在那里。 对于 Redis 锁来说，可重入也可以代表锁续期 在理解了可重入锁的意义之后，我们该如何让 Redis 分布式锁支持重入呢？同样参考 Redission 即可 Redission 使用 Redis Hash 结构，存储结构为 lock_key，client_id，重入次数 123456789101112131415161718192021222324252627&lt;T&gt; RFuture&lt;T&gt; tryLockInnerAsync(long leaseTime, TimeUnit unit, long threadId, RedisStrictCommand&lt;T&gt; command) &#123; internalLockLeaseTime = unit.toMillis(leaseTime); return commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, command, // 是否存在锁 "if (redis.call('exists', KEYS[1]) == 0) then " + // 不存在则创建 "redis.call('hset', KEYS[1], ARGV[2], 1); " + // 设置过期时间 "redis.call('pexpire', KEYS[1], ARGV[1]); " + // 竞争锁成功 返回null "return nil; " + "end; " + // 如果锁已经被当前线程获取 "if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " + // 重入次数加1 "redis.call('hincrby', KEYS[1], ARGV[2], 1); " + "redis.call('pexpire', KEYS[1], ARGV[1]); " + "return nil; " + "end; " + // 锁被其他线程获取，返回锁的过期时间 "return redis.call('pttl', KEYS[1]);", // 下面三个参数分别为 KEYS[1], ARGV[1], ARGV[2] // 即锁的name，锁释放时间，当前线程唯一标识 Collections.&lt;Object&gt;singletonList(getName()), internalLockLeaseTime, getLockName(threadId));&#125; Redis 主备切换，导致多个客户端同时持有锁这个是 Redis 作为分布式锁最大的问题。 通常来说，我们会通过使用 Redis Sentinel 或者 Redis Cluster 来提高 Redis 的可用性，但是在提高 Redis 可用性的同时，也带来了丢失数据的风险 场景：现在我们有一个 Redis Sentinel，客户端 A 从 Redis Master 获取了锁（写入了一个 key），此时 Master 挂了。并且非常倒霉，在挂掉之前并没有将客户端 A 写入的数据同步到 Slave 节点，然后 Slave 升级为 Master，客户端 B 也获取了相同的锁。 这就是 Redis 分布式锁最尴尬的地方，当提高了 Redis 可用性之后，居然无法保证锁的互斥性，这是让人难以接受的。 关于这个问题，有什么好的解决方法呢？就是我们接下来要讲到的 Redis RedLock Redis RedLockRedLock 核心逻辑该算法的前提是我们需要准备 N 个完全独立的 Redis Master。我们参考 Redis 官方文档，将 N 设为 5 获取当前时间戳（毫秒） 尝试对 N 个实例进行 lock 操作，在所有实例中使用相同的 key 和 value。在执行 Redis 命令时，需要设置一个较小的 command timeout。例如锁的施放时间是 10s，则 command timeout 范围最好在 5 ~ 50ms。因为我们要与多个 Redis 实例通信，尽可能防止命令阻塞在某台实例中。 当客户端能够在大多数实例中获取锁（大多数至少为 N / 2 + 1），并且获取锁的总用时小于锁的释放时间，则认为成功持有锁 锁的实际持有时间 = 执行的持有时间（或者说施放时间）- 获取锁用时 如果客户端未能成功持有锁（不能满足步骤3），则对所有已经加锁的实例进行 unlock 操作 针对每一个节点的 lock 和 unlock 操作与上边提到过的实现方式相同 RedLock 潜在问题在网上看到了国外的分布式专家与 RedLock 作者对 RedLock 是否可靠进行了许多的讨论。 再结合一些其他的文章，对 RedLock 的问题总结一下，讨论中认为 RedLock 不可靠的几个关键点 时间钟跃进Redis key 的过期时间，最终会计算出一个时间戳。Redis 保证在系统时间超过这个时间戳时，删除这个 key。 如果对系统时间进行修改，例如快进了一段时间，可能会造成 key 失效，最终多个客户端同时获得锁 解决方法： 合理运维，不要修改系统时间。通常来说也没有人会这么做 客户端进程 GC 时间过长带来的问题客户端进程 GC 时间过长，导致锁过期，但是客户端无法感知，最终可能导致多个客户端同时持有锁。 上文提到了使用看门狗的概念，一定程度上可以解决了锁过期导致多个客户端同时持有锁的问题。但是如果真的 GC 时间真的特别长，导致看门狗机制也无法续期 key 的话，那确实会让多个客户端都持有锁。 解决方法： 延长 watchDog 的过期时间 优化 GC 时间 网络分区带来的问题现有 A、B、C、D、E 5台 Redis 实例，分别部署在五台机器上。客户端如果只能访问到 A、B 两台，与 C、D、E 网络无法通信。这种情况和 Redis 宕机一样，在网络恢复之前，没有办法能够解决 某个节点宕机引发的问题还是 A、B、C、D、E 5台 Redis 实例。客户端1在 A、B、C 三台机器上成功加锁，此时已成功持有 RedLock。 C 节点突然宕机，然后重启，但是客户端1写入的数据并没有及时落盘，此时重启后数据丢失。 最终可能导致客户端2 成功在 C、D、E 三台机器加锁，无法保证锁的互斥性 解决方法： Redis AOF 设置为 fsync=always，通常来说使用 Redis 的用户不会开启这个选项，势必会影响性能，但也是一个可以考虑的选项 从运维方面解决，保证在 key 的最大 TTL 之后重启 代码优化方面的一些建议。尽可能在所有实例上加锁。但是这并不能完全解决。最好的方法还是方案2。 其它问题使用 RedLock 必须要多搭建一套环境，比如项目中已经使用了 Redis Cluster 或者 Redis Sentinel，为了保证锁的可靠性必须再搭建一套 RedLock 环境。在一定程度上，增加了运维成本 RedLock 实现Redission 也提供了关于 RedLock 的实现，各位可参考 Redission 源码 总结本文基本覆盖了 Redis 分布式锁的常见问题与解决方案 如果对于分布式锁的互斥性要求并不高的话（例如系统需要计算某个数据，计算一次即可但是多次计算不会对业务有影响），传统 Redis 集群（Sentinel，Cluster）做分布式锁是没有问题的 当对于分布式锁的互斥性有严格要求的话，就需要考虑使用 RedLock、Zookeeper、或者数据库写锁来解决了 参考资料https://redis.io/docs/reference/patterns/distributed-locks/https://github.com/redisson/redisson/https://xiaomi-info.github.io/2019/12/17/redis-distributed-lock/]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>RedLock</tag>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Kafka：Retry Topic、DLT 的使用与原理]]></title>
    <url>%2Fpost%2F194835b9.html</url>
    <content type="text"><![CDATA[背景原生 Kafka 是不支持 Retry Topic 和 DLT （Dead Letter Topic，死信队列）。但是 Spring Kafka 在客户端实现了这两个功能。 版本spring-kafka 2.7.14（2.7.x 以下版本不支持 Retry Topic） 默认重试策略默认情况下，spring-kafka 在消费逻辑抛出异常时，会快速重试 10 次（无间隔时间），如果重试完成后，依旧消费失败，spring-kafka 会 commit 这条记录。 默认重试的实现原理是：重置当前 consumer offset，感兴趣的同学可以在 SeekUtils#doSeeks debug 一下 可以通过自定义 SeekToCurrentErrorHandler 来控制消费失败后的处理逻辑。例如：添加重试间隔，重试完成后依旧失败的消息发送到 DLT 自定义 SeekToCurrentErrorHandler12345678@Beanpublic ErrorHandler errorHandler(KafkaTemplate&lt;Object, Object&gt; kafkaTemplate) &#123; ConsumerRecordRecoverer recoverer = new DeadLetterPublishingRecoverer(kafkaTemplate); // 设置重试间隔 10秒 次数为 3次 BackOff backOff = new FixedBackOff(10 * 1000L, 3L); // 创建 SeekToCurrentErrorHandler 对象 return new SeekToCurrentErrorHandler(recoverer, backOff);&#125; 添加上述代码后，消费逻辑抛出异常后，会间隔 10s 重试 3 次，重试后依旧失败，会将消息发送到 DLT 关于默认重试策略，Kafka 的 TopicPartition 只会分配给一个消费者，而消费者对于某条消息的重试，会占用消费线程，影响整个 TopicPartition 的消费速度。如果使用 Retry Topic 功能，不会占用消费线程，会有专门的 retry 线程订阅 Retry Topic 执行重试消费。 Retry Topic + DLT 使用可以通过注解和全局配置的方式开启 Retry Topic 功能 @RetryableTopic使用注解的方式启用 Retry Topic，在 @KafkaListener 方法上添加 @RetryableTopic 即可123456789101112@Slf4j@Componentpublic class SimpleConsumer &#123; @RetryableTopic() @KafkaListener(topics = "test_topic", groupId = "demo01-consumer-group-1") public void onMessage(MessageWrapper message) &#123; log.info("[onMessage][线程编号:&#123;&#125; 消息内容：&#123;&#125;]", Thread.currentThread().getId(), message); throw new RuntimeException("test kafka exception"); &#125;&#125; 此时 Retry Topic 功能已经启用了。当消费逻辑抛出异常时，spring-kafka 会先将消息发送到 Retry Topic，随后在 Main Topic（对应上文的test_topic）中 commit 这条消息。会有专门的线程订阅 Retry Topic，不会影响正常消费 默认重试 3 次，间隔为 1s，如果在重试结束后，还没有成功被消费，该消息会被发送到 DLT 中 默认情况，消息被发送到死信队列后，会输出一条日志。12022-08-09 16:05:03.920 INFO 4048 --- [ner#3-dlt-0-C-1] o.s.k.retrytopic.RetryTopicConfigurer : Received message in dlt listener: test_topic-dlt-0@233 上述的日志输出是默认的死信订阅逻辑，用户可以在类中添加 @DltHandler 方法自定义死信消费逻辑1234@DltHandlerpublic void processMessage(MessageWrapper message) &#123; log.info("dlt &#123;&#125;", message);&#125; 至此，你的 Kafka 就拥有了类似 RocketMQ 的消息重试能力，但是配置方面还需要调整一下。 定制 @RetryableTopic可以自定义重试次数，延迟时间，死信策略等等，同时大部分参数还支持使用 Spring EL 表达式读取配置，这里简单列举下，更多的配置读者可以自行探索 12345678910@RetryableTopic( attempts = "$&#123;kafka.retry.attempts&#125;", backoff = @Backoff(delayExpression = "$&#123;kafka.retry.delay&#125;", multiplierExpression = "$&#123;kafka.retry.multiplier&#125;"), fixedDelayTopicStrategy = FixedDelayStrategy.SINGLE_TOPIC)@KafkaListener(topics = "test_topic", groupId = "demo01-consumer-group-1")public void onMessage(MessageWrapper message) &#123; log.info("[onMessage][线程编号:&#123;&#125; 消息内容：&#123;&#125;]", Thread.currentThread().getId(), message); throw new RuntimeException("test kafka exception");&#125; 解释一下上述的配置 attempts：重试次数 @Backoff delayExpression：消费延迟时间 @Backoff multiplierExpression：乘数。举个例子，第一次delay = 10s，如果 multiplier = 2，则下次 delay = 20s，以此类推，但是会有一个 maxDelay 作为延迟时间上限 fixedDelayTopicStrategy：可选策略包括：每次重试发送到单独的 Topic、只使用一个重试 Topic fixedDelayTopicStrategy 这个参数还是挺重要的，具体应该怎么选呢，我们稍后再说 RetryTopicConfiguration123456789@Beanpublic RetryTopicConfiguration myRetryTopic(KafkaTemplate&lt;String, Object&gt; template) &#123; return RetryTopicConfigurationBuilder .newInstance() .maxAttempts(4) .fixedBackOff(15000) .includeTopic("test_topic") .create(template);&#125; 使用这个方式配置项基本和注解一样，如果你有多个需要配置重试的消费者，使用 RetryTopicConfiguration 的方式要比注解方式更简单 源码解析延迟重试怎么实现的延迟重试这个功能应该分为两步 将需要重试的消息发送到 Retry Topic Retry Topic 的订阅者延迟消费 非常遗憾的是，Kafka 并没有延迟消息这样的功能，所以这个延迟消费也是 spring-kafka 自己实现的，不得不说这个组件真的下了很多功夫 接下来聊聊延迟重试的实现原理 延迟消息标识消息发送到 Retry Topic 这个步骤，感兴趣的同学可以 debug 一下 SeekToCurrentErrorHandler#handle 这里就不详细说了 每个需要被重试的消息，都会被添加 retry_topic-backoff-timestamp 这个 header，这个值代表这个消息的期望执行时间 开启了重试功能的 KafkaListener，在执行消费逻辑前，会先执行KafkaBackoffAwareMessageListenerAdapter#onMessage，该方法会先对消息进行检查 这部分逻辑是： 首先检查 consumerRecord 是否包含 retry_topic-backoff-timestamp，如果有则进入步骤2 现在时间是否达到了期望执行时间，if ( nowTime &gt; executeTime ) 该方法什么也不做，程序会立刻执行消费逻辑 未达到期望执行时间，准备暂停消费者对当前 TopicPartition 的消费，但是并不是在这里完成的，这个方法内部只是记录了一下需要暂停的 TopicPartition（这个数据存储在 KafkaMessageListenerContainer 的 pauseRequestedPartitions 中），并在 PartitionPausingBackoffManager 中存储了 BackOffContext，随后抛出一个异常打断消费流程 暂停分区只要 Kafka 消费线程还在运行，就会无限调用 KafkaMessageListenerContainer#pollAndInvoke pollAndInvoke 中 pausePartitionsIfNecessary 方法会根据 KafkaMessageListenerContainer 中存储的 pauseRequestedPartitions 暂停 partition，使用的方法是 Kafka Client 的 consumer.pause 调用 consumer.pause 之后，之后调用 consumer.poll 不会返回任何数据，直到调用 resume 恢复消费。该方法不会造成 Rebalance 恢复分区有了上面暂停消费的逻辑，还得有对应的恢复消费才能实现“延迟消费”，下面来看下恢复消费的逻辑 KafkaMessageListenerContainer#checkIdlePartition 方法会不断地检查 partition 是否空闲（长时间未拉取到消息）。如果符合了空闲 partition 的标准，则发送事件 ListenerContainerPartitionIdleEvent。 PartitionPausingBackoffManager 监听该事件，并尝试查找该 TopicPartition 是否存在 BackOffContext。存在则代表该分区被暂停，如果时间条件满足，从 KafkaMessageListenerContainer 的 pauseRequestedPartitions 删除该分区 最后 KafkaMessageListenerContainer#resumePartitionsIfNecessary 会将“已被 Kafka Consumer 暂停”但是“不存在于 KafkaMessageListenerContainer 的 pauseRequestedPartitions 的分区”恢复消费（通过 consumer.resume） 小结画一张图来总结一下 Retry Topic 的执行流程 这里补充说明一下 其实 MAIN_TOPIC 和 RETRY TOPIC 执行的代码是完全相同的，上图只是为了更好的让大家理解 Retry Topic 的流程 本身 Kafka 消费流程是一个无限循环 关于 Retry Topic 策略下面详细说说 Topic 策略这个事 FixedDelayStrategy.MULTIPLE_TOPICS以 test_topic 为例，此时我 attempts = 3, delay=10, multiplier=2，会额外创建以下三个 Topic test_topic-retry-0 test_topic-retry-1 test_topic-dlt 第一次消费失败，会发送到 test_topic-retry-0，消息延迟为 10s第二次消费失败，会发送到 test_topic-retry-1，消息延迟为 20s第三次消费失败，会发送到 test_topic-dlt 此时每个 Retry Topic 中的消息延迟时间是相同的，在消费时间可控的情况下，消息延迟的时间不会有过大的偏差 该策略的缺点就是，使用了过多的 Topic，但是可以实现重试时间指数级上升 FixedDelayStrategy.SINGLE_TOPIC延迟时间固定的情况适合使用 SINGLE_TOPIC 策略，该策略下只有一个 Retry Topic。如果 SINGLE_TOPIC 延迟时间指数级增长的话，很可能出现的问题是，第一条消息第三次重试延迟时间为 30s，第二条消息第一次重试延迟时间为 10s，两条消息被分配到同一分区，这二条消息被迫在 40s 之后才能重试 补充：如何使用多个 retry 线程默认情况下，Main Topic，每个 Retry Topic，DLT 分别有 1 个消费线程，默认情况下 Retry 和 DLT 会使用 KafkaListener 提供的 ContainerFactory 初始化。 例如我把 KafkaListener concurrency 设置为 4。此时 Retry Topic，每个 Retry Topic，DLT 分别有 4 个消费线程 也可以自定义 Retry Topic 消费者使用的 ContainerFactory spring-kafka 相关 demohttps://github.com/TavenYin/taven-springboot-learning/tree/master/springboot-kafka 参考 spring kafka 最新版本 =&gt; https://docs.spring.io/spring-kafka/docs/current/reference/html/#retry-config spring kafka 2.7.14 =&gt; https://docs.spring.io/spring-kafka/docs/2.7.14/reference/html/#retry-topic]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Kafka</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Gateway Filter 执行原理刨析]]></title>
    <url>%2Fpost%2F7e492ebf.html</url>
    <content type="text"><![CDATA[准备工作需要了解响应式编程，推荐阅读 『响应式编程入门之 Project Reactor』 『Project Reactor：OptimizableOperator 原理』 版本Spring Cloud Gateway：2.2.3.RELEASE 本文目标了解 Gateway Filter 内部执行原理 问题：123456789101112131415@Componentpublic class TestGlobalFilter implements GlobalFilter, Ordered &#123; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; log.info("start"); Mono&lt;Void&gt; mono = chain.filter(exchange); log.info("end") return mono; &#125; @Override public int getOrder() &#123; return 1; &#125;&#125; 我编写了一个 TestGlobalFilter，下一个 Filter 的逻辑是输出日志 HelloWorld。日志中输出的顺序会是什么样？ 正确答案是123startendHelloWorld 如果按照 Servlet 的开发思想，调用 chain.filter 一定会立刻执行下一个 Filter，Gateway 为什么不可以呢？ 因为 chain.filter 的返回值是 Mono，必须要有订阅者调用 subscribe 后才会执行发布者逻辑 DefaultGatewayFilterChain我们来看下 DefaultGatewayFilterChain 的代码 DefaultGatewayFilterChain 返回的是一个 MonoDefer。其内部包含了调用下一个 Filter 的内部函数，那么这个逻辑怎样才能触发的呢？下面继续来看 MonoDefer 的源码 MonoDefer MonoDefer subscribe 逻辑如下 调用 supplier.get()，执行内部函数的命令式代码，执行结束后，内部函数会返回一个 Mono p.subscribe(actual); 订阅内部函数返回的 Mono 当 supplier.get() 抛出异常时，首先向订阅者传递一个空的 Subscription，然后再传递异常 MonoDefer 虽然也是发布者，但是他只是在真正的发布者和订阅者之间做一个承载的作用 过滤器链刨析在理解了上述两个类之后，我们现在可以梳理一下 Gateway 过滤器链的执行逻辑了 虽然从 Gateway 接收到请求到过滤器链中间还会经历很多步骤，这里我们为了方便理解，直接把过滤器链的调用方，抽象为一个订阅者（因为最终过滤器链会返回一个 Publisher） 除此之外，再简化一下 Filter 的返回值。正常来说 Filter 可以返回任何响应式的发布者逻辑，我们这里简化为每个 Filter 都返回 chain.filter （将最简单的流程理解后，其实复杂的响应式返回也是大同小异） 订阅者请求 First Filter，这里首先会执行 filter 方法中所有的命令式的代码（响应式的代码并不会执行，因为 Mono 并没有被消费） 订阅者调用 Filter 返回的 MonoDefer 的 subscribe 方法。MonoDefer 被订阅时首先会执行内部函数。如果还有下一个过滤器，则执行并返回 nextFilter.filter，如果所有过滤器都已执行完毕则返回 Mono.empty（对应 MonoDefer 的 44 行） nextFilter.filter 先执行 filter 方法中所有的命令式的代码，然后返回 chain.filter First Filter 返回的 MonoDefer 内部会去订阅 nextFilter.filter 返回的 Mono（对应 MonoDefer 的 52 行）。Second MonoDefer（nextFilter.filter 的返回值）被订阅，接下来就是重复步骤 2 的逻辑，无限套娃下去直到所有 Filter 执行完毕… 下面用一张图来解释一下上面的逻辑 通过上述的图文讲解，我们可以看到响应式编程中一个过滤器链该怎么设计和实现 回到问题回到最开始的问题，如果想在 Spring Cloud Gateway 中实现先执行过滤器链再执行某某操作，应该怎么写呢？ 123456789101112131415161718@Slf4j@Componentpublic class LogFilter implements GlobalFilter, Ordered &#123; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; log.info("hello"); return chain.filter(exchange) .then(Mono.defer(() -&gt; &#123; log.info("world"); return Mono.empty(); &#125;)); &#125; @Override public int getOrder() &#123; return -9; &#125;&#125; Mono.then 的作用就是内部消费并忽略第一个 Mono（但是 Error 信号会被传递下去），然后入参的 Mono 作为生产者向下游传播数据。 忽略了 chain.filter 返回的 Mono 不会造成问题吗？当然不会，Gateway 的 Filter 链的订阅者并不需要我们传递什么数据，我们只需要将所有过滤器代码执行完即可 最后 如果觉得我的文章对你有帮助，动动小手点下喜欢和关注，你的支持是对我最大的帮助]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>微服务</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Project Reactor：OptimizableOperator 原理]]></title>
    <url>%2Fpost%2Facb21a09.html</url>
    <content type="text"><![CDATA[前言通常来说在响应式编程中 Publisher 的创建到真正的订阅者中间会经过许多的响应式操作符，而大部分的操作符其实都是 OptimizableOperator 的实现。 随便举几个例子，例如：map，flatMap，filter，doOnNext 等等。基本上所有对上游数据做处理的函数都实现了 OptimizableOperator 准备工作你需要对 Reactor 或者响应式编程有一定了解 推荐阅读：「响应式编程入门之 Project Reactor」 OptimizableOperator 首先 OptimizableOperator 继承了 CorePublisher，这没什么可说的，因为不管是使用 Mono 还是 Flux 执行完任何操作之后返回的依旧是一个 Publisher 下面我们来看下 OptimizableOperator 的核心方法，先来简单说下，后边会结合源码来详细理解 OptimizableOperator.subscribeOrReturn该方法有两种方式实现 返回一个 CoreSubscriber，返回的订阅者包装了下游的实际订阅者 返回 null当选择这种实现方式时，当前 OptimizableOperator 自行消费真正的 Publisher。将自己作为下游 Subscriber 的发布者 这两种实现方式有什么区别呢？第一种方式仅仅是作为一个消费者订阅。而第二种方式，Operator 相当于翻身农奴把歌唱了，自己当上了 Publisher，能做的事情比一种方式更多 OptimizableOperator.source返回当前 OptimizableOperator 的上游（上游可能还是一个 OptimizableOperator） OptimizableOperator.nextOptimizableSource返回链中的下一个 OptimizableOperator（如果上游是 OptimizableOperator 则返回，否则返回 null） InternalMonoOperator我们来通过 InternalMonoOperator 来理解一下上述的几个方法 InternalMonoOperator 构造方法InternalMonoOperator 是所有 Mono 操作符的父类，在执行操作符动作之前都会先构建操作符对象，最终会调用到 InternalMonoOperator 构造方法。 入参 source 为当前操作符的上游， super(source) 内部实现是将上游赋值给成员变量 source 如果上游同样也是操作符，还会将其赋值给 optimizableOperator subscribe当最后一个操作符被订阅时会执行如下逻辑。看似很难读，其实和上面我们讲的是一样的，下面一起来分析下 123456789101112131415161718192021222324252627282930public final void subscribe(CoreSubscriber&lt;? super O&gt; subscriber) &#123; // 将当前对象赋值给 operator OptimizableOperator operator = this; try &#123; while (true) &#123; // 调用操作符的 subscribeOrReturn // 1. 返回 != null，我们认为 operator 已经对实际订阅者做了包装。所以继续调用下一个操作符（也就是当前操作符的上游） // 2. 如果返回 == null，则代表 operator 要自己处理上游的消费和下游的订阅，方法结束 subscriber = operator.subscribeOrReturn(subscriber); if (subscriber == null) &#123; // null means "I will subscribe myself", returning... return; &#125; // 返回当前 operator 的下一个操作符（也就是返回的上游的操作符） OptimizableOperator newSource = operator.nextOptimizableSource(); if (newSource == null) &#123; // 如果所有 operator 都执行完了，那么可以直接向 Publisher 发起订阅了 // 订阅结束后，退出当前方法 operator.source().subscribe(subscriber); return; &#125; // 存在下一个操作符，继续执行该逻辑 operator = newSource; &#125; &#125; catch (Throwable e) &#123; Operators.reportThrowInSubscribe(subscriber, e); return; &#125;&#125; 在理解了 subscribe 的逻辑之后，我们在以后在阅读 Reactor 操作符的源码时，就可以清楚地知道只需要关注该操作符的 subscribeOrReturn 方法即可。 如果 subscribeOrReturn 仅是返回一个 subscriber，那么我们只需要关注其 Subscriber 的相关逻辑即可。如果返回的 null，则代表该操作符要自行消费上游然后向下游传递订阅，我们需要关注他的 Subscription 相关 最后 如果觉得我的文章对你有帮助，动动小手点下关注，你的支持是对我最大的帮助]]></content>
      <categories>
        <category>响应式编程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Reactor</tag>
        <tag>响应式编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Drools：规则加载 & 动态更新方案]]></title>
    <url>%2Fpost%2F907c3799.html</url>
    <content type="text"><![CDATA[前言本文主要想聊下这几个问题 Drools 的规则资源加载有几种方式 Drools 的规则动态更新有几种方式 版本7.69.0.Final 规则的加载1. 使用 KieClasspathContainer最简单的加载方式，官方的 demo 中使用的也是这种方式，从 classpath 下加载 kmodule 和规则资源。可以快速开始 Drools 应用开发 1.1. 引入 Drools 依赖12345678910&lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-compiler&lt;/artifactId&gt; &lt;version&gt;$&#123;drools.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-traits&lt;/artifactId&gt; &lt;version&gt;$&#123;drools.version&#125;&lt;/version&gt;&lt;/dependency&gt; 1.2. 新建 resource/META-INF/kmodule.xml123456789&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;kmodule xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.drools.org/xsd/kmodule&quot;&gt; &lt;kbase name=&quot;HelloWorldKB&quot; packages=&quot;org.example.drools.helloworld&quot;&gt; &lt;ksession name=&quot;HelloWorldKS&quot;/&gt; &lt;/kbase&gt;&lt;/kmodule&gt; 1.3. 新建 resource/org/example/drools/helloworld/hello.drl12345678910111213package org.example.drools.helloworld;rule &quot;helloworld1&quot; when then System.out.println(&quot;Hello World11111&quot;);endrule &quot;helloworld2&quot; when then System.out.println(&quot;Hello World2&quot;);end 1.4. 创建 ClasspathContainer，并触发规则12345KieServices ks = KieServices.Factory.get();kieContainer = ks.newKieClasspathContainer();KieSession kieSession = kieContainer.newKieSession("HelloWorldKS");kieSession.fireAllRules();kieSession.dispose(); 创建 ClasspathContainer 流程浅析当执行 ks.newKieClasspathContainer(); 时，会自动寻找 META-INF/kmodule.xml，用于创建 KieModule（KieModule 仅是对 KieBase 以及 KieSession 的定义） 当执行 kieContainer.newKieSession(&quot;HelloWorldKS&quot;) 时，会先创建 KieBase，此时也会去编译规则（如果你的规则文件比较大的话，这个编译过程可能会很慢）。KieBase 创建完成后，使用 KieBase 创建 KieSession ClasspathContainer 方式小结使用该方式的优点是简单、可以快速开发，但是缺点也很明显，规则和配置文件绑定在项目中（耦合度太高）。如果你不需要修改规则文件，这种方式还是可以采纳的 2. KieBuilder123456789101112KieServices ks = KieServices.Factory.get();KieFileSystem kfs = ks.newKieFileSystem();// kfskfs.write("src/main/resources/KBase1/ruleSet1.drl", drl);kfs.write("src/main/resources/META-INF/kmodule.xml", ResourceFactory.newClassPathResource("META-INF/kmodule.xml"));kfs.write("pom.xml", ResourceFactory.newFileResource("your_path/pom.xml"));KieBuilder kieBuilder = ks.newKieBuilder(kfs);kieBuilder.buildAll();// releaseId 与 pom 中声明的一致// 如果 kfs 中未写入 pom 的话，使用 ks.getRepository().getDefaultReleaseId()KieContainer kieContainer = ks.newKieContainer(releaseId); 使用这种方式可以将规则和 kmodule.xml 存储在外部，简单说下流程 使用 KieFileSystem 创建一个基于内存的虚拟文件系统，kfs 中的文件路径规范参考 ClasspathContainer 方式 KieBuilder 使用 kfs 中的 kmodule.xml 以及规则文件创建 KieModule（KieBuilder 内部再将 KieModule 保存在了 KieRepository） 通过 releaseId 创建 KieContainer，如果 kfs 中未指定 pom，则需要将 ks.getRepository().getDefaultReleaseId() 作为参数传入 当你希望把 Drools 资源外部存储时，使用 KieBuilder 是不错的方案 3. KieHelper1234Resource resource = ...;KieHelper helper = new KieHelper();helper.addResource(resource, ResourceType.DRL);KieBase kBase = helper.build(); 使用 KieHelper 可以帮你快速创建一个 KieBase，可以认为是 KieBuilder 的操作简化，内部还是使用了 KieFileSystem 和 KieBuilder，只不过在创建 KieContainer 之后新建了一个 KieBase 作为返回值 测试的时候，或者说想自己管理 KieBase 的话，可以使用这个 API，总的来说不推荐使用。 4. KieScanner这是在 Drools 官方文档中看到的一个骚操作，通过动态加载 jar 的方式来实现资源加载和动态更新，下面简单介绍下。 首先我们需要将业务服务与 Drools 资源分离成两个 jar Drools 资源 jar 具体结构如下，如果你习惯使用 drools-workbench 的话，也可以用它来创建资源 jar 12345678910111213│ pom.xml│└───src ├───main │ ├───java │ └───resources │ ├───com │ │ └───company │ │ └───hello │ │ helloworld.drl │ │ │ └───META-INF │ kmodule.xml pom 中需要注意的两点是 你需要配置一个 jar 推送的远端仓库地址（这里我直接使用的是公司内部搭建的 Nexus） 资源 jar 的 version 必须以 -SNAPSHOT 结尾 资源 jar 准备完成之后，使用命令 mvn clean deploy 将其推送到远端 下面是业务工程的操作 首先 pom 中引入 kie-ci，这里注意啊，不要引入你刚刚创建的资源 jar 12345&lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-ci&lt;/artifactId&gt; &lt;version&gt;7.69.0.Final&lt;/version&gt;&lt;/dependency&gt; 项目中加入如下代码 123456KieServices kieServices = KieServices.Factory.get();// 注意这里的 releaseId 就是对应的是你资源 jar 的 groupId，artifactId，versionReleaseId releaseId = kieServices.newReleaseId( "org.company", "drl-base", "0.1.0-SNAPSHOT" );KieContainer kContainer = kieServices.newKieContainer( releaseId );KieScanner kScanner = kieServices.newKieScanner( kContainer );kScanner.start( 10000L ); 然后启动业务服务 jar 访问业务服务验证规则是否加载 更新资源 jar 并推送至远端这时候可以看到业务进程会打出如下日志，说明规则更新成功 12022-05-19 16:43:11.223 INFO 20684 --- [ Timer-0] org.kie.api.builder.KieScanner : The following artifacts have been updated: &#123;yourjarName&#125; 验证规则更新 KieScanner 原理浅析 KieScanner 会启动一个线程，按照规定时间去扫描远端 maven 仓库（部署前要在 setting 中配置好 maven 远端仓库 url） 当发现快照时间戳发生变化时，下载到本地（具体如何动态加载的 class 这里我没太关注） 之后会新建一个 KieModule 通过 KieContainerImpl.updateToKieModule 来更新容器，本质上是更新 KBase 看到第三步后，我在想我自己是否可以利用这个 updateToKieModule 方法来实现更新 Container 呢？后来尝试了一下，证明可以 这里就不贴代码了，大概就是下面这样 123KieBuilder = ...KieModule kieModule = kieBuilder.getKieModule();kContainer.updateToKieModule((InternalKieModule) kieModule); 规则库更新1. updateToKieModule上面讲到了 KieContainerImpl.updateToKieModule 的方式来更新规则库。 2. 创建新的 KieContainer基于上面讲到的方式，其实可以想到。如果重新创建 KieContainer 的话，也相当于实现规则库动态更新。但是这种方式也存在一定问题 这是开销最大的一种方式 旧的 container 需要销毁，如果直接调用 dispose 方法清理资源可能会销毁正在使用的 kSession。 3. InternalKnowledgeBase除此之外，KieBase 的实现类本身也提供了更新以及删除的 API 123456789101112131415// 新增或者更新KnowledgeBuilder kBuilder = KnowledgeBuilderFactory.newKnowledgeBuilder();kBuilder.add(resource, ResourceType.DRL);if (kBuilder.hasErrors()) &#123; KnowledgeBuilderErrors errors = kBuilder.getErrors(); log.error(errors.toString()); return;&#125;InternalKnowledgeBase knowledgeBase = (InternalKnowledgeBase) kContainer.getKieBase(kieBaseName);knowledgeBase.addPackages(kBuilder.getKnowledgePackages());// 删除规则knowledgeBase.removeKiePackage(packageName);// 或者knowledgeBase.removeRule(packageName, ruleName); 重点说明下，如果你要更新一个规则的话，直接调用 addPackages 即可，并不需要先删除再新增（这样反而有可能造成问题） 这种方式相比上面说到的 KieContainerImpl.updateToKieModule 的方式颗粒度要小一些，updateToKieModule 会更新所有的 KBase 并发更新规则起因就是我想了解一下，KSession 正在执行时，更新 KBase 会有什么影响 举个例子具体说下 12345678910111213KnowledgeBuilder kbuilder = getKnowledgeBuilder("helloworld.drl");InternalKnowledgeBase kieBase = KnowledgeBaseFactory.newKnowledgeBase();kieBase.addPackages(kbuilder.getKnowledgePackages());KieSession kieSession = kieBase.newKieSession();kieSession.insert(1d);CompletableFuture.runAsync(() -&gt; &#123; kieBase.removeKiePackage("com.example.drools.helloworld"); log.info("remove package");&#125;).join();kieSession.fireAllRules();kieSession.dispose(); helloworld.drl 只有一个规则，在执行 fireAllRules 之前，执行了 KBase remove 操作，这会导致本次 fire 没有触发任何规则，因为此时 KBase 内部没有规则 这看起来好像挺合理的，但是如果你的本意是想先删除，再新增呢？删除 + 新增并没有一个原子操作，导致业务数据可能没有触发任何规则。 线程1 线程2 创建 kSession 并插入事实 kBase removePackage fireAllRules kBase addPackage 所以推荐尽可能不要在运行时做这种 删除 + 新增的操作 看到这时，其实我还有一个问题。当执行 kieSession.fireAllRules(); 时，规则库也允许被更新吗？ 由于篇幅问题，这里我直接说结论： fireAllRules 成功修改内部状态为 FIRING_ALL_RULES 时，任何 kBase 的修改操作会进入等待队列（等待 fire 结束） 如果 kBase 修改操作先执行了，fireAllRules 会等待 kBase 更新成功后再触发规则 所以仅是动态更新规则的话，对 Drools 的执行是没有影响的 全文总结 我觉得既然使用了规则引擎，解耦是非常重要的，所以比较推荐使用 KieBuilder 的方式来加载规则；如果你真的就不需要规则资源外部存储的话，直接使用 ks.newKieClasspathContainer(); 就可以了 如果你想使用 KieScanner 的话，一定要注意做好快照版本的管理。生产环境和开发环境不能使用同一个 maven 仓库，或者使用不同的版本防止开发环境更新影响生产环境 规则库动态更新方案的话，本文总结了三种 以创建 KieContainer 的方式，实现动态更新 使用 KieContainerImpl.updateToKieModule 使用 InternalKnowledgeBase 的 API 如果你需要动态更新 KieModule 的话，可以考虑使用 updateToKieModule 或者重新创建 KieContainer 的方式（需要注意销毁旧的 Container） 如果你仅仅是需要动态更新规则的话，可以考虑使用 InternalKnowledgeBase（该方式开销更小，需要注意不要使用删除+新增的方式）和 updateToKieModule （开销相对前者较大）]]></content>
      <categories>
        <category>Drools</category>
      </categories>
      <tags>
        <tag>Drools</tag>
        <tag>规则引擎</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[规则引擎 Drools 执行流程浅析]]></title>
    <url>%2Fpost%2Fa1c836ef.html</url>
    <content type="text"><![CDATA[什么是规则引擎 规则引擎是处理复杂规则集合的引擎。通过输入一些基础事件，以推演或者归纳等方式，得到最终的执行结果。规则引擎的核心作用在于将复杂、易变的规则从系统中抽离出来，由灵活可变的规则来描述业务需求 Drools 简介Drools 是 Java 编写的一款开源规则引擎。Drools 的核心算法基于 Rete。早些版本中，Drools 使用的是基于 Rete 二次开发的 ReteOO 算法。在 7.x 版本的 Drools 中，其内部算法已经改为使用 Phreak。Phreak 也是Drools 团队自研的算法，虽然网上关于该算法的资料很少，但是总体来说与 Rete 算法相似。阅读本文之前可以先了解下 Rete 算法 编写一个简单的规则使用 Drools 需要我们将原有的代码抽象成：Rule（规则） + Fact（事实） 首先我们先来编写一个简单的 demo 用于后文的原理学习 引入 pom 依赖 123456789101112131415&lt;properties&gt; &lt;drools.version&gt;7.62.0.Final&lt;/drools.version&gt;&lt;/properties&gt;//...&lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-compiler&lt;/artifactId&gt; &lt;version&gt;$&#123;drools.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-mvel&lt;/artifactId&gt; &lt;version&gt;$&#123;drools.version&#125;&lt;/version&gt;&lt;/dependency&gt; resource 目录下新建 order.drl 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 包名用于逻辑上区分 rulepackage com.example.drools.order;import com.example.drools.demo.HelloDrools.Orderimport com.example.drools.demo.HelloDrools.Userimport java.util.ArrayList;global java.util.List list// 指定方言为 javadialect &quot;java&quot;// 规则的组成包括，条件（when 部分）和动作（then 部分）// 当满足 when 时，会执行 then 的逻辑rule &quot;order can pay&quot; when // 要求插入的 fact 必须有一个 User 对象 // 并且 Order fact 必须满足 price &lt; $user.price $user: User() $order: Order(price &lt; $user.price) then System.out.println(&quot;username:&quot; + $user.getName() + &quot;, order price:&quot; + $order.getPrice());endrule &quot;calculate member point&quot; when $user: User(level &gt; 0) $order: Order() then Double point = $user.getPoint(); if ($user.getLevel() &gt; 10) &#123; $user.setPoint(point + $order.getPrice()); &#125; else &#123; $user.setPoint(point + $order.getPrice() * 0.5); &#125; System.out.println(&quot;previous point:&quot; + point + &quot;, present point:&quot; + $user.getPoint());endrule &quot;user age &gt; 18&quot; when $user: User(age &gt; 18) then System.out.println(&quot;user age &gt; 18&quot;);end resource 下新建 META-INF\kmodule.xml 1234&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;kmodule xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.drools.org/xsd/kmodule&quot;&gt;&lt;/kmodule&gt; java 代码部分 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.example.drools.demo;import lombok.Data;import org.kie.api.KieServices;import org.kie.api.runtime.KieContainer;import org.kie.api.runtime.KieSession;/** * @author tianwen.yin */public class HelloDrools &#123; public static void main(String[] args) &#123; // 初始化 KieServices kieServices = KieServices.Factory.get(); KieContainer kieContainer = kieServices.newKieClasspathContainer(); KieSession kieSession = kieContainer.newKieSession(); // 构建 fact User user = new User(); user.setName("taven"); user.setPoint(10D); user.setLevel(5); user.setPrice(100D); user.setAge(19); Order order = new Order(); order.setPrice(58D); // insert fact kieSession.insert(user); kieSession.insert(order); // 触发所有规则 int fireCount = kieSession.fireAllRules(); System.out.println("fireRuleCount:" + fireCount); kieSession.dispose(); &#125; @Data public static class Order &#123; private Double price; &#125; @Data public static class User &#123; private String name; private Integer age; private Double price; private Integer level; private Double point; &#125;&#125; 执行结果如下 1234username:taven, order price:58.0previous point:10.0, present point:39.0user age &gt; 18fireRuleCount:3 Drools 执行流程浅析Drools 的使用看起来还是比较简单的，但是实际上真正落地使用还是需要详读官方文档的，不是本文重点，就不多赘述了。接下来我们进入正题，分析下执行流程 上述的图，是我结合源码总结的 Drools 执行流程图，最终目的就是根据插入的 fact 进行推演，如果能走到最后的 Terminal 节点则代表规则会被执行 我们先来了解一下上图中的所有节点 Object Type Node：简称 OTN，fact 会根据类型流转到不同的 OTN Alpha Node：也被称为单输入节点，所有单对象的约束条件都会被构建为 Alpha 节点，例如 “age &gt; 18”，”leve &gt; 0” Beta Node：双输入节点，不同对象之间的约束会被构建为 Beta 节点，例如 “order.price &gt; user.price”；当一个节点需要同时满足多个单对象约束时也是 Beta 节点；一个节点有超过两个条件约束时，会构建为多个 Beta 节点相连 Beta 节点又分为 Join，Not，Exist 等，本文主要以 Join 节点为例进行说明。对于其他节点来说流程也是一样的，只不过某些具体细节的实现不同 补充一张多 Beta 节点相连的图 LeftInputAdapterNode：左输入节点，这个节点的作用我最开始也很迷惑。后来在反复 Debug 后终于顿悟了，Beta 节点被设计成只存储右侧进入的 fact，左侧的数据来自 LeftInputAdapterNode 或者另一个 Beta 节点（可能理解不了，请继续往下读） Rule Terminal：当一个 fact 流转到 Terminal 时，代表当前 fact 会触发该规则 内存结构：关于 Drools 内存结构这块，与传统 RETE 算法不太一样，我也没有太仔细的研究这块，上图中只是把会存储 fact 的位置标识了出来 实际上 Drools 的源码非常复杂，其中包含的节点远不止提到的这些，我这里仅是基于 RETE 算法的核心内容来刨析下 Drools 原理 注：这里我补充下，Beta 节点的右侧分支，在进入到 Beta 之前，也是可以有 Alpha 节点的。并且当多个 rule 中包含相同条件时也会共用分支。改图和编 demo 实在太麻烦了 准备环节 在解析规则文件时，应该就已经创建了类似上图的节点关系（这个具体源码没有阅读） 上述示例中，kieSession.insert(user); 会将 fact 插入到 PropagationList 调用 kieSession.fireAllRules(); 后，进入到规则引擎核心环节 fireAllRules字面意思已经很明显了，触发 Session 中的所有规则 flush 阶段传播 PropagationList 中所有 fact，对照上图中 flush，OTN 下游的所有分支都会遍历访问 如果某条分支全部都是 Alpha 节点的话，可以直接传播到 Terminal 节点 如果 fact 流转到 LeftInputAdapterNode 的话，会将 fact 存储在 LeftInputAdapterNode 对应的内存中 如果 fact 流转到 Beta 节点右侧的话，会将 fact 存储在 Beta 节点的右侧 当分支走到 Alpha Terminal 节点时，构建一个 RuleAgendaItem 插入到 InternalAgendaGroup 中。这个动作代表当前规则需要进行下一个阶段 evaluateAndFire Beta 节点的逻辑是，当所有的分支入口都存储了数据时，插入 InternalAgendaGroup（这句话可能不太好理解，当仅有一个 Beta 节点时，左右都存储了数据，就会插入 InternalAgendaGroup。如果是多个 Beta 节点相连的话，必须要满足第一个 Beta 的左右，以及下游所有 Beta 的右节点都有数据时才会插入 InternalAgendaGroup）。 evaluate（评估）纯 Alpha 节点的分支，是没有这个步骤的 以 Beta 节点为例，evaluate 就是基于左右内存进行匹配，找到所有配对成功的数据放入一个集合，将这个集合继续带入到下一个节点，下个节点又可能是 Beta 节点或者 Terminal 节点。 如果是 Beta 节点的话，则继续进行匹配，配对成功的集合带入到下一个节点… 如果是 Terminal 节点的话，会将数据插入到 RuleExecutor 的 tupleList 中。这步又是啥意思呢，tupleList 的数据代表，这些数据会真正的代入到规则执行当中去（Alpha Terminal 也会执行这个操作） Beta 节点这里还有一个细节，就是在进行左右配对的时候，并不只是遍历查找，而是在条件允许的情况下，Drools 在存储这些数据的时候会建立索引。上述示例的话，并没有建立索引，随便把条件改成 xx.a = yy.b 这种条件的话，就会建立索引。具体索引的实现也很简单，Drools 实现了一个类似 HashMap 的结构来管理索引，感兴趣的同学可以自己打个断点 debug 下。 断点 Class：PhreakJoinNode 注：上图中两个位置，只有一处会被执行 fire这里会遍历 RuleExecutor 的 tupleList 执行这些规则。我们的规则文件在 Drools 运行时会被编译成字节码动态执行，具体这块具体用啥实现的没研究。 fire 阶段还有一个细节就是，我们的规则文件内部是可以调用 insert modify 这些函数的，这些 fact 同样会被插入到 PropagationList 中，内部也会再执行一次 PropagationList flush 操作。整个 fireAllRules 方法内部是一个循环，如果 fire 内部的 fact 命中了规则的话，在 fire 结束后还会继续执行 evaluateAndFire 直到全部触发完为止（所以在规则编写错误的情况下，Drools 可能进入死循环） Conflict resolution冲突解决简单来说就是，当我们知道了要执行的规则都有哪些时，还需要明确这些规则执行的顺序。 Drools 这里如何解决顺序问题的呢？回顾一下上面提到的 flush 阶段。RuleAgendaItem 插入到 InternalAgendaGroup 中这一步，InternalAgendaGroup 的默认实现为 AgendaGroupQueueImpl，AgendaGroupQueueImpl 中使用了 BinaryHeapQueue（二叉堆队列）来存储元素 通过二叉堆算法保证每次队列弹出优先级最高的规则，优先级的计算通过 PhreakConflictResolver 来完成 PhreakConflictResolver 从两个方面来判断优先级 规则是否声明 salience（salience 越大，优先级越高） 无法通过 salience 来计算的话，则通过规则 loadOrder 来决定优先级（规则在文件中越靠前则 loadOrder 就越高） 总结下Drools 这种算法逻辑有什么好处呢？下述结论参考了 https://en.wikipedia.org/wiki/Rete_algorithm 通过共享节点来减少节点的冗余（如果多个 Rule 中有相同的条件，不会重复计算） fact 的变化，不需要完全重新评估，只需要进行增量评估（只需要对 fact 对应的 OTN 重新评估就可以） 支持高效的删除 fact （从 Drools 的角度来看这句话，fact 存储时会建立一个双向关联，也就是 fact 知道自己被哪些节点存储了，所以可以高效的删除） 本文介绍了 Drools 的执行流程，由于网上没有找到太多参考资料，大多数结论都是我自己总结出来的，如果有写错的地方欢迎各位指正。 最后如果觉得我的文章对你有帮助，欢迎点赞，关注，转发。你的支持是对我最大的帮助]]></content>
      <categories>
        <category>Drools</category>
      </categories>
      <tags>
        <tag>Drools</tag>
        <tag>规则引擎</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud NamedContextFactory 原理与使用]]></title>
    <url>%2Fpost%2Ff5339159.html</url>
    <content type="text"><![CDATA[最近在阅读 Ribbon 的源码，发现 SpringCloud 中 NamedContextFactory 这个类可以实现子容器。Ribbon 为每个 ServiceName 都拥有自己的 Spring Context 和 Bean 实例（不同服务之间的 LoadBalancer 和其依赖的 Bean 都是完全隔离的）。 这么做有什么好处呢 子容器之间数据隔离。不同的 LoadBalancer 只管理自己的服务实例，明确自己的职责。 子容器之间配置隔离。不同的 LoadBalancer 可以使用不同的配置。例如报表服务需要统计和查询大量数据，响应时间可能很慢。而会员服务逻辑相对简单，所以两个服务的响应超时时间可能要求不同。 子容器之间 Bean 隔离。可以让子容器之间注册不同的 Bean。例如订单服务的 LoadBalancer 底层通过 Nacos 获取实例，会员服务的 LoadBalancer 底层通过 Eureka 获取实例。也可以让不同的 LoadBalancer 采用不同的算法 NamedContextFactory 简介 NamedContextFactory 可以创建一个子容器（或者说子上下文），每个子容器可以通过 Specification 定义 Bean。 移植自 spring-cloud-netflix FeignClientFactory 和 SpringClientFactory 上面是对于 NamedContextFactory 类注释的翻译。 接下来，我会使用 NamedContextFactory 实现一个 demo，便于各位理解。 我实在没想到什么特别好的场景。所以我仿照 Ribbon 实现一个 HttpClient，每个子容器的 HttpClient 生效不同的配置，创建不同的 Bean 子容器的定制NamedContextFactory下面来进入正题 子容器需要通过 NamedContextFactory 来创建。首先我们先继承一下该类实现一个自己的 Factory12345678@Componentpublic class NamedHttpClientFactory extends NamedContextFactory&lt;NamedHttpClientSpec&gt; &#123; public NamedHttpClientFactory() &#123; super(NamedHttpClientConfiguration.class, "namedHttpClient", "http.client.name"); &#125;&#125; 解释一下上述 super 构造方法三个参数的含义 第一个参数，默认配置类。当使用 NamedHttpClientFactory 创建子容器时，NamedHttpClientConfiguration 一定会被加载 第二个参数，我目前没发现有什么用，真的就是随便定义一个 name 第三个参数，很重要。创建子容器时通常会提供子容器的容器 name。子容器中的 Environment 会被写入一条配置，http.client.name=容器name（也就是说，子容器可以通过读取配置 http.client.name 来获取容器名） 看到这可能还是很迷惑，这实际有什么用呢？以 Ribbon 为例，容器名就是 ServiceName，Ribbon 可以在配置文件中定制每个子容器的配置或者 Bean，配置如下 1234# 订单服务的超时时间为 3000orderService.ribbon.ReadTimeout = 3000# 指定 orderService 容器的 ServerListorderService.ribbon.NIWSServerListClassName = com.netflix.loadbalancer.ConfigurationBasedServerList 当每个子容器都知道自己的容器名时，就可以找到自己对应的配置了 接下来看下，我的默认配置类都干了什么123456789101112131415161718public class NamedHttpClientConfiguration &#123; @Value("$&#123;http.client.name&#125;") private String httpClientName; // 1. @Bean @ConditionalOnMissingBean public ClientConfig clientConfig(Environment env) &#123; return new ClientConfig(httpClientName, env); // 2. &#125; @Bean @ConditionalOnMissingBean public NamedHttpClient namedHttpClient(ClientConfig clientConfig) &#123; return new NamedHttpClient(httpClientName, clientConfig); // 3. &#125;&#125; @Value(&quot;${http.client.name}&quot;)，结合上边讲的，这样可以读到当前子容器的 name ClientConfig，负责根据容器 name，加载属于自己的配置。代码比较简单就不贴出来了 NamedHttpClient，简单的包装一下 HttpClient，会根据 ClientConfig 对 HttpClient 进行配置 NamedContextFactory.Specification上面讲的是可以手动编程来定制子容器的 Bean，NamedContextFactory 也提供了定制子容器的接口 NamedContextFactory.Specification。 1234567891011121314151617181920public class NamedHttpClientSpec implements NamedContextFactory.Specification &#123; private final String name; private final Class&lt;?&gt;[] configuration; public NamedHttpClientSpec(String name, Class&lt;?&gt;[] configuration) &#123; this.name = name; this.configuration = configuration; &#125; @Override public String getName() &#123; return name; &#125; @Override public Class&lt;?&gt;[] getConfiguration() &#123; return configuration; &#125;&#125; 我们简单的实现一下该接口，然后通过 NamedHttpClientFactory#setConfigurations，将 Specification 赋值给 NamedHttpClientFactory。 创建子容器时，如果容器的 name 匹配了 Specification 的 name，则会加载 Specification 对应 Configuration 类。 题外话: @RibbonClient 也是通过 NamedContextFactory.Specification 实现的 Run 一下讲到这，也许你还是没懂，没关系，建议 Debug 一下这个单元测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class NamedContextFactoryTest &#123; private void initEnv(AnnotationConfigApplicationContext parent) &#123; Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put("baidu.socketTimeout", 123); map.put("google.socketTimeout", 456); parent.getEnvironment() .getPropertySources() .addFirst(new MapPropertySource("test", map)); &#125; @Test public void test() &#123; // 创建 parent context AnnotationConfigApplicationContext parent = new AnnotationConfigApplicationContext(); // parent context 的 Bean，可以被子容器继承 parent.register(ParentConfiguration.class); initEnv(parent); parent.refresh(); // 容器 name = baidu 的 context 中会注册 TestConfiguration NamedHttpClientSpec spec = new NamedHttpClientSpec("baidu", new Class[]&#123;TestConfiguration.class&#125;); NamedHttpClientFactory namedHttpClientFactory = new NamedHttpClientFactory(); // SpringBoot 中无需手动设置，会自动注入 parent namedHttpClientFactory.setApplicationContext(parent); namedHttpClientFactory.setConfigurations(List.of(spec)); // 准备工作完成，现在开始通过 NamedContextFactory get Bean ParentBean baiduParentBean = namedHttpClientFactory.getInstance("baidu", ParentBean.class); NamedHttpClient baidu = namedHttpClientFactory.getInstance("baidu", NamedHttpClient.class); TestBean baiduTestBean = namedHttpClientFactory.getInstance("baidu", TestBean.class); Assert.assertNotNull(baiduParentBean); Assert.assertEquals("baidu", baidu.getServiceName()); Assert.assertEquals(123, baidu.getRequestConfig().getSocketTimeout()); Assert.assertNotNull(baiduTestBean); ParentBean googleParentBean = namedHttpClientFactory.getInstance("google", ParentBean.class); NamedHttpClient google = namedHttpClientFactory.getInstance("google", NamedHttpClient.class); TestBean googleTestBean = namedHttpClientFactory.getInstance("google", TestBean.class); Assert.assertNotNull(googleParentBean); Assert.assertEquals("google", google.getServiceName()); Assert.assertEquals(456, google.getRequestConfig().getSocketTimeout()); Assert.assertNull(googleTestBean); &#125; static class ParentConfiguration &#123; @Bean public ParentBean parentBean() &#123; return new ParentBean(); &#125; &#125; static class TestConfiguration &#123; @Bean public TestBean testBean() &#123; return new TestBean(); &#125; &#125; static class ParentBean &#123; &#125; static class TestBean &#123; &#125;&#125; UT 完整运行参考👉https://github.com/TavenYin/taven-springcloud-learning/blob/master/springcloud-alibaba-nacos/nacos-discovery/src/test/java/com/github/taven/NamedContextFactoryTest.java Spring 项目中使用子容器参考 👉https://github.com/TavenYin/taven-springcloud-learning/tree/master/springcloud-alibaba-nacos/nacos-discovery/src/main/java/com/github/taven/namedcontext 如果某个 Configuration 类，只需要子容器加载，那么你可以不添加 @Configuration，这样就不会被 Spring 容器（父容器）加载了。 NamedContextFactory 源码分析使用该类的入口通常是 getInstance 方法123456789101112public &lt;T&gt; T getInstance(String name, Class&lt;T&gt; type) &#123; // 1. 获取子容器 AnnotationConfigApplicationContext context = getContext(name); try &#123; // 2. 从子容器中获取 Bean return context.getBean(type); &#125; catch (NoSuchBeanDefinitionException e) &#123; // ignore &#125; return null;&#125; 这个方法内部逻辑很简单 获取子容器（如果不存在的话，会创建） 从（子）容器中获取 Bean，这步就可理解为和常规 Spring 操作一样了，从容器中获取 Bean（子容器只是概念上的一个东西，实际 API 都是一样的） 所以下面我们重点看下 getContext 方法做了什么 1234567891011121314151617181920212223242526272829303132333435363738394041public abstract class NamedContextFactory&lt;C extends NamedContextFactory.Specification&gt; implements DisposableBean, ApplicationContextAware &#123; private Map&lt;String, C&gt; configurations = new ConcurrentHashMap&lt;&gt;(); // 1. // 省略其他成员变量 protected AnnotationConfigApplicationContext createContext(String name) &#123; AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(); if (this.configurations.containsKey(name)) &#123; // 2. for (Class&lt;?&gt; configuration : this.configurations.get(name) .getConfiguration()) &#123; context.register(configuration); &#125; &#125; for (Map.Entry&lt;String, C&gt; entry : this.configurations.entrySet()) &#123; if (entry.getKey().startsWith("default.")) &#123; // 3. for (Class&lt;?&gt; configuration : entry.getValue().getConfiguration()) &#123; context.register(configuration); &#125; &#125; &#125; context.register(PropertyPlaceholderAutoConfiguration.class, this.defaultConfigType); // 4. context.getEnvironment().getPropertySources().addFirst(new MapPropertySource( this.propertySourceName, Collections.&lt;String, Object&gt;singletonMap(this.propertyName, name))); // 5. if (this.parent != null) &#123; // Uses Environment from parent as well as beans context.setParent(this.parent); // jdk11 issue // https://github.com/spring-cloud/spring-cloud-netflix/issues/3101 context.setClassLoader(this.parent.getClassLoader()); &#125; context.setDisplayName(generateDisplayName(name)); context.refresh(); return context; &#125; // 省略其他方法&#125; 该 Map 的 value 为 Specification 的实现（用于提供 Configuration），name 为容器名，用于定制每个子容器的配置 如果 name 匹配，则加载 Configuration 如果 Specification 的 name 以 default. 开头，则每个子容器创建时，都会加载这些配置 子容器中注册 PropertyPlaceholderAutoConfiguration，以及 defaultConfigType（PropertyPlaceholderAutoConfiguration 用于解析 Bean 或者 @Value 中的占位符。defaultConfigType 是上文中提到过的，构造方法中提供的子容器默认配置类） 也是我们上文中说过的，子容器中写入一条配置。以上文为例，会在容器中写入一条 http.client.name=容器name 剩下的就是，设置父容器，以及初始化操作 最后如果觉得我的文章对你有帮助，动动小手点下关注或者喜欢，你的支持是对我最大的帮助]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringCloud</tag>
        <tag>SpringBoot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring RestTemplate 设置每次请求的 Timeout]]></title>
    <url>%2Fpost%2F1a81d82f.html</url>
    <content type="text"><![CDATA[前言在实现这个功能之前，我也上网搜索了一下方案。大多数的解决方法都是定义多个 RestTemplate 设置不同的超时时间。有没有更好的方式呢？带着这个问题，我们一起来深入一下 RestTemplate 的源码 提示：本文包含了大量的源码分析，如果想直接看笔者是如何实现的，直接跳到最后的改造思路 版本SpringBoot：2.3.4.RELEASE RestTemplate RestTemplate#doExecute RestTemplate 发送请求的方法，随便找一个最后都会走到上图的 doExecute。 从上图来看，这个方法做的就是这几件事 createRequest 执行 RequestCallback 执行 Request 处理响应，将响应转换成用户声明的类型 RequestCallback 做了什么 根据 RestTemplate 中的定义 HttpMessageConverter 填充 Header Accept（支持的响应类型） 通过 HttpMessageConverter 转换 HttpBody 这里我们需要重点关注的是，createRequest 和 执行 Request 部分 createRequestRestTemplate 中的 Request 是由 RequestFactory 完成创建。所以我们先来看下获取 RequestFactory 的逻辑 如果 RestTemplate 配置了 ClientHttpRequestInterceptor（拦截器）的话，则创建 InterceptingClientHttpRequestFactory，反之则直接获取 RequestFactory 我们可以通过 RestTemplate#setInterceptors 手动添加拦截器； 当使用 @LoadBalanced 标记 RestTemplate 时，RestTemplate 中也会被加入拦截器，具体原理可以参考 org.springframework.cloud.client.loadbalancer.LoadBalancerAutoConfiguration 我们先来看下 InterceptingClientHttpRequestFactory 是什么逻辑 InterceptingClientHttpRequestFactory createRequest 方法直接返回了 InterceptingClientHttpRequest，参考 doExecute 的逻辑，接下来会执行 InterceptingClientHttpRequest#execute，其内部会执行到 InterceptingRequestExecution#execute 这里随便找一个拦截器的实现配合着来看 逻辑梳理一下： InterceptingRequestExecution 会先去执行所有的拦截器 拦截器在执行完逻辑之后，再次 InterceptingRequestExecution#execute。InterceptingRequestExecution 再次调用下一个拦截器 在拦截器逻辑执行完之后，会去调用真正的 RequestFactory 创建请求，然后执行请求 在阅读完 InterceptingRequestExecution#execute 的代码之后，我们可以发现。这里仅仅是将 request 的 uri，method，header，body 复制到了 delegate 中。说明拦截器只能对这些属性进行处理，并不能在拦截器层面添加 timeout 的相关处理。 默认情况的 RequestFactory默认情况下 RestTemplate 会使用 SimpleClientHttpRequestFactory 来创建请求，我们也可以在这个类中看到 setReadTimeout 方法。但是 SimpleClientHttpRequestFactory 并没有提供可以拓展的点，只能设置一个针对所有请求的超时时间。感兴趣的同学可以自己阅读下源码，这里就不贴出来了 HttpComponentsClientHttpRequestFactory 在阅读 HttpComponentsClientHttpRequestFactory 时，发现了可以扩展的地方。每次在创建 Request 的时候，都需要在 HttpContext 这个类中设置 RequestConfig，使用过 apache http client 的同学可能知道 RequestConfig 这个类，这个类包含了大量的属性可以定义请求的行为，这其中有一个属性 socketTimeout 正是我们所需要的。 这个类中我们可以扩展的地方就在 createHttpContext 方法中 默认情况下 createHttpContext 返回 null，然后会尝试从 HttpUriRequest 和 HttpClient 中获取 RequestConfig 赋值到 HttpContext 中。 createHttpContext 这个方法我们也来看一下 12345678910111213141516171819202122232425262728@Nullableprivate BiFunction&lt;HttpMethod, URI, HttpContext&gt; httpContextFactory;/** * Configure a factory to pre-create the &#123;@link HttpContext&#125; for each request. * &lt;p&gt;This may be useful for example in mutual TLS authentication where a * different &#123;@code RestTemplate&#125; for each client certificate such that * all calls made through a given &#123;@code RestTemplate&#125; instance as associated * for the same client identity. &#123;@link HttpClientContext#setUserToken(Object)&#125; * can be used to specify a fixed user token for all requests. * @param httpContextFactory the context factory to use * @since 5.2.7 */public void setHttpContextFactory(BiFunction&lt;HttpMethod, URI, HttpContext&gt; httpContextFactory) &#123; this.httpContextFactory = httpContextFactory;&#125;/** * Template methods that creates a &#123;@link HttpContext&#125; for the given HTTP method and URI. * &lt;p&gt;The default implementation returns &#123;@code null&#125;. * @param httpMethod the HTTP method * @param uri the URI * @return the http context */@Nullableprotected HttpContext createHttpContext(HttpMethod httpMethod, URI uri) &#123; return (this.httpContextFactory != null ? this.httpContextFactory.apply(httpMethod, uri) : null);&#125; 至此，已经很清晰了。我们可以通过调用 setHttpContextFactory 来改变 createHttpContext 的结果。 改造思路我们可以开始进行改造了，思路如下 默认的超时时间等属性，我们可以通过 HttpComponentsClientHttpRequestFactory#setHttpClient 或者 HttpComponentsClientHttpRequestFactory#setReadTimeout 来决定 在需要自定义 RequsetConfig 的场景，将 RequsetConfig 存储在 ThreadLocal 中 我们自定义的 HttpContextFactory 在读取到 ThreadLocal 中的 RequsetConfig 后，会生成一个 HttpContext，其他情况返回 null（走原来的逻辑） 代码如下123456789101112public class CustomHttpContextFactory implements BiFunction&lt;HttpMethod, URI, HttpContext&gt; &#123; @Override public HttpContext apply(HttpMethod httpMethod, URI uri) &#123; RequestConfig requestConfig = RequestConfigHolder.get(); if (requestConfig != null) &#123; HttpContext context = HttpClientContext.create(); context.setAttribute(HttpClientContext.REQUEST_CONFIG, requestConfig); return context; &#125; return null; &#125;&#125; 12345678910111213141516public class RequestConfigHolder &#123; private static final ThreadLocal&lt;RequestConfig&gt; threadLocal = new ThreadLocal&lt;&gt;(); public static void bind(RequestConfig requestConfig) &#123; threadLocal.set(requestConfig); &#125; public static RequestConfig get() &#123; return threadLocal.get(); &#125; public static void clear() &#123; threadLocal.remove(); &#125;&#125; 配置类1234567891011121314@Configurationpublic class RestTemplateConfiguration &#123; @Bean("customTimeoutRestTemplate") public RestTemplate customTimeout() &#123; RestTemplate restTemplate = new RestTemplate(); HttpComponentsClientHttpRequestFactory requestFactory = new HttpComponentsClientHttpRequestFactory(); requestFactory.setHttpContextFactory(new CustomHttpContextFactory()); requestFactory.setReadTimeout(3000); restTemplate.setRequestFactory(requestFactory); return restTemplate; &#125;&#125; 使用案例1234567891011@GetMapping("custom/setTimeout")public String customSetTimeout(Integer timeout) &#123; RequestConfig requestConfig = RequestConfig.custom().setSocketTimeout(timeout).build(); try &#123; RequestConfigHolder.bind(requestConfig); customTimeoutRestTemplate.getForObject("https://www.baidu.com", String.class); &#125; finally &#123; RequestConfigHolder.clear(); &#125; return "OK";&#125; 思路就是这样，可以将这个使用方式封装为 注解 + AOP，这样用起来会更简单。 Demo本文完整 demo：https://github.com/TavenYin/taven-springboot-learning/tree/master/springboot-restTemplate 最后如果觉得我的文章对你有帮助，动动小手点下关注或者喜欢，你的支持是对我最大的帮助]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[响应式编程入门之 Project Reactor]]></title>
    <url>%2Fpost%2Fe1d95725.html</url>
    <content type="text"><![CDATA[本文目标 理解响应式编程 前言之前的 《聊聊 IO 多路复用》 中，我们理解了非阻塞 IO 的意义。但是 Spring MVC 并不能完美的应用非阻塞编程，于是 Spring 团队开发了 WebFlux，而 WebFlux 的基础正是本文要讲到的 Project Reactor（下文简称为 Reactor） 本文以 Reactor 为例带大家入门响应式编程 版本12345&lt;dependency&gt; &lt;groupId&gt;io.projectreactor&lt;/groupId&gt; &lt;artifactId&gt;reactor-core&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt;&lt;/dependency&gt; 什么是 Reactor Reactor 是 JVM 的非阻塞响应式编程基础，支持背压。 它直接与 Java 8 函数式 API 集成，特别是 CompletableFuture、Stream 和 Duration。 它提供了可组合的异步序列 API — Flux（用于 [N] 个元素）和 Mono（用于 [0|1] 个元素），并实现了 Reactive Streams 规范。在 Reactor 的基础上还演化出了适合微服务架构的 Reactor Netty 。为 HTTP（包括 Websockets）、TCP 和 UDP 提供支持背压和响应式的网络引擎。 上面是对于官方文档的翻译。下面来说说我自己对 Reactor 和响应式编程的理解。 回想一下之前的非阻塞 IO 编程，例如我们现在要用非阻塞的方式调用一个远程服务，当远程接口数据可用时去做一些业务处理。这时候代码怎么写呢？我们需要提供一个回调函数，然后在响应就绪的时候，去调用我们的回调函数。 从逻辑上来看，这完全没有问题。但是如果我们的回调很复杂，代码看起来会是什么样呢？ 1234567891011121314151617181920212223242526272829303132333435363738// 以下案例来自 Reactor 官网userService.getFavorites(userId, new Callback&lt;List&lt;String&gt;&gt;() &#123; public void onSuccess(List&lt;String&gt; list) &#123; if (list.isEmpty()) &#123; suggestionService.getSuggestions(new Callback&lt;List&lt;Favorite&gt;&gt;() &#123; public void onSuccess(List&lt;Favorite&gt; list) &#123; UiUtils.submitOnUiThread(() -&gt; &#123; list.stream() .limit(5) .forEach(uiList::show); &#125;); &#125; public void onError(Throwable error) &#123; UiUtils.errorPopup(error); &#125; &#125;); &#125; else &#123; list.stream() .limit(5) .forEach(favId -&gt; favoriteService.getDetails(favId, new Callback&lt;Favorite&gt;() &#123; public void onSuccess(Favorite details) &#123; UiUtils.submitOnUiThread(() -&gt; uiList.show(details)); &#125; public void onError(Throwable error) &#123; UiUtils.errorPopup(error); &#125; &#125; )); &#125; &#125; public void onError(Throwable error) &#123; UiUtils.errorPopup(error); &#125;&#125;); 这个代码说实话已经有点回调地狱那味儿了，让一段不是很复杂的逻辑变得很难读了。但是如果用 Reactor 写呢？ 1234567// 以下案例来自 Reactor 官网userService.getFavorites(userId) .flatMap(favoriteService::getDetails) .switchIfEmpty(suggestionService.getSuggestions()) .take(5) .publishOn(UiUtils.uiThreadScheduler()) .subscribe(uiList::show, UiUtils::errorPopup); 可以看到，代码变得非常的简洁。唯一带来的困扰就是，我们不知道这些函数到底是啥意思 😂 响应式编程虽然有非常多的特性，但是它并不是什么神奇的技术，它也是建立在传统命令式编程的基础上。只不过它所提供的 API 以及规范更适合在非阻塞 IO 中使用。虽然在非阻塞 IO 框架中几乎只使用响应式编程（Vertx，WebFlux），只是因为这样做更合适，并不是说没了响应式编程，就玩不了非阻塞 IO 了。 响应式编程内幕Reactor 实现了 org.reactivestreams 提供的 Java 响应式编程规范，我们只要了解 reactivestreams 中代码是如何运转的，再看 Reactor 相关的代码就容易多了。 下图展示了 reactivestreams 中的核心接口 Publisher：发布者 Subscriber：订阅者 Subscription：这个单词中文翻译为名词的订阅，在代码中它是发布者和订阅者之间的媒介 Processor：该接口继承了发布者和订阅者，可以理解为发布者和订阅者的中间操作（但是 Reactor 的中间操作并没有实现 Processor，在最新版本的 Reactor 中，Processor 的相关实现接口已经被弃用） 在了解了响应式编程的核心接口之后，我们来看下响应式编程是如何运作的 在 Reactor 中大部分实现都是按照上图的逻辑来执行的 首先是Subscriber（订阅者）主动订阅 Publisher（发布者），通过调用 Publisher 的 subscribe 方法 Publisher 在向下游发送数据之前，会先调用 Subscriber 的 onSubscribe 方法，传递的参数为 Subscription（订阅媒介） Subscriber 通过 Subscription#request 来请求数据，或者 Subscription#cancel 来取消数据发布（这就是响应式编程中的背压，订阅者可以控制数据发布） Subscription 在接收到订阅者的调用后，通过 Subscriber#onNext 向下游订阅者传递数据。 在数据发布完成后，调用 Subscriber#onComplete 结束本次流，如果数据发布或者处理遇到错误会调用 Subscriber#onError 调用 Subscriber#onNext，onComplete，onError 这三个方法，可能是在 Publisher 中做的，也可能是在 Subscription 中做的，根据不同的场景有不同的实现方式，并没有什么严格的要求。可以认为 Publisher 和 Subscription 共同配合完成了数据发布 其实 Reactor 中 API 实现原理也都是这个套路，我这边也自己写了个例子便于让读者加深对响应式编程的理解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import org.reactivestreams.Publisher;import org.reactivestreams.Subscriber;import org.reactivestreams.Subscription;/** * @author tianwen.yin */public class SimpleReactiveStream &#123; /** * 实现一个简单的响应式编程发布者 * 逻辑：当订阅者发起订阅时，像下游发送一个 HelloWorld，发布逻辑由 SimpleSubscription 完成 */ static class SimplePublisher implements Publisher &#123; @Override public void subscribe(Subscriber s) &#123; // 2. Publisher 发布数据之前，调用 Subscriber 的 onSubscribe s.onSubscribe(new SimpleSubscription(data(), s)); &#125; private String data() &#123; return "Hello World"; &#125; &#125; static class SimpleSubscriber implements Subscriber &#123; @Override public void onSubscribe(Subscription s) &#123; // 3. Subscriber 通过 Subscription#request 来请求数据 // 或者 Subscription#cancel 来取消数据发布 s.request(Long.MAX_VALUE); &#125; @Override public void onNext(Object o) &#123; System.out.println(o); &#125; @Override public void onError(Throwable t) &#123; System.out.println("error"); &#125; @Override public void onComplete() &#123; System.out.println("complete"); &#125; &#125; static class SimpleSubscription implements Subscription &#123; String data; Subscriber actual; boolean isCanceled; public SimpleSubscription(String data, Subscriber actual) &#123; this.data = data; this.actual = actual; &#125; @Override public void request(long n) &#123; if (!isCanceled) &#123; try &#123; // 4. Subscription 在接收到订阅者的调用后 // 通过 Subscriber#onNext 向下游订阅者传递数据 actual.onNext(data); // 5. 在数据发布完成后，调用 Subscriber#onComplete 结束本次流 actual.onComplete(); &#125; catch (Exception e) &#123; // 如果数据发布或者处理遇到错误会调用 Subscriber#onError actual.onError(e); &#125; &#125; &#125; @Override public void cancel() &#123; isCanceled = true; &#125; &#125; public static void main(String[] args) &#123; // 1. Subscriber ”订阅“ Publisher new SimplePublisher().subscribe(new SimpleSubscriber()); &#125;&#125; 响应式编程思想响应式编程，就像装配一条流水线。Publisher 规定了数据如何生产，中间会有 Operators（操作符）对流水线的数据进行解析，校验，转换等等操作，最终处理好的数据流转到 Subscriber。 这条流水线还有一个特点。大部分情况下当 Publisher 的 subscribe 方法被调用之前，什么都不会发生。在被订阅之前我们只是在定义流水线该如何工作，直到真正有人需要的时候，流水线才会启动。 Reactor 中的 OperatorOperators 怎么理解呢？对于上游来说，Operators 像一个订阅者，而对于它的下游来说，它像一个发布者（我们上文说过了 Reactor 中的中间操作并没有实现 Processor 接口） 123Mono.just("hello") .map(a -&gt; a + "world") .subscribe(System.out::println); 举个简单的例子，在上面的代码中，map 就是一个 Operator，它的实现思路是什么？来看下面的代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 注意，这是我基于 Reactor API 实现的伪代码！public static class MonoMap implements Publisher &#123; // 我们自定义的转换逻辑 private Function mapper; // source 代表当前操作符的上游发布者 private Publisher source; public MonoMap(Publisher source, Function mapper) &#123; this.source = source; this.mapper = mapper; &#125; @Override public void subscribe(Subscriber actual) &#123; source.subscribe(new MonoMapSubscriber(mapper, actual)); &#125;&#125;public static class MonoMapSubscriber implements Subscriber &#123; // 我们自定义的转换逻辑 private Function mapper; // 真正的下游 private Subscriber actual; public MonoMapSubscriber(Function mapper, Subscriber actual) &#123; this.mapper = mapper; this.actual = actual; &#125; @Override public void onSubscribe(Subscription s) &#123; actual.onSubscribe(s); &#125; @Override public void onNext(Object o) &#123; // 当上游数据发送过来时，先进行转换再发送给下游 Object result = mapper.apply(o); actual.onNext(result); &#125; @Override public void onError(Throwable t) &#123; actual.onError(t); &#125; @Override public void onComplete() &#123; actual.onComplete(); &#125;&#125; 上述代码是我自己实现的一个伪代码，用于让大家理解操作符的实现思路，实际 Reactor 代码也是这个思路，只不过实现的更加巧妙和严谨 我们首先来分析一下 Mono.just(&quot;hello&quot;).map(a -&gt; a + &quot;world&quot;) 这句话 当执行到 Mono.just 时，会新建一个 MonoJust 对象作为当前的 Publisher。该发布者的逻辑是，当订阅时，向下游发送数据 “hello” 当执行到 map 方法时，会新建一个 MonoMap 对象替作为当前的 Publisher，MonoJust 成为了 MonoMap 中的一个属性 source（实际的上游） 当 MonoMap 被订阅时，会先将它的下游 actual 做一层包装，也就是我们上面的 MonoMapSubscriber。然后去调用 source 的 subscribe 方法。上游发布数据时，MonoMapSubscriber 先对数据进行转换（我们上面的拼接字符串操作），然后再发送给 actual（它的下游） 当 MonoMap 被再次转换时，MonoMap 就变成了下游操作符的 source… 最后通过一张图来总结一下 Reactor 该如何学习本文并没有介绍太多 Reactor 的细节，因为这些东西实在是太多了。我想聊聊我自己是如何学习 Reactor 的 如果你已经通过本文理解了响应式编程的核心接口是如何工作的了，那恭喜你已经迈向了成功的第一步了。接下来就是阅读官方文档，不断的练习和阅读 Reactor 的源码。源码追踪的方向已经很明确了，当我们想了解一个发布者的实现原理是什么，我就要去关注这个发布者的 subscribe 方法和 Subscription 都做了什么。想了解消费者的逻辑，就看它的 onNext，onComplete，onError。 最后 如果觉得我的文章对你有帮助，动动小手点下关注，你的支持是对我最大的帮助]]></content>
      <categories>
        <category>响应式编程</category>
      </categories>
      <tags>
        <tag>Reactor</tag>
        <tag>响应式编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opentracing 链路追踪实战]]></title>
    <url>%2Fpost%2F8966d8dd.html</url>
    <content type="text"><![CDATA[链路追踪的作用当系统架构从单机转变为微服务后，我们的一次后端请求，可能历经了多个服务才最终响应到客户端。如果请求按照预期正确响应还好，万一在调用链的某一环节出现了问题，排查起来是很麻烦的。但是如果有链路追踪的话，就容易很多了。 可以通过链路埋点，记录请求链中所有重要的步骤，例如与哪些数据库做了交互，调用了哪些下游服务，下游服务又与哪些数据库做了交互，又调用了哪些下游服务… 下图是 jaeger UI 为例，每次链路追踪都会产生一个唯一的 TraceId，通过该 Id 可以查看请求链路的状态 下游服务可以正常访问时 RestTemplate 无法访问下游服务时 当有了链路追踪之后。我们可以清楚的看到问题出在哪。 什么是 OpentracingOpentracing 制定了一套链路追踪的 API 规范，支持多种编程语言。虽然OpenTracing不是一个标准规范，但现在大多数链路跟踪系统都在尽量兼容OpenTracing 使用 Opentracing 时，还需要集成实现该规范的链路追踪系统，例如我们的项目正在使用 Jaeger，本文也同样以 Jaeger 为例 Opentracing 核心接口 Tracer：用于创建 Span，以及跨进程链路追踪 Span：Tracer 中的基本单元，上图中每一个蓝色或者黄色的块都是一个 Span。Span 包含的属性有 tag，log，SpanContext，BaggageItem SpanContext：Span 上下文，用于跨进程传递 Span，以及数据共享 ScopeManager：用于获取当前上下文中 Span，或者将 Span 设置到当前上下文 Scope：与 ScopeManager 配合使用，我们在后面的例子中会说明 快速开始首先，我们先部署一个 jaeger 服务。关于 jaeger 服务的更多细节，这里不多说了，各位读者可以自行去 jaeger 官网阅读 执行如下命令，启动 jaeger 服务docker run -d --name jaeger -e COLLECTOR_ZIPKIN_HOST_PORT=:9411 -p 5775:5775/udp -p 6831:6831/udp -p 6832:6832/udp -p 5778:5778 -p 16686:16686 -p 14268:14268 -p 14250:14250 -p 9411:9411 jaegertracing/all-in-one:1.23 引入 maven 依赖123456789101112131415161718&lt;!-- guava 依赖，与 jaeger 无关 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;30.1.1-jre&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.opentracing&lt;/groupId&gt; &lt;artifactId&gt;opentracing-api&lt;/artifactId&gt; &lt;version&gt;0.33.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.jaegertracing&lt;/groupId&gt; &lt;artifactId&gt;jaeger-client&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt;&lt;/dependency&gt; 记录一个简单的链路1234567891011121314151617181920212223242526272829303132333435363738394041import com.google.common.collect.ImmutableMap;import io.jaegertracing.Configuration;import io.jaegertracing.internal.JaegerTracer;import io.opentracing.Span;import io.opentracing.Tracer;import java.time.LocalDateTime;public class GettingStarter &#123; public static void main(String[] args) &#123; // 指定服务名，初始化 Tracer Tracer tracer = initTracer("starter-service"); // 指定 Span 的 operationName Span span = tracer.buildSpan("") // 指定当前 Span 的 Tag， key value 格式 .withTag("env", "local") .start(); span.setTag("system", "windows"); // log 也是 key value 格式，默认 key 为 event span.log("create first Span"); // 传入一个 Map span.log(ImmutableMap.of("currentTime", LocalDateTime.now().toString())); // 输出当前 traceId System.out.println(span.context().toTraceId()); // 结束并上报 span span.finish(); &#125; public static JaegerTracer initTracer(String service) &#123; Configuration.SamplerConfiguration samplerConfig = Configuration.SamplerConfiguration.fromEnv().withType("const").withParam(1); Configuration.ReporterConfiguration reporterConfig = Configuration.ReporterConfiguration.fromEnv().withLogSpans(true); Configuration config = new Configuration(service).withSampler(samplerConfig).withReporter(reporterConfig); return config.getTracer(); &#125;&#125; jaeger UI 查询链路，访问 http://localhost:16686/search，右上角输入 traceId 搜索。结果如下 可以看到我们刚刚在代码中的 Tag 和 Log 都记录在链路上了 线程传递 SpanSpan 之间是可以建立父子关系的，使用1234Span parent= tracer.buildSpan("say-hello").start();tracer.buildSpan("son") .asChildOf(parent) .start(); 效果如图 parent Span 肯定不能一直在调用栈中传递下去，这对集成 opentracing 的程序来说侵入性太大了。回顾一下上面我们提到了一个 ScopeManager ，来看下其核心方法 activate(Span span)：用于将当前 Span Set 到当前上下文中。上图中的注释也给出了该方法如何使用。返回值 Scope 字面翻译为范围 / 跨度。稍后我们找一个实现类具体分析一下 activeSpan()：这个方法很好理解，从当前上下文中 Get Span ThreadLocalScopeManagerJaeger 中默认的 ScopeManager，该类由 opentracing 提供。 1234567891011121314public class ThreadLocalScopeManager implements ScopeManager &#123; final ThreadLocal&lt;ThreadLocalScope&gt; tlsScope = new ThreadLocal&lt;ThreadLocalScope&gt;(); @Override public Scope activate(Span span) &#123; return new ThreadLocalScope(this, span); &#125; @Override public Span activeSpan() &#123; ThreadLocalScope scope = tlsScope.get(); return scope == null ? null : scope.span(); &#125;&#125; 1234567891011121314151617181920212223242526public class ThreadLocalScope implements Scope &#123; private final ThreadLocalScopeManager scopeManager; private final Span wrapped; private final ThreadLocalScope toRestore; ThreadLocalScope(ThreadLocalScopeManager scopeManager, Span wrapped) &#123; this.scopeManager = scopeManager; this.wrapped = wrapped; this.toRestore = scopeManager.tlsScope.get(); scopeManager.tlsScope.set(this); &#125; @Override public void close() &#123; if (scopeManager.tlsScope.get() != this) &#123; // This shouldn't happen if users call methods in the expected order. Bail out. return; &#125; scopeManager.tlsScope.set(toRestore); &#125; Span span() &#123; return wrapped; &#125;&#125; ThreadLocalScopeManager#activate：直接调用了new ThreadLocalScope。在 ThreadLocalScope 构造方法中，首先将从 ThreadLocal 中获取到目前上下文中的 ThreadLocalScope 赋值给 toRestore，然后将 this (ThreadLocalScope 对象) set 到 ThreadLocal。 ThreadLocalScope 中存储着当前的 Span。后续的代码如果想要获取 Span，只需要调用 ScopeManager#activeSpan 就可以（ScopeManager 可以在 Tracer 对象中拿到） 在执行 close 时（Scope 继承了 Closeable），将之前的 Span 重新放回到上下文中 线程传递 Span 演示1234567891011121314Span parentSpan = tracer.buildSpan("parentSpan").start();try (Scope scope = tracer.activateSpan(parentSpan)) &#123; xxxMethod();&#125; finally &#123; parentSpan.finish();&#125;public void xxxMethod() &#123; // 这里并不需要手动从 ScopeManager 中取出上下文中的 Span // start 方法中已经做了 // 如果 ScopeManager.activeSpan() != null 会自动调用 asChildOf tracer.buildSpan("sonSpan").start();&#125; 链路中数据共享如果需要和链路的下游共享某些数据，使用如下方法12345// 写span.setBaggageItem(&quot;key&quot;, &quot;value&quot;);// 读span.getBaggageItem(&quot;key&quot;); 只要保证在同一条链路中，即使下游 Span 在不同的进程，依然可以通过 getBaggageItem 读到数据 跨进程链路追踪opentracing 中提供了实现跨进程追踪的规范 Tracer 接口中提供了如下两个方法 inject：将 SpanContext 注入到 Carrier 中，传递给下游的其他进程 extract：下游进程从 Carrier 中抽取出 SpanContext，用于创建下游的 Child Span 简单举个例子解释下，例如我们使用 Http 协议访问下游服务，inject 可以将 SpanContext 注入到 HttpHeaders 中。下游服务再从 HttpHeaders 中按照链路中的约定取出有特殊标识的 header 来构建 SpanContext。这样一来就实现了链路的跨进程 再回到代码层面，通过接口方法的声明我们可以看出来，Format 决定了 Carrier 的类型。下面来看看实际代码中如何实现 跨进程链路追踪演示123456789101112131415161718192021222324252627282930313233public class TracingRestTemplateInterceptor implements ClientHttpRequestInterceptor &#123; private static final String SPAN_URI = "uri"; private final Tracer tracer; public TracingRestTemplateInterceptor(Tracer tracer) &#123; this.tracer = tracer; &#125; @Override public ClientHttpResponse intercept(HttpRequest request, byte[] body, ClientHttpRequestExecution execution) throws IOException &#123; ClientHttpResponse httpResponse; // 为当前 RestTemplate 调用，创建一个 Span Span span = tracer.buildSpan("RestTemplate-RPC") .withTag(Tags.SPAN_KIND.getKey(), Tags.SPAN_KIND_CLIENT) .withTag(SPAN_URI, request.getURI().toString()) .start(); // 将当前 SpanContext 注入到 HttpHeaders tracer.inject(span.context(), Format.Builtin.HTTP_HEADERS, new HttpHeadersCarrier(request.getHeaders())); try (Scope scope = tracer.activateSpan(span)) &#123; httpResponse = execution.execute(request, body); &#125; catch (Exception ex) &#123; TracingError.handle(span, ex); throw ex; &#125; finally &#123; span.finish(); &#125; return httpResponse; &#125;&#125; TracingRestTemplateInterceptor 实现了 RestTemplate 的拦截器，用于在 Http 调用之前，将 SpanContext 注入到 HttpHeaders 中。 Format.Builtin.HTTP_HEADERS 决定了当前的 Carrier 类型必须 TextMap （源码中可以看到，这里我没有列出） 1234567891011121314151617public class HttpHeadersCarrier implements TextMap &#123; private final HttpHeaders httpHeaders; public HttpHeadersCarrier(HttpHeaders httpHeaders) &#123; this.httpHeaders = httpHeaders; &#125; @Override public void put(String key, String value) &#123; httpHeaders.add(key, value); &#125; @Override public Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator() &#123; throw new UnsupportedOperationException("Should be used only with tracer#inject()"); &#125;&#125; tracer.inject 内部会调用 TextMap 的 put 方法，这样就将 SpanContext 注入到 HttpHeaders 了。 下面再来看看下游怎么写 123456789101112131415161718192021222324252627282930313233public class TracingFilter implements Filter &#123; private final Tracer tracer; public TracingFilter(Tracer tracer) &#123; this.tracer = tracer; &#125; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; HttpServletRequest httpRequest = (HttpServletRequest) servletRequest; HttpServletResponse httpResponse = (HttpServletResponse) servletResponse; // 通过 HttpHeader 构建 SpanContext SpanContext extractedContext = tracer.extract(Format.Builtin.HTTP_HEADERS, new HttpServletRequestExtractAdapter(httpRequest)); String operationName = httpRequest.getMethod() + ":" + httpRequest.getRequestURI(); Span span = tracer.buildSpan(operationName) .asChildOf(extractedContext) .withTag(Tags.SPAN_KIND.getKey(), Tags.SPAN_KIND_SERVER) .start(); httpResponse.setHeader("TraceId", span.context().toTraceId()); try (Scope scope = tracer.activateSpan(span)) &#123; filterChain.doFilter(servletRequest, servletResponse); &#125; catch (Exception ex) &#123; TracingError.handle(span, ex); throw ex; &#125; finally &#123; span.finish(); &#125; &#125;&#125; TracingFilter 实现了 Servlet Filter，每次请求访问到服务器时创建 Span，如果可以抽取到 SpanContext，则创建的是 Child Span 12345678910111213141516171819202122232425262728293031323334353637public class HttpServletRequestExtractAdapter implements TextMap &#123; private final IdentityHashMap&lt;String, String&gt; headers; public HttpServletRequestExtractAdapter(HttpServletRequest httpServletRequest) &#123; headers = servletHeadersToMap(httpServletRequest); &#125; @Override public Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator() &#123; return headers.entrySet().iterator(); &#125; @Override public void put(String key, String value) &#123; throw new UnsupportedOperationException("This class should be used only with Tracer.inject()!"); &#125; private IdentityHashMap&lt;String, String&gt; servletHeadersToMap(HttpServletRequest httpServletRequest) &#123; IdentityHashMap&lt;String, String&gt; headersResult = new IdentityHashMap&lt;&gt;(); Enumeration&lt;String&gt; headerNamesIt = httpServletRequest.getHeaderNames(); while (headerNamesIt.hasMoreElements()) &#123; String headerName = headerNamesIt.nextElement(); Enumeration&lt;String&gt; valuesIt = httpServletRequest.getHeaders(headerName); while (valuesIt.hasMoreElements()) &#123; // IdentityHashMap 判断两个 Key 相等的条件为 k1 == k2 // 为了让两个相同的字符串同时存在，必须使用 new String headersResult.put(new String(headerName), valuesIt.nextElement()); &#125; &#125; return headersResult; &#125;&#125; tracer.extract 内部会调用 HttpServletRequestExtractAdapter iterator 方法用于构建 SpanContext 如果你看完了这些还是对于跨进程链路追踪有疑惑的，可以下载一下我写的 Demo，通过 Debug 来更进一步了解 https://github.com/TavenYin/taven-springcloud-learning/tree/master/jaeger-mutilserver Demo 中的代码参考了 opentracing 的实现，做了相应的简化，诸位可以放心食用 实际使用opentracing 已经实现了一些常用 api 的链路埋点，在没有什么特殊需求的时候，我们可以直接使用这些代码。具体参考]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>微服务</tag>
        <tag>Opentracing</tag>
        <tag>链路追踪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊 IO 多路复用]]></title>
    <url>%2Fpost%2Fa52e1819.html</url>
    <content type="text"><![CDATA[像 Nginx 这种以高并发高性能闻名的项目，之所以性能如此优秀，其原因是使用了 IO 多路复用技术，可以用最少的进程来支持大量的请求。本文和大家一起聊聊什么是 IO 多路复用，它能带来什么 常见的 IO 模型 所有的 IO 都分为两个阶段：”等待数据就绪” 和 “拷贝到用户空间”。 我最开始不熟悉 IO 的时候，很难理解为什么有这两个过程，下面我来仔细说说 等待数据就绪：以 socket.read 为例，不管我们使用什么编程语言，这个操作都是去交给操作系统完成的（调用系统函数）。在执行系统函数时 CPU 向网卡发出 IO 请求。网卡在接收到数据之后，由网卡的 DMA 把的数据写到操作系统的内核缓冲区，完成后通知 CPU，之后 CPU 会将内核缓冲区的数据拷贝到用户空间 拷贝到用户空间：操作系统有自己的内存区域，叫做内核空间。我们平常跑的程序都在用户空间，用户空间无法直接访问内核空间的数据，所以需要拷贝。拷贝到用户空间这个过程可以理解为纯 CPU 操作，非常地快，可以认为基本不耗时 聊聊开发中 BIO 场景我们平时 Java 开发常用的框架 Spring 或者 Servlet 这都是经典的 BIO （Blocking IO，阻塞 IO）模型。 使用 Servlet 这种 BIO 的模型都需要大量的线程，其根本原因有两点 虽然 Servlet 已经支持了 NIO，但是本文还是把它作为一个经典的 BIO 模型来讨论 1. 压榨 CPU当 IO 阻塞时，CPU 处于空闲状态。想象一下，你一条 SQL 发给数据库，这个时候必须等到数据库给你响应才能继续往下执行。而 IO 本身并不是时刻都需要 CPU 的参与（例如我们上面说的等待就绪过程不需要 CPU 参与）。虽然这个过程对于人类来说可能很快，但是 CPU 确实在摸鱼。所以 BIO 模型中为了充分的利用 CPU 必须使用大量的线程（当发生 IO 阻塞时，CPU切换到其他线程继续工作） 2. 为了处理更多的连接以前的我很傻的认为一个线程只能处理一个连接。但是其实 Thread 和 Connection 这两个东西之间根本没有什么羁绊，你完全可以在一个线程中处理 N 多个请求，只不过对于靠后的请求来说，他要等待的时间太长了。所以在 BIO 模型中，大多数都是 每连接每线程 的方式 BIO 带来的瓶颈BIO 为了更充分的利用 CPU 和处理更多的连接，必须要使用大量的线程。而线程过多带来的副作用就是占用大量内存，线程上下文切换占用大量 CPU 时间片等等 那么有没有更好的方案呢？ NIO相对于 BIO 来说，NIO 调用可以立刻得到反馈，不需要再傻等了。以 read 为例，如果数据已经就绪则返回给用户；反之返回 0，永远不会阻塞。 NIO 特性貌似可以解决 BIO 的痛点，我们通过一个线程来监听所有的 socket，当 socket 就绪时，再进行读写操作，这样做可以吗？ 可以，但是需要不断的遍历所有的 socket，这样做的话效率还是有点低，有更好的办法吗？ IO 多路复用IO 多路复用可以在一个线程中监听多个文件描述符（Linux 中万物皆是文件描述符），一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。 select poll epoll这三个函数都可以用于实现 IO 多路复用，简单来聊聊这三个函数 select：当被监听的 fd（文件描述符）就绪后会返回，但是我们无法知道具体是哪些 fd 就绪了，只能遍历所有的 fd。通常来说某一时刻，就绪的 fd 并不会很多，但是使用 select 必须要遍历所有的 fd，这就造成了一定程度上的性能损失。select 最多可监听的 fd 是有限制的，32位操作系统默认1024个，64位默认2048 poll：和 select 一样，使用 poll 时也无法知道具体哪些 fd 就绪了，还是需要遍历。poll 最大的改进就是没有了监听数量的限制，但是监听了过多的 fd 会导致性能不佳 epoll：通常在 Linux 系统中使用 IO 多路复用，都是在使用 epoll 函数。epoll 是 select 和 poll 的增强，可以通知我们哪些 fd 已经就绪了，并且没有监听数量的限制。所以使用 epoll 的性能要远远优于 select 和 poll 关于这三个函数的细节，感兴趣的同学自行谷歌一下，这里我就不多说了。 基于 IO 多路复用的开发模型BIO 线程模型 传统的 BIO 模型，每当服务端 accept 到一个 socket 时，就将其分配给一个线程单独处理。 单线程 Reactor 上图展示了一个单线程的 Reactor 模型 步骤1：accept，等待事件到来（Reactor负责）步骤2：read + dispatch，将读就绪事件分发给用户定义的处理器（Reactor负责）步骤3：decode，读数据（用户处理器负责）步骤4：compute，处理数据（用户处理器负责）步骤5：encode（用户处理器负责）步骤6：send，写就绪后发送数据（Reactor负责） 为了方便大家理解，这里贴出单线程 Reactor 伪代码1234567891011121314151617181920212223242526272829303132333435// 参考自美团技术团队interface ChannelHandler&#123; void channelReadComplate(Channel channel，byte[] data); void channelWritable(Channel channel);&#125;class Channel&#123; Socket socket; Event event;//读，写或者连接&#125;//IO线程主循环：class IOThread extends Thread&#123; Map&lt;Channel, ChannelHandler&gt; handlerMap;//所有channel的对应事件处理器 public void run()&#123; Channel channel; while(channel=Selector.select())&#123;//选择就绪的事件和对应的连接 if(channel.event==accept)&#123; registerNewChannelHandler(channel);//如果是新连接，则注册一个新的读写处理器 Selector.interested(read); &#125; if(channel.event==write)&#123; getChannelHandler(channel).channelWritable(channel);//如果可以写，则执行写事件 &#125; if(channel.event==read)&#123; byte[] data = channel.read(); if(channel.read()==0)//没有读到数据，表示本次数据读完了 &#123; getChannelHandler(channel).channelReadComplate(channel, data);//处理读完成事件 &#125; &#125; &#125; &#125;&#125; 通过使用 IO 多路复用，Reactor 模型可以非阻塞的在单个线程中处理多个 Socket，这样做性能确实很不错，但是能否在此基础上充分利用 CPU 多核来实现多路 IO 复用呢？ 我们理解了单线程 Reactor 模型的工作原理之后，再来看看多线程 Reactor 模型如何工作 多线程 Reactor 1 上图是 Reactor 的多线程的一种。例如 Netty 以及基于 Netty 的一些框架（Vert.x 和 WebFlux），使用的就是类似的线程模型，可以充分利用多核 CPU mainReactor 负责 accpet。subReactor 是一组线程，在监听到 Socket 读写就绪时，进行相应的处理 需要注意的是，我们刚刚讲过的这两种 Reactor 模型，不能阻塞主 IO 线程。在 compute 过程中如果涉及 IO 调用，或者大量的计算的话，会导致整体系统吞吐量和响应时间降低。 但是不管什么系统，不涉及 IO 调用是不现实的。通常在 Vert.x 和 WebFlux 这种 Reactor 模型的框架中，执行阻塞 IO 和数据库调用都会在单独的线程池中 多线程 Reactor 2 这也是多线程 Reactor 的一种，同样可以利用 IO 多路复用非阻塞的进行 read 和 send。 和单线程 Reactor 的区别是，把 decode, compute, encode 这三个步骤交给线程池来处理 该模型的缺点也和单线程模型类似，只使用了一个线程进行 Socket 的读写，所以该模型可以再优化一下。如下图所示 和上面的多线程 Reactor 1 思路是一样的，mainReactor 负责 accept，subReactor 是一组线程负责 Socket 的读写 Tomcat NIO 模式使用的就是类似的线程模型。虽然 Tomcat 中支持了 NIO，但是为什么基于 Tomcat 的应用还是需要大量的线程？首先 Tomcat 的NIO 仅仅是作用在 Socket 的读写。其次我们业务开发中使用了太多的 BIO API（例如 JDBC），没法完全的非阻塞化 使用 IO 多路复用框架能带来什么IO 多路复用这么强，如果把业务开发全部改造成这种模型是不是性能会大幅度提升？实则不然，IO 多路复用的优势是使用更少的线程处理更多的连接，例如 Nginx，网关，这种可能需要处理海量连接转发的服务，它们就非常适合使用 IO 多路复用。IO 多路复用并不能让你的业务系统提速，但是它可以让你的系统支撑更多的连接 参考https://tech.meituan.com/2016/11/04/nio.htmlhttp://c.biancheng.net/view/2349.htmlhttps://segmentfault.com/a/1190000003063859https://www.zhihu.com/question/28594409https://www.zhihu.com/question/37271342https://juejin.cn/post/6844903492121788429https://www.pdai.tech/md/java/io/java-io-nio-select-epoll.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>多线程</tag>
        <tag>NIO</tag>
        <tag>IO 多路服用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sentinel 热点参数限流原理]]></title>
    <url>%2Fpost%2Fb174070f.html</url>
    <content type="text"><![CDATA[何为热点热点即经常访问的数据。很多时候我们希望统计某个热点数据中访问频次最高的 Top K 数据，并对其访问进行限制，比如： 商品 ID 为参数，统计一段时间内最常购买的商品 ID 并进行限制 用户 ID 为参数，针对一段时间内频繁访问的用户 ID 进行限制 版本本文基于 1.8.0 如何使用1.pom 中引入如下 1234567891011&lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-core&lt;/artifactId&gt; &lt;version&gt;$&#123;sentinel.version&#125;&lt;/version&gt;&lt;/dependency&gt; &lt;!-- 热点参数限流 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-parameter-flow-control&lt;/artifactId&gt; &lt;version&gt;$&#123;sentinel.version&#125;&lt;/version&gt;&lt;/dependency&gt; 2.定义 ParamFlowRule 123456789101112131415private static void loadRules() &#123; ParamFlowRule rule = new ParamFlowRule(RESOURCE_KEY) .setParamIdx(0) // 指定当前 rule 对应的热点参数索引 .setGrade(RuleConstant.FLOW_GRADE_QPS) // 限流的维度，该策略针对 QPS 限流 .setDurationInSec(1) // 限流的单位时间 .setCount(50) // 未使用指定热点参数时，该资源限流大小为50 .setParamFlowItemList(new ArrayList&lt;&gt;()); // item1 设置了对 goods_id = goods_uuid1 的限流，单位时间（DurationInSec）内只能访问10次 ParamFlowItem item1 = new ParamFlowItem().setObject("goods_uuid1") // 热点参数 value .setClassType(String.class.getName()) // 热点参数数据类型 .setCount(10); // 针对该value的限流值 ParamFlowRuleManager.loadRules(Collections.singletonList(rule));&#125; 这里的配置属性后文讲源码的时候都会看到，所以要重点关注一下 Rule 本身可以定义一个限流阈值，每个热点参数也可以定义自己的限流阈值 还可以为限流阀值设置一个单位时间 3.调用 12345678910111213try &#123; // 调用限流 entry = SphU.entry(RESOURCE_KEY, EntryType.IN, 1, hotParamValue); // 业务代码...&#125; catch (BlockException e) &#123; // 当前请求被限流 e.printStackTrace();&#125; finally &#123; if (entry != null) &#123; entry.exit(1, hotParamValue); &#125;&#125; 完整 demo 参考：传送门 之前有用过 Sentinel 的同学的话其实很好理解。配置方面的话 Rule 属性有些不同，调用方面，需要添加上本次调用相关的参数 举个例子，我们配置了对商品 ID = 1 的限流规则，每次请求商品接口之前调用 Sentinel 的限流 API，指定 Resource 并传入当前要访问的商品 ID。 如果 Sentinel 能找到 Resource 对应的 Rule，则根据 Rule 进行限流。Rule 中如果找到 arg 对应的热点参数配置，则使用热点参数的阈值进行限流。找不到的话，则使用 Rule 中的阈值。 实现原理Sentinel 整体采用了责任链的设计模式（类似 Servlet Filter），每次调用 SphU.entry 时，都会经历一系列功能插槽（slot chain）。不同的 Slot 职责不同，有的是负责收集信息，有的是负责根据不同的算法策略进行熔断限流操作，关于整体流程大家可以阅读下 官网 中对 Sentinel 工作流程的介绍。 ParamFlowSlot关于热点参数限流的逻辑在 com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowSlot 中 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class ParamFlowSlot extends AbstractLinkedProcessorSlot&lt;DefaultNode&gt; &#123; @Override public void entry(Context context, ResourceWrapper resourceWrapper, DefaultNode node, int count, boolean prioritized, Object... args) throws Throwable &#123; // ParamFlowManager 中没有对应的 Rule，则执行下一个Slot if (!ParamFlowRuleManager.hasRules(resourceWrapper.getName())) &#123; fireEntry(context, resourceWrapper, node, count, prioritized, args); return; &#125; // 限流检查 checkFlow(resourceWrapper, count, args); // 执行下一个Slot fireEntry(context, resourceWrapper, node, count, prioritized, args); &#125; @Override public void exit(Context context, ResourceWrapper resourceWrapper, int count, Object... args) &#123; // 执行下一个Slot fireExit(context, resourceWrapper, count, args); &#125; void applyRealParamIdx(/*@NonNull*/ ParamFlowRule rule, int length) &#123; int paramIdx = rule.getParamIdx(); if (paramIdx &lt; 0) &#123; if (-paramIdx &lt;= length) &#123; rule.setParamIdx(length + paramIdx); &#125; else &#123; // Illegal index, give it a illegal positive value, latter rule checking will pass. rule.setParamIdx(-paramIdx); &#125; &#125; &#125; void checkFlow(ResourceWrapper resourceWrapper, int count, Object... args) throws BlockException &#123; if (args == null) &#123; return; &#125; if (!ParamFlowRuleManager.hasRules(resourceWrapper.getName())) &#123; return; &#125; // 获取 resource 对应的全部 ParamFlowRule List&lt;ParamFlowRule&gt; rules = ParamFlowRuleManager.getRulesOfResource(resourceWrapper.getName()); for (ParamFlowRule rule : rules) &#123; applyRealParamIdx(rule, args.length); // 初始化该 Rule 需要的限流指标数据 ParameterMetricStorage.initParamMetricsFor(resourceWrapper, rule); // 如果不满足某个 Rule 则抛出异常，代表当前请求被限流 if (!ParamFlowChecker.passCheck(resourceWrapper, rule, count, args)) &#123; String triggeredParam = ""; if (args.length &gt; rule.getParamIdx()) &#123; Object value = args[rule.getParamIdx()]; triggeredParam = String.valueOf(value); &#125; throw new ParamFlowException(resourceWrapper.getName(), triggeredParam, rule); &#125; &#125; &#125;&#125; ParamFlowSlot 中代码不多，也没做什么事。参考注释的话应该很好理解。咱们直接挑干的讲，来看下 ParamFlowChecker 中是如何实现限流的 ParamFlowChecker 数据结构热点参数限流使用的算法为令牌桶算法，首先来看一下数据结构是如何存储的 1234567891011121314151617181920public class ParameterMetric &#123; /** * Format: (rule, (value, timeRecorder)) * * @since 1.6.0 */ private final Map&lt;ParamFlowRule, CacheMap&lt;Object, AtomicLong&gt;&gt; ruleTimeCounters = new HashMap&lt;&gt;(); /** * Format: (rule, (value, tokenCounter)) * * @since 1.6.0 */ private final Map&lt;ParamFlowRule, CacheMap&lt;Object, AtomicLong&gt;&gt; ruleTokenCounter = new HashMap&lt;&gt;(); private final Map&lt;Integer, CacheMap&lt;Object, AtomicInteger&gt;&gt; threadCountMap = new HashMap&lt;&gt;(); // 省略... &#125; Sentinel 中 Resource 代表当前要访问的资源（方法或者api接口），一个 Resource 可以对应多个 Rule，这些 Rule 可以是相同的 class。 现在再来看 ParameterMetric 的结构，每个 Resource 对应一个 ParameterMetric 对象，上述 CacheMap&lt;Object, AtomicLong&gt; 的 Key 代表热点参数的值，Value 则是对应的计数器。 所以这里数据结构的关系是这样的 一个 Resource 有一个 ParameterMetric 一个 ParameterMetric 统计了多个 Rule 所需要的限流指标数据 每个 Rule 又可以配置多个热点参数 CacheMap 的默认实现，包装了 com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap 使用该类的主要原因是为了实现热点参数的 LRU 详细解释一下，这三个变量 ruleTimeCounters ：记录令牌桶的最后添加时间，用于 QPS 限流 ruleTokenCounter ：记录令牌桶的令牌数量，用于 QPS 限流 threadCountMap ：用于线程级别限流，这个其实和令牌桶算法没有关系了，线程限流只是在 Rule 中定义了最大线程数，请求时判断一下当前的线程数是否大于最大线程，具体的应用在 ParamFlowChecker#passSingleValueCheck 实际使用 ParameterMetric 时，使用 ParameterMetricStorage 获取 Resource 对应的 ParameterMetric 12345public final class ParameterMetricStorage &#123; // Format (Resource, ParameterMetric) private static final Map&lt;String, ParameterMetric&gt; metricsMap = new ConcurrentHashMap&lt;&gt;(); // 省略相关代码 &#125; ParamFlowChecker 执行逻辑ParamFlowChecker 中 QPS 级限流支持两种策略 CONTROL_BEHAVIOR_RATE_LIMITER ：请求速率限制，对应的方法ParamFlowChecker#passThrottleLocalCheck DEFAULT ：只要桶中还有令牌，就可以通过，对应的方法ParamFlowChecker#passDefaultLocalCheck 接下来我们将以 passDefaultLocalCheck 为例，进行分析。但是在这之前，先来捋一下，从 ParamFlowSlot#checkFlow 到 ParamFlowChecker#passDefaultLocalCheck 这中间都经历了什么，详见👇 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// 伪代码，忽略了一些参数传递checkFlow() &#123; // if 没有对应的 rule，跳出 ParamFlowSlot 逻辑 // if args == null，跳出 ParamFlowSlot 逻辑 List&lt;ParamFlowRule&gt; rules = ParamFlowRuleManager.getRulesOfResource(resourceWrapper.getName()); rules.forEach(r -&gt; &#123; // 初始化该 Rule 需要的限流指标数据 ParameterMetricStorage.initParamMetricsFor(resourceWrapper, rule); if (!ParamFlowChecker.passCheck(resourceWrapper, rule, count, args)) &#123; // 抛出限流异常 &#125; &#125;)&#125;passCheck() &#123; // 从 args 中获取本次限流需要使用的 value int paramIdx = rule.getParamIdx(); Object value = args[paramIdx]; // 根据 rule 判断是该请求使用集群限流还是本地限流 if (rule.isClusterMode() &amp;&amp; rule.getGrade() == RuleConstant.FLOW_GRADE_QPS) &#123; return passClusterCheck(resourceWrapper, rule, count, value); &#125; return passLocalCheck(resourceWrapper, rule, count, value);&#125;passLocalCheck() &#123; // 如果 value 是 Collection 或者 Array // Sentinel 认为这一组数据都需要经过热点参数限流校验 // 遍历所有值调用热点参数限流校验 if (isCollectionOrArray(value)) &#123; value.forEach(v -&gt; &#123; // 当数组中某个 value 无法通过限流校验时，return false 外部会抛出限流异常 if (!passSingleValueCheck(resourceWrapper, rule, count, param)) &#123; return false; &#125; &#125;) &#125;&#125;passSingleValueCheck() &#123; if (rule.getGrade() == RuleConstant.FLOW_GRADE_QPS) &#123; if (rule.getControlBehavior() == RuleConstant.CONTROL_BEHAVIOR_RATE_LIMITER) &#123; // 速率限制 return passThrottleLocalCheck(resourceWrapper, rule, acquireCount, value); &#125; else &#123; // 默认限流 return passDefaultLocalCheck(resourceWrapper, rule, acquireCount, value); &#125; &#125; else if (rule.getGrade() == RuleConstant.FLOW_GRADE_THREAD) &#123; // 线程级限流逻辑 &#125;&#125; 上面提到了一个集群限流，和上一篇中说到的集群限流实现原理是一样的，选出一台 Server 来做限流决策，所有客户端的限流请求都咨询 Server，由 Server 来决定。由于不是本文重点，就不多说了。 ParamFlowChecker 限流核心代码铺垫了这么多，终于迎来了我们的主角 ParamFlowChecker#passDefaultLocalCheck，该方法中实现了简单的令牌桶算法，用于热点参数限流 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586static boolean passDefaultLocalCheck(ResourceWrapper resourceWrapper, ParamFlowRule rule, int acquireCount, Object value) &#123; // 根据 resource 获取 ParameterMetric ParameterMetric metric = getParameterMetric(resourceWrapper); // 根据 rule 从 metric 中获取当前 rule 的计数器 CacheMap&lt;Object, AtomicLong&gt; tokenCounters = metric == null ? null : metric.getRuleTokenCounter(rule); CacheMap&lt;Object, AtomicLong&gt; timeCounters = metric == null ? null : metric.getRuleTimeCounter(rule); if (tokenCounters == null || timeCounters == null) &#123; return true; &#125; // Calculate max token count (threshold) Set&lt;Object&gt; exclusionItems = rule.getParsedHotItems().keySet(); long tokenCount = (long)rule.getCount(); // 如果热点参数中包含当前 value，则使用热点参数配置的count，否则使用 rule 中定义的 count if (exclusionItems.contains(value)) &#123; tokenCount = rule.getParsedHotItems().get(value); &#125; if (tokenCount == 0) &#123; return false; &#125; long maxCount = tokenCount + rule.getBurstCount(); // 当前申请的流量 和 最大流量比较 if (acquireCount &gt; maxCount) &#123; return false; &#125; while (true) &#123; long currentTime = TimeUtil.currentTimeMillis(); // 这里相当于对当前 value 对应的令牌桶进行初始化 AtomicLong lastAddTokenTime = timeCounters.putIfAbsent(value, new AtomicLong(currentTime)); if (lastAddTokenTime == null) &#123; // Token never added, just replenish the tokens and consume &#123;@code acquireCount&#125; immediately. tokenCounters.putIfAbsent(value, new AtomicLong(maxCount - acquireCount)); return true; &#125; // Calculate the time duration since last token was added. long passTime = currentTime - lastAddTokenTime.get(); // A simplified token bucket algorithm that will replenish the tokens only when statistic window has passed. if (passTime &gt; rule.getDurationInSec() * 1000) &#123; // 补充 token AtomicLong oldQps = tokenCounters.putIfAbsent(value, new AtomicLong(maxCount - acquireCount)); if (oldQps == null) &#123; // Might not be accurate here. lastAddTokenTime.set(currentTime); return true; &#125; else &#123; long restQps = oldQps.get(); // 每毫秒应该生成的 token = tokenCount / (rule.getDurationInSec() * 1000) // 再 * passTime 即等于应该补充的 token long toAddCount = (passTime * tokenCount) / (rule.getDurationInSec() * 1000); // 补充的 token 不会超过最大值 long newQps = toAddCount + restQps &gt; maxCount ? (maxCount - acquireCount) : (restQps + toAddCount - acquireCount); if (newQps &lt; 0) &#123; return false; &#125; if (oldQps.compareAndSet(restQps, newQps)) &#123; lastAddTokenTime.set(currentTime); return true; &#125; Thread.yield(); &#125; &#125; else &#123; // 直接操作计数器扣减即可 AtomicLong oldQps = tokenCounters.get(value); if (oldQps != null) &#123; long oldQpsValue = oldQps.get(); if (oldQpsValue - acquireCount &gt;= 0) &#123; if (oldQps.compareAndSet(oldQpsValue, oldQpsValue - acquireCount)) &#123; return true; &#125; &#125; else &#123; return false; &#125; &#125; Thread.yield(); &#125; &#125;&#125; 令牌桶算法核心思想如下图所示，结合这个图咱们再来理解理解代码 核心逻辑在 while 循环中，咱们直接挑干的讲 先回顾一下上面说过 tokenCounters 和 timeCounters，在默认限流实现中，这两个参数分别代表最后添加令牌时间，令牌剩余数量 while 逻辑： 首先如果当前 value 对应的令牌桶为空，则执行初始化 计算当前时间到上次添加 token 时间经历了多久，即 passTime = currentTime - lastAddTokenTime.get() 用于判断是否需要添加 token 2.1) if (pass &gt; rule 中设定的限流单位时间) ，则使用原子操作为令牌桶补充 token（具体补充 token 的逻辑详见上面代码注释） 2.2) else 不需要补充 token，使用原子操作扣减令牌 可以看到关于 token 的操作全是使用原子操作（CAS），保证了线程安全。如果原子操作更新失败，则会继续执行。 速率限制的实现再顺便叨咕下上面说过CONTROL_BEHAVIOR_RATE_LIMITER 速率限制策略是如何实现的，只简单说说思路，具体细节大家可以自己看下源码 该策略中，仅使用 timeCounters，该参数存储的数据变成了 lastPassTime（最后通过时间），所以这个实现和令牌桶也没啥关系了 新的请求到来时，首先根据 Rule 中定义时间范围，count 计算 costTime，代表每隔多久才能通过一个请求1long costTime = Math.round(1.0 * 1000 * acquireCount * rule.getDurationInSec() / tokenCount); 只有 lastPassTime + costTime &lt;= currentTime ，请求才有可能成功通过，lastPassTime + costTime 过大会导致限流。 最后 如果觉得我的文章对你有帮助，动动小手点下关注，你的支持是对我最大的帮助]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>源码</tag>
        <tag>分布式</tag>
        <tag>Sentinel</tag>
        <tag>限流</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[权重随机（Weight random）算法详解]]></title>
    <url>%2Fpost%2F25a528d8.html</url>
    <content type="text"><![CDATA[应用场景 客户端负载均衡，例如 Nacos 提供的客户端负载均衡就是使用了该算法 游戏抽奖（普通道具的权重很高，稀有道具的权重很低） 本文目标Java 实现权重随机算法 算法详解比如我们现在有三台 Server，权重分别为1，3，2。现在想对三台 Server 做负载均衡 1234Server1 Server2 Server3 weight weight weight 1 3 2 权重比例 我们算出每台 Server 的权重比例，权重比例 = 自己的权重 / 总权重 1234567server1 server2 server3 weight weight weight 1 3 2 radio radio radio 1/6 3/6 2/6 根据权重比例计算覆盖区域 123456 server1 server2 server3 ^ ^ ^|---------||---------|---------|---------||---------|---------||0 1/6 4/6 6/6 ^ ^ ^ 0.16666667 0.66666667 1.0 根据权重负载均衡 如步骤2所示，每个 server 都有自己的范围，把每一个格子作为单位来看的话 server1 (0,1] server2 (1,4] server3 (4,6] 使用随机数函数，取 (0,6] 之间的随机数，根据随机数落在哪个范围决定如何选择。例如随机数为 2，处于 (1,4] 范围，那么就选择 server2。 思路大概就是这样，落实到代码上，用一个数组 [0.16666667, 0.66666667, 1] 来表示这三个 server 的覆盖范围，使用 ThreadLocalRandom 或者 Random 获取 [0,1) 内的随机数。然后使用二分查找法快速定位随机数处于哪个区间 Java 实现代码基本上与 com.alibaba.nacos.client.naming.utils.Chooser 一致，在可读性方面做了下优化。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134import java.util.*;import java.util.concurrent.ThreadLocalRandom;import java.util.concurrent.atomic.AtomicInteger;public class WeightRandom&lt;T&gt; &#123; private final List&lt;T&gt; items = new ArrayList&lt;&gt;(); private double[] weights; public WeightRandom(List&lt;ItemWithWeight&lt;T&gt;&gt; itemsWithWeight) &#123; this.calWeights(itemsWithWeight); &#125; /** * 计算权重，初始化或者重新定义权重时使用 * */ public void calWeights(List&lt;ItemWithWeight&lt;T&gt;&gt; itemsWithWeight) &#123; items.clear(); // 计算权重总和 double originWeightSum = 0; for (ItemWithWeight&lt;T&gt; itemWithWeight : itemsWithWeight) &#123; double weight = itemWithWeight.getWeight(); if (weight &lt;= 0) &#123; continue; &#125; items.add(itemWithWeight.getItem()); if (Double.isInfinite(weight)) &#123; weight = 10000.0D; &#125; if (Double.isNaN(weight)) &#123; weight = 1.0D; &#125; originWeightSum += weight; &#125; // 计算每个item的实际权重比例 double[] actualWeightRatios = new double[items.size()]; int index = 0; for (ItemWithWeight&lt;T&gt; itemWithWeight : itemsWithWeight) &#123; double weight = itemWithWeight.getWeight(); if (weight &lt;= 0) &#123; continue; &#125; actualWeightRatios[index++] = weight / originWeightSum; &#125; // 计算每个item的权重范围 // 权重范围起始位置 weights = new double[items.size()]; double weightRangeStartPos = 0; for (int i = 0; i &lt; index; i++) &#123; weights[i] = weightRangeStartPos + actualWeightRatios[i]; weightRangeStartPos += actualWeightRatios[i]; &#125; &#125; /** * 基于权重随机算法选择 * */ public T choose() &#123; double random = ThreadLocalRandom.current().nextDouble(); int index = Arrays.binarySearch(weights, random); if (index &lt; 0) &#123; index = -index - 1; &#125; else &#123; return items.get(index); &#125; if (index &lt; weights.length &amp;&amp; random &lt; weights[index]) &#123; return items.get(index); &#125; // 通常不会走到这里，为了保证能得到正确的返回，这里随便返回一个 return items.get(0); &#125; public static class ItemWithWeight&lt;T&gt; &#123; T item; double weight; public ItemWithWeight() &#123; &#125; public ItemWithWeight(T item, double weight) &#123; this.item = item; this.weight = weight; &#125; public T getItem() &#123; return item; &#125; public void setItem(T item) &#123; this.item = item; &#125; public double getWeight() &#123; return weight; &#125; public void setWeight(double weight) &#123; this.weight = weight; &#125; &#125; public static void main(String[] args) &#123; // for test int sampleCount = 1_000_000; ItemWithWeight&lt;String&gt; server1 = new ItemWithWeight&lt;&gt;("server1", 1.0); ItemWithWeight&lt;String&gt; server2 = new ItemWithWeight&lt;&gt;("server2", 3.0); ItemWithWeight&lt;String&gt; server3 = new ItemWithWeight&lt;&gt;("server3", 2.0); WeightRandom&lt;String&gt; weightRandom = new WeightRandom&lt;&gt;(Arrays.asList(server1, server2, server3)); // 统计 (这里用 AtomicInteger 仅仅是因为写起来比较方便，这是一个单线程测试) Map&lt;String, AtomicInteger&gt; statistics = new HashMap&lt;&gt;(); for (int i = 0; i &lt; sampleCount; i++) &#123; statistics .computeIfAbsent(weightRandom.choose(), (k) -&gt; new AtomicInteger()) .incrementAndGet(); &#125; statistics.forEach((k, v) -&gt; &#123; double hit = (double) v.get() / sampleCount; System.out.println(k + ", hit:" + hit); &#125;); &#125;&#125; 这里重点说一下 Arrays.binarySearch(weights, random)，这个 API 我之前没有用过导致我在读 Nacos 源码时，对这块的操作十分费解 来看一下 java API 文档对该方法返回值的解释 Returns:index of the search key, if it is contained in the array; otherwise, (-(insertion point) - 1). The insertion point is defined as the point at which the key would be inserted into the array: the index of the first element greater than the key, or a.length if all elements in the array are less than the specified key. Note that this guarantees that the return value will be &gt;= 0 if and only if the key is found. 解释下，首先该方法的作用是通过指定的 key 搜索数组。（前提条件是要保证数组的顺序是从小到大排序过的） 如果数组中包含该 key，则返回对应的索引 如果不包含该 key，则返回该 key 的 (-(insertion point)-1) insertion point（插入点）：该 key 应该在数组的哪个位置。举个例子，数组 [1,3,5]，我的搜索 key 为 2，按照顺序排的话 2 应该在数组的 index = 1 的位置，所以此时 insertion point = 1。 （这里 jdk 将能查到 key 和 查不到 key 两种情况做了区分。为了将未找到的情况全部返回负数，所以做了 (-(insertion point)-1) 这样的操作） 看到这，我们就懂了，insertion point 就是我们需要的，现在我们用小学数学来推导一下如何计算 insertion point 12345678// 小学数学推导一下 insertion point 如何计算returnValue = (- (insertionPoint) - 1)insertionPoint = (- (returnValue + 1) )// 所以就有了上边代码中的if (index &lt; 0) &#123; index = -index - 1;&#125; 参考https://github.com/alibaba/nacos/blob/develop/client/src/main/java/com/alibaba/nacos/client/naming/utils/Chooser.java]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Sentinel 中的限流算法]]></title>
    <url>%2Fpost%2F690c857.html</url>
    <content type="text"><![CDATA[最近在学习 Sentinel，深入学习了源码之后分享一下心得 Sentinel 版本1.8.0 固定窗口算法先介绍一下最简单的限流算法 每个窗口都有一个计数器（counter）用于统计流量，如果 counter + 本次申请的请求数 &gt; 预设的 QPS，则拒绝请求。 固定窗口很简单，但是也有很大的问题 假设我们规定 QPS 不能超过 100，如上图所示 r1 和 r2 两个时间点分别来了 60 个请求， QPS 已经大于 100 了。此时应该触发限流了，但是固定窗口算法傻傻的只关注自己窗口的流量，感知不到 QPS 已经超了 滑动窗口算法 该算法将单位时间切成了多个窗口，每次计算 QPS 时，计算 当前窗口 + 过去几个窗口 的流量总和，这样就避免了固定窗口的问题（具体使用几个窗口，取决于窗口大小和单位时间大小。例如上图，每个窗口大小为 500 ms，以 1 s 为单位时间做限流，每次使用 current + last 即可） 算法实现细节思考理解算法思路之后，接下来要思考如何实现这个算法了 首先我们需要有一个上图中的时间轴，来记录时间窗口，可以通过数组来实现这个时间轴。 时间轴有了，我们再来考虑一下时间窗口。 每个时间窗口肯定要有一个线程安全的计数器以及当前窗口对应的时间 12345678910// 时间轴List&lt;Window&gt; timeline = new ArrayList&lt;&gt;();// 每个窗口的大小int windowTime;// 时间窗口class Window &#123; Timestamp startTime; AtomicInteger counter;&#125; 但是如果仔细一想，还是存在一些问题的 由于时间是会一直增长的，那我们的数组怎么办？也要跟着时间无限的增大吗？ 旧的时间窗口（例如几秒之前的）在之后的计算不会再用到了，如何清理这些无用的窗口？ Sentinel 中滑动窗口算法如何实现的带着上述的问题与思考来看下 Sentinel 中是如何实现的 LeapArraySentinel 中滑动窗口算法的核心类，首先来了解一下他的核心成员变量 123456789101112131415161718192021222324public abstract class LeapArray&lt;T&gt; &#123; // 要统计的单位时间大小，例如计算QPS时，为1000 protected int intervalInMs; // 样本数量 protected int sampleCount; // 窗口大小 该值 = intervalInMs / sampleCount protected int windowLengthInMs; // 存储时间窗口的数组 protected final AtomicReferenceArray&lt;WindowWrap&lt;T&gt;&gt; array; public LeapArray(int sampleCount, int intervalInMs) &#123; AssertUtil.isTrue(sampleCount &gt; 0, "bucket count is invalid: " + sampleCount); AssertUtil.isTrue(intervalInMs &gt; 0, "total time interval of the sliding window should be positive"); AssertUtil.isTrue(intervalInMs % sampleCount == 0, "time span needs to be evenly divided"); this.windowLengthInMs = intervalInMs / sampleCount; this.intervalInMs = intervalInMs; this.sampleCount = sampleCount; this.array = new AtomicReferenceArray&lt;&gt;(sampleCount); &#125; &#125; 单机限流在统计 QPS 时，默认 sampleCount = 2，intervalInMs = 1000，windowLengthInMs = 500 LeapArray#calculateTimeIdx大体思路相同，同样是利用一个数组实现时间轴，每个元素代表一个时间窗口 Sentinel 中 数组长度是固定的，通过方法 LeapArray#calculateTimeIdx 来 确定时间戳在数组 中的位置 （找到时间戳对应的窗口位置） 怎么理解这个方法呢？ 我们把数据带入进去，假设 windowLengthInMs = 500 ms （每个时间窗口大小是 500 ms） 如果 timestamp 从 0 开始的话，每个时间窗口为 [0,500) [500,1000) [1000,1500) … 这时候先不考虑 timeId % array.length() ，也不考虑数组长度。假设当前 timeMillis = 601，将数值代入到 timeMillis / windowLengthInMs 其实就可以确定出当前的 timestamp 对应的时间窗口在数组中的位置了 由于数组长度是固定的，所以再加上求余数取模来确定时间窗在数组中的位置 LeapArray#currentWindow先来看一下 Sentinel 中 Window 的结构，基本和我们上面想的一致，计数器使用了泛型，可以更灵活12345678910111213141516171819public class WindowWrap&lt;T&gt; &#123; /** * Time length of a single window bucket in milliseconds. */ private final long windowLengthInMs; /** * Start timestamp of the window in milliseconds. */ private long windowStart; /** * Statistic data. */ private T value; // 省略。。。&#125; 继续说 currentWindow，该方法根据传入的 timestamp 找到 或者 创建 这个时间戳对应的 Window 这个方法源码中注释很多，我删除了部分注释1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public WindowWrap&lt;T&gt; currentWindow(long timeMillis) &#123; if (timeMillis &lt; 0) &#123; return null; &#125; int idx = calculateTimeIdx(timeMillis); // Calculate current bucket start time. long windowStart = calculateWindowStart(timeMillis); /* * Get bucket item at given time from the array. * * (1) Bucket is absent, then just create a new bucket and CAS update to circular array. * (2) Bucket is up-to-date, then just return the bucket. * (3) Bucket is deprecated, then reset current bucket and clean all deprecated buckets. */ while (true) &#123; WindowWrap&lt;T&gt; old = array.get(idx); if (old == null) &#123; WindowWrap&lt;T&gt; window = new WindowWrap&lt;T&gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis)); if (array.compareAndSet(idx, null, window)) &#123; // Successfully updated, return the created bucket. return window; &#125; else &#123; // Contention failed, the thread will yield its time slice to wait for bucket available. Thread.yield(); &#125; &#125; else if (windowStart == old.windowStart()) &#123; return old; &#125; else if (windowStart &gt; old.windowStart()) &#123; if (updateLock.tryLock()) &#123; try &#123; // Successfully get the update lock, now we reset the bucket. return resetWindowTo(old, windowStart); &#125; finally &#123; updateLock.unlock(); &#125; &#125; else &#123; // Contention failed, the thread will yield its time slice to wait for bucket available. Thread.yield(); &#125; &#125; else if (windowStart &lt; old.windowStart()) &#123; // Should not go through here, as the provided time is already behind. return new WindowWrap&lt;T&gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis)); &#125; &#125;&#125; 方法逻辑分析如下： 首先要做的两件事 计算 timestamp 在数组中的位置，就是我们上文说的 calculateTimeIdx 计算 timestamp 的 windowStart（窗口开始时间），通过 timeMillis - timeMillis % windowLengthInMs，这个值在后边会用到 然后进入一个 while(true) 循环， 通过 WindowWrap&lt;T&gt; old = array.get(idx) 找出对应的窗口，接下来就是三种情况了 old == null 这个时候代表数组中还没有这个 window，创建这个 window 加入到数组中（由于此时可能会有多个线程同时添加数组元素，所以一定要保证线程安全，所以这里使用的数组为 AtomicReferenceArray），添加成功后返回新建的 window windowStart == old.windowStart() window 已经存在了，直接返回即可 windowStart &gt; old.windowStart() 代表数组中的元素已经至少是 25s 之前的了，重置当前窗口的 windowStart 和 计数器，这个操作同样也是一个多线程操作，所以使用了 updateLock.tryLock()。 仔细看了代码后，我提出了一个问题。我觉得这个地方并不能一定保证能锁住。会不会出现两个线程同时判断需要更新，由于一个线程很快执行成功并释放了锁，第二个线程也成功获取到 Lock，会执行多次 resetWindow。我认为需要再 tryLock 之后再判断一下执行条件，目前已经给 Sentinel 提交了 Issue windowStart &lt; old.windowStart() 通常情况下不会走到这个逻辑分支，上面源码的注释也是这样解释的 LeapArray#values上文中提到过，计算流量时具体使用几个窗口，取决于窗口大小和单位时间大小 该方法的作用通过传入一个时间戳，找出本次计算所需的所有时间窗口 123456789101112131415161718192021 public List&lt;T&gt; values(long timeMillis) &#123; if (timeMillis &lt; 0) &#123; return new ArrayList&lt;T&gt;(); &#125; int size = array.length(); List&lt;T&gt; result = new ArrayList&lt;T&gt;(size); for (int i = 0; i &lt; size; i++) &#123; WindowWrap&lt;T&gt; windowWrap = array.get(i); if (windowWrap == null || isWindowDeprecated(timeMillis, windowWrap)) &#123; continue; &#125; result.add(windowWrap.value()); &#125; return result; &#125; public boolean isWindowDeprecated(long time, WindowWrap&lt;T&gt; windowWrap) &#123;// intervalInMs 在单机限流计算QPS时默认为 1000(ms) return time - windowWrap.windowStart() &gt; intervalInMs; &#125; values 的逻辑没什么可说的，遍历数组将时间符合的窗口加入到 List 中 重点看一下 isWindowDeprecated 这个方法 还是像上面那样把数值带进去。每个窗口大小为 500 ms，例如 timestamp 为 1601，这个 timestamp 对应的 windowStart 为 1500，此时 (1601 - 1500 &gt; 1000) = false 即这个窗口是有效的，再往前推算，上一个窗口 windowStart 为 1000 也是有效的。再往前推算，或者向后推算都是无效的窗口。 intervalInMs 我是这样理解的，以多长的时间段作为单位时间来限流。即可以以 1s 为一个时间段来做限流，也可以以 60s 为一个时间段来限流。 Sentinel 限流思路在理解了 LeapArray#currentWindow 和 LeapArray#values 方法的细节之后，其实我们就可以琢磨出限流的实现思路了 首先根据当前时间戳，找到对应的几个 window，根据 所有 window 中的流量总和 + 当前申请的流量数 决定能否通过 如果不能通过，抛出异常 如果能通过，则对应的窗口加上本次通过的流量数 Sentinel 限流实现Sentinel 基本也是这个思路，只不过逻辑复杂一些，这里贴出几处代码，感兴趣的同学可以自己 debug 一下 Sentinel 限流检查根据 Sentinel 文档中的解释，我们可以知道负责限流的类为 FlowSlot，FlowSlot 会使用 FlowRuleChecker 来检查当前资源是否需要限流 FlowSlot#entry FlowRuleChecker#checkFlow 根据 FlowRule 的设定来做限流检查，这中间我省略了几段代码，默认情况没有设置 ControlBehavior 会使用 DefaultController#canPass 做限流检查。如下图，通过判断 当前流量数 + 申请的数量 是否大于预设的数量，来决定是否限流 注：当使用 SphU.entry 时 prioritized = false，使用 SphU.entryWithPriority 时 prioritized = true。node.tryOccupyNext 的含义：如果想占用未来的时间窗口令牌，需要等待多久（上图中的waitInMs）。如果小于规定的超时时间，则记录正在等待的请求数，然后执行 sleep(waitInMs)，外层捕获到 PriorityWaitException 会自己处理掉，然后执行用户逻辑，用户完全无感知。 通过上图 avgUsedTokens 可以看到，当 Rule 的 grade 为 FLOW_GRADE_QPS 时，会调用 node.pass()。这里调用的具体实现为 StatisticNode#passQps，如下图 rollingCounterInSecond.getWindowIntervalInSec() 计算 QPS 时为 1 秒 rollingCounterInSecond.pass() 计算 QPS 时，最多返回两个窗口的通过请求数（currentWindow + lastWindow） rollingCounterInSecond#pass 首先先尝试是否需要创建当前的时间窗口，然后找到相关的窗口，计算流量总和。 Sentinel 请求记录代码位置 StatisticSlot#entry，fireEntry 会根据我们配置的规则进行检查（例如上述的限流）。 如果检查没有抛出异常，则记录线程数和申请的请求数（限流检查依赖的数据就是这里记录的）。 集群限流集群限流有什么用在没有集群限流之前，如果想把整个服务的 QPS 限制在某个值。举个例子现在某 Server 有十个实例，我们希望总 QPS 不超过 100，这时我们的做法是把每个实例的 QPS 设置为 10。 在理想情况下，这样做可以将 QPS 控制在 100。但是如果每台 Server 分配到的流量不均匀。这可能会导致总量在没达到 100 的时候，某些 Server 就开始限流了。 这种情况就需要 Sentinel 的集群限流出场了。 集群限流原理由于篇幅限制，我们这里不讨论如何搭建集群限流，只是来说说 Sentinel 如何在这一基础上做的集群限流。 思路很简单，选出一个 Token Server。在开启集群限流后，所有的 Client 在需要限流时，询问 Token Server，Server 决定当前请求是否限流。具体的实现细节与单机限流略有不同，但是核心的算法还是使用的 LeapArray 这里也是给出几处源码位置，感兴趣的同学自行阅读一下 Client 端根据 Rule 决定本次使用本地限流还是集群限流，FlowRuleChecker#canPassCheck Server 端，DefaultTokenService#requestToken 并发下限流的问题在完整的阅读完单机和集群的限流代码之后，发现了一个问题，限流流程可以简化为如下1234567891011121314// 伪代码// 最大QPSint maxCount;// 当前申请的流量数int aquireCount;int passQps = getPassQPS();if (passQps + aquireCount &lt;= maxCount) &#123; addPass(aquireCount);&#125; else &#123; // 限流处理&#125; 由于没有并发控制，并发场景下会出现，多个线程同时满足 passQps + aquireCount &lt;= maxCount，然后增加流量统计，这样的话，没法保证一定将 QPS 控制在 maxCount，并发的情况下会出现实际流量超出预设 QPS 的情况。 这肯定不是个Bug。这里没有并发控制可能是出于性能考虑，在性能和准确度可以接受的情况下做了一个折中 所以在使用时，如果实际 QPS 高于预设值，可能是并发导致的 demo 单机限流： https://github.com/TavenYin/taven-springcloud-learning/blob/master/sentinel-example/src/main/java/com/github/taven/limit/SentinelExample.java 集群限流：https://github.com/TavenYin/taven-springcloud-learning/blob/master/sentinel-example/src/main/java/com/github/taven/limit/SentinelClusterEmbedded.java]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>源码</tag>
        <tag>分布式</tag>
        <tag>Sentinel</tag>
        <tag>限流</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊 Java GC 算法]]></title>
    <url>%2Fpost%2F1e4a5faa.html</url>
    <content type="text"><![CDATA[Java 和 C++ 之间有一堵由内存动态分配和垃圾回收技术所围成的高墙，墙外面的人想进去，墙里面的人却想出来 今天来聊聊 Java GC（Garbage Collection，垃圾回收）中的常见算法 引用与GC的关系正题开始前，先来了解一下 Java 中的引用。对象使用不同的引用类型，决定 GC 发生时是否会回收它 引用类型 特点 强引用（Strong Reference） Java中的默认引用类型。例如Object obj = new Object()，只要强引用存在，对象永远不会被回收 软引用（Soft Reference） 内存足够时，软引用不会被回收。只有当系统要发生内存溢出时，才会被回收。适合用于缓存场景 弱引用（Weak Reference） 只要发生垃圾回收，弱引用的对象就会被回收 虚引用（Phantom Reference） 一个对象有虚引用的存在不会对生存时间都构成影响，也无法通过虚引用来获取对一个对象的真实引用。唯一的用处：能在对象被GC时收到系统通知 如何判断对象是否可以被回收？在GC开始之前，首先要做的事就是确定哪些对象『活着』，哪些对象『已死』 常见的两种算法用于判断该对象是否可以被回收 引用计数算法：每个对象中添加引用计数器。每当对象被引用，引用计数器就会加 1；每当引用失效，计数器就会减 1。当对象的引用计数器的值为 0 时，就说明该对象不再被引用，可以被回收了。强调一点，虽然引用计数算法的实现简单，判断效率也很高，但它存在着对象之间相互循环引用的问题（所以在后来的JVM版本中已经不采用这种算法了）。 可达性分析算法：GC Roots 是该算法的基础，GC Roots 是所有对象的根对象。这些对象作为正常对象的起始点，在垃圾回收时，会从这些 GC Roots 开始向下搜索，当一个对象到 GC Roots 没有任何引用链相连时，就证明此对象不再被引用。目前 HotSpot 虚拟机采用的就是这种算法。 可以作为 GC Roots 的对象： 虚拟机栈中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中 JNI（Native 方法）引用的对象 Java 虚拟机中内部的引用 synchronized 持有的对象 JMXBean、JVMTI 中注册的回调、本地代码缓存 除此之外，不同的垃圾回收器可能还会加入一些『临时的』对象共同构建 GC Roots 基础的 GC 算法标记-清除算法 （mark-sweep）如下图所示，该算法分为『标记』和『清除』两个阶段 从 GC Roots 出发标记出存活的对象，然后遍历堆清除未被标记的对象 最基础的收集算法，标记-清除的效率中等，缺点也比较明显：会产生内存碎片 复制算法（copying）将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块内存用完，需要进行垃圾收集时，就将存活者的对象复制到另一块上面，然后将第一块内存全部清除 相比较『标记-清除』不会有内存碎片的问题，但每次只使用一半的内存，内存利用率很低。当内存中存活的对象较多时，会进行大量的复制操作，效率会较低（对象的复制也是有成本的，需要复制的对象越多、越大，复制带来的代价也就越大）。适用于存活率低的情况 标记-整理算法（mark-compact）和『标记-清除』算法有点类似，从 GC Roots 出发标记出存活的对象，然后是整理阶段，将存活移动到一端。整理阶段结束后我们可以知道一个临界点，另一端的内存空间就可以被重新分配了 该算法的优点：不像复制算法那样会浪费内存空间，也不会产生内存碎片。 写到这时，我很想知道『标记-整理』的效率如何。一番搜索后，发现该算法是这三种算法中效率最低的，因为『整理』这个过程会遍历整个堆三次，具体实现思路如下 『标记-整理』以 lisp2 算法为例，实现思路如下 标记阶段：首先从 GC Roots 出发，标记出所有存活的对象 设置 forwarding 指针：每个对象头中都有一个forwarding指针，指向该对象整理后的位置。遍历整个堆计算存活对象的 forwarding 更新指针：遍历整个堆，根据 forwarding 更新 GC Roots 指针以及所有存活对象的子对象的指针，将指针指向新的位置 移动对象：遍历整个堆，根据 forwarding 移动对象并且清除对象中的标记 聊聊我个人的理解，为什么2,3,4一定要遍历整个堆？都是针对存活对象的操作，直接遍历 GC Roots 不行吗？ 设置 forwarding 阶段：一定要顺序遍历整个堆。如上图所示，如果仅是遍历 GC Roots 的话，你没法知道A这个区域是否可以覆盖 更新指针阶段：我觉得这个阶段仅遍历 GC Roots 的话，确实可行 移动对象阶段：由于 GC Roots 指针已经全部指向新的位置了，只能遍历整个堆 写到这，我又想为什么一定要三个步骤搞这么复杂，直接移动对象不行吗？ 根据上面的理解，一定要遍历整个堆来确定存活的对象该移动到哪。就现有的结构来看，如果直接移动，子对象的指针还好说可以处理，GC Roots 中的指针就没法更新了 关于lisp2 实现的更多细节可以参考下这篇Blog 『gc-标记整理算法的两种实现』 小结总结下上述三种基础GC算法的优缺点 维度 标记-清除 标记-整理 复制算法 速度 中等 最慢 最快 时间开销 mark阶段与存活对象的数量成正比，sweep阶段与整堆大小成正比 mark阶段与存活对象的数量成正比，compact阶段与整堆大小成正比，与存活对象的大小成正比 与存活对象大小成正比 空间开销 少（但会堆积碎片） 少（不堆积碎片） 通常需要存活对象的2倍大小（不堆积碎片） 移动对象 否 是 是 分代回收理论垃圾回收器都不会只选择一种算法，JVM根据对象存活周期的不同，将内存划分为几块。一般是把堆分为新生代和老年代，根据年代的特点来选择最佳的收集算法。 HotSpot 中大部分垃圾回收器都采用分代回收的思想 新生代：复制算法 老年代：标记-清除 / 标记-整理 / 或者两者同时使用 堆大小=新生代+老年代（默认分别占堆空间为1/3、2/3），新生代又被分为Eden、from survivor (S0)、to survivor (S1)，默认分配比例为 8:1:1 对象的分配对象的分配通常在 Eden 中（需要大量连续内存空间的 Java 对象，如很长的字符串或数据可以直接进入老年代，由 -XX:PretenureSizeThreshold 决定） 新生代的回收当 Eden 区满后，会触发 Young GC（新生代回收），复制 Eden 区和 S0 区中存活的对象到 S1 或者老年代（其中到达年龄的会被放入老年代，未到达年龄的放入 S1 区） 每经历一次 Young GC，survivor 区中对象年龄 +1 然后清空 Eden 区和 S0 区，交换 S0 与 S1 的名字 若存活对象大于 S1 区容量，则会被直接放入老年代。若打开了自适应（-XX:+AdaptiveSizePolicy），GC会自动重新调整新生代大小 老年代的回收在发生 Full GC 或者 Old GC 时，会根据不同的垃圾回收器或者情况选择使用 标记-清除 / 标记-整理 来进行回收 除了 Young GC 之外，常见的还有 Full GC（新生代、老生代、元空间或永久代的回收） Old GC（只有 CMS 有这个模式） Mixed GC（只有 G1 有这个模式） 通常情况下 Full GC 的触发条件，当准备要触发一次 Young GC时，如果发现统计数据说之前 Young GC 的平均晋升大小比目前老年代剩余的空间大，则不会触发 Young GC 而是转为触发 Full GC 小结由于新生代的特点是大多数对象都是「朝生夕死」的，存活率低，所以非常适合复制算法。而 survivor 区存在的意义是为了确保「朝生夕死」的对象不会轻易进入老年代，当对象的年龄满足（经历了多次 Young GC）才会进入老年代。 又到了提问环节，为什么分代回收中需要有两个 survivor 区，一个不行吗？ 答案是不行。假设只有一个 survivor，Eden 回收后存活的对象进入了 survivor。那么 survivor 区可以被回收的对象该怎么处理？难道要用标记清除和标记整理？那可太没有必要了，所以划分出两个 survivor 区，将新生代的复制算法贯彻到底 参考『深入理解Java虚拟机』 『极客时间 - Java性能调优实战』 『Java-GC 垃圾收集算法』 『gc-标记整理算法的两种实现』 『Mark-compact algorithm - Wikipedia』 『为什么新生代内存需要有两个Survivor区』 『Major GC和Full GC的区别是什么？触发条件呢？ - RednaxelaFX的回答 - 知乎』 『关于JVM垃圾搜集算法（标记-整理算法与复制算法）的效率？ - RednaxelaFX的回答 - 知乎』]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle LogMiner 数据迁移实战]]></title>
    <url>%2Fpost%2F82cb4508.html</url>
    <content type="text"><![CDATA[LogMiner 是什么LogMiner 是Oracle官方提供的工具，可以解析 Redo log 和 Archived Redo log LogMiner 可以做什么？官方文档中列举了很多，大家可以自己去看下。 我们目前的项目在使用基于LogMiner 的 Debezium Oracle Connector 做数据迁移 Oracle LogMiner 数据迁移的原理是什么？首先需要了解几个概念，这里简单介绍下 Redo log：Redo中记录了所有对数据块的更改，Oralce 要求至少有两个以上的Redo Log Group Archived Redo log：当一个Redo Log 写满之后，会发生日志切换，数据的更改会记录到下一个Redo Log中（所以一定要有两个以上的Redo）。如果开启了归档模式，Oracle 会将写满的Redo Log 归档。 SCN (System Change Number)：Oracle 内部逻辑时间戳 Flashback：通过闪回查询 SELECT ... AS OF SCN 可以查询Oracle某个时间点的全量数据 思路如下： 首先查询出一下当前的SCN 根据SCN 查询出这一时刻的全量数据 通过Logminer 指定Start_SCN，获取增量数据 安装与配置想尝试却不太熟悉Oracle的同学，可以参考一下我整理的文档 Oralce Install (docker)： https://github.com/TavenYin/database-cdc/blob/master/doc/oracle/oracle-install.md Logminer：https://github.com/TavenYin/database-cdc/blob/master/doc/oracle/oracle12c-logminer.md 小试牛刀在准备好了环境之后，我们来开箱体验一下Logminer logminer 用户登录 conn c##logminer/password 1. 构建数据字典LogMiner使用数据字典将内部对象标识符和数据类型转换为正常字段和数据格式 12345# 这是一条常规的SQL INSERT INTO HR.JOBS(JOB_ID, JOB_TITLE, MIN_SALARY, MAX_SALARY) VALUES(&apos;IT_WT&apos;,&apos;Technical Writer&apos;, 4000, 11000);# 如果没有数据字典，数据会是这样的insert into &quot;UNKNOWN&quot;.&quot;OBJ# 45522&quot;(&quot;COL 1&quot;,&quot;COL 2&quot;,&quot;COL 3&quot;,&quot;COL 4&quot;) values (HEXTORAW(&apos;45465f4748&apos;),HEXTORAW(&apos;546563686e6963616c20577269746572&apos;),HEXTORAW(&apos;c229&apos;),HEXTORAW(&apos;c3020b&apos;)); 官方文档中提到三种方式： 在线数据字典：当你可以访问创建Redo的源数据库并且表结构不会发生任何变动时。可以考虑使用在线数据字典。这是最简单有效的，也是Oracle的推荐选项由于在线数据字典永远存储的是最新的结构。如果发生了表结构变动，Logminer 捕获到旧版本的数据，SQL将会如上述代码块中那样 提取数据字典到redo中：需要执行命令BEGIN DBMS_LOGMNR_D.BUILD (options =&gt; DBMS_LOGMNR_D.STORE_IN_REDO_LOGS); END;该操作会占用一定数据库资源 提取数据字典到Flat File：Oracle维护该选项是为了兼容历史版本，本文并没有使用到该方式，不多做介绍 LogMiner 在启动时会通过指定的数据字典选项维护一个内部数据字典，当启动LogMiner时指定 DBMS_LOGMNR.DDL_DICT_TRACKING，LogMiner会自动捕获DDL来更新内部字典，这样即使发生了表结构变动时，也可以正确的解析DDL。注意：该选项不能和在线数据字典同时使用更多解释参考Oracle文档：https://docs.oracle.com/en/database/oracle/oracle-database/12.2/sutil/oracle-logminer-utility.html#GUID-56743517-A0C0-4CCD-9D20-2883AFB5683B 这一步我选择在线数据字典，什么都不用做，直接进入下一步 2. 添加日志文件12345678910111213141516171819# 查询目前的redol ogSQL&gt; select member from v$logfile;MEMBER--------------------------------------------------------------------------------/opt/oracle/oradata/ORCLCDB/redo03.log/opt/oracle/oradata/ORCLCDB/redo02.log/opt/oracle/oradata/ORCLCDB/redo01.log# 添加redo logSQL&gt; EXECUTE DBMS_LOGMNR.ADD_LOGFILE( - LOGFILENAME =&gt; &apos;/opt/oracle/oradata/ORCLCDB/redo03.log&apos;, - OPTIONS =&gt; DBMS_LOGMNR.NEW);EXECUTE DBMS_LOGMNR.ADD_LOGFILE( - LOGFILENAME =&gt; &apos;/opt/oracle/oradata/ORCLCDB/redo02.log&apos;, - OPTIONS =&gt; DBMS_LOGMNR.ADDFILE);EXECUTE DBMS_LOGMNR.ADD_LOGFILE( - LOGFILENAME =&gt; &apos;/opt/oracle/oradata/ORCLCDB/redo01.log&apos;, - OPTIONS =&gt; DBMS_LOGMNR.ADDFILE); 3. START_LOGMNR123# 使用在线数据字典进行log解析SQL&gt; EXECUTE DBMS_LOGMNR.START_LOGMNR(-OPTIONS =&gt; DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG); 然后执行一条INSERT 4. 查询结果通过查询V$LOGMNR_CONTENTS 获取LogMiner捕获的结果。当执行该视图查询时，LogMiner会按照顺序解析Redo和Archived Log，所有执行时间会有一点慢123SELECT OPERATION, SQL_REDO, SQL_UNDOFROM V$LOGMNR_CONTENTSWHERE table_name='TEST_TAB'; 结果如下，可以看到我们刚刚INSERT的SQL 实战我们已经知道了迁移的思路和Logminer如何使用，现在可以动手搞一个demo了。 由于篇幅问题，这里我只讨论思路和我的一些想法。 完整代码参考👉 https://github.com/TavenYin/database-cdc/tree/master/oracle-logminer 1. 整体思路相关实现思路参考自Debezium 需要解释一下第四步为什么，发生Redo发生切换时，需要重启Logminer流程，两点原因 Redo Log 切换后，会生成新的归档，我们需要Add新的归档日志 长时间开启LogMiner会话，会导致PGA使用量一直上升无法释放，End LogMiner 可以解决这个问题。所以代码逻辑中需要找一个时机去重启LogMiner，而Redo 切换这个时间点确实也挺合适的。 写到这的时候，我突然有了一个疑问 我们刚刚已经说过了，只有在查询 V$LOGMNR_CONTENTS 时，LogMiner才会去解析Redo Log，然后动态的生成视图。 参考上图。如果在第四步和第六步之间，程序检查到没有RedoLog切换准备继续执行。突然插入了大量数据导致Current Redo Log 被覆盖（注意必须是已经被覆盖而不是切换）了，此时是不是我们再查询 V$LOGMNR_CONTENTS 岂不是会丢失一部分数据？ 由于start_logminer时会指定，起始和结束SCN，所以即使下次执行时添加了新的Archived Log，由于SCN已经被跨过去了，所以一定不会读这部分数据 在我做了测试之后发现，如果情况真的如此极端，确实会这样。 那么Debezium为什么没有考虑这个问题呢？ 个人理解，在生产环境通常Redo Log 不会频繁切换，并且一定会有多个Redo Group。这么短时间内被覆盖的情况几乎不可能发生。 2. 处理 V$LOGMNR_CONTENTS 结果集最开始在看Debezium源码的时候，没仔细注意这个地方，在自己动手搞一遍之后，发现这个地方的逻辑有点麻烦 V$LOGMNR_CONTENTS 每一行可能是事务的提交、回滚，DDL，DML 上面提到了一个 TransactionalBuffer是什么？ 我们在读取 V$LOGMNR_CONTENTS 会发生如下图的情况，因为每次只从startScn 读取到 当前Scn。而这中间可能发生的情况是，事务并没有Commit，但是我们拿到了其中一部分的DML，我们并不能确定这些DML是不是要Commit，所以需要将这些“一半”的事务暂时缓存在内存中 其实在调用 DBMS_LOGMNR.START_LOGMNR 时，可以指定一个选项 COMMITTED_DATA_ONLY，仅读出已提交的事务。这样就不必要这么麻烦的处理结果集了。但是为什么不选择 COMMITTED_DATA_ONLY？使用该策略会一直等待事务提交才会响应客户端，这很容易造成 “Out of Memory”，所以这个策略不适合我们的程序。 3. 迁移进程宕机处理数据迁移必定是一个漫长的过程，如果在执行中遇到什么意外，导致Java进程挂了，那么一切都要从头开始吗？ 如果我们能确定某个SCN之前的所有记录都已经被处理了，那么下次重启时从这个SCN开始处理即可 两处可以确定之前SCN已经被全部处理的地方，代码如下： a. 当前TransactionalBuffer中没有数据，代表END_SCN之前所有的事务都已经被提交了 b. 提交事务时，如果当前要提交的事务的Start_SCN 早于TransactionalBuffer中的所有事务 4. SQL解析如果你想将Oracle的数据同步到其他数据库（包含NoSQL）的话，最好的办法是将SQL解析成结构化的对象，让下游服务去消费这些对象。 Debezium的做法，我还没抽出空研究。目前的解决方法是用com.alibaba.druid.sql.SQLUtils，这个类可以将SQL解析成结构化对象，我们再对这些对象进行一些处理，即可让下游服务消费了。 DEMO运行效果如下 GitHub 👉 https://github.com/TavenYin/database-cdc/tree/master/oracle-logminer 参考 Oracle Redo : https://docs.oracle.com/cd/B28359_01/server.111/b28310/onlineredo001.htm Oracle Archived : https://docs.oracle.com/cd/B28359_01/server.111/b28310/archredo001.htm Oracle Flashback : https://docs.oracle.com/cd/E11882_01/appdev.112/e41502/adfns_flashback.htm LogMiner : https://docs.oracle.com/en/database/oracle/oracle-database/12.2/sutil/oracle-logminer-utility.html Debezium Oracle Connector : https://debezium.io/documentation/reference/connectors/oracle.html]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
        <tag>LogMiner</tag>
        <tag>数据库迁移技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka Connect-入门]]></title>
    <url>%2Fpost%2F7ec53704.html</url>
    <content type="text"><![CDATA[前提首先你需要了解MQ / Kafka相关的知识 本文目标了解 Kafka Connect 基本概念与功能 什么是Kafka Connect Kafka Connect 是一款可扩展并且可靠地在 Apache Kafka 和其他系统之间进行数据传输的工具。 可以很简单的定义 connectors（连接器） 将大量数据迁入、迁出Kafka。 例如我现在想要把数据从MySQL迁移到ElasticSearch，为了保证高效和数据不会丢失，我们选择MQ作为中间件保存数据。这时候我们需要一个生产者线程，不断的从MySQL中读取数据并发送到MQ，还需要一个消费者线程消费MQ的数据写到ElasticSearch，这件事情似乎很简单，不需要任何框架。 但是如果我们想要保证生产者和消费者服务的高可用性，例如重启后生产者恢复到之前读取的位置，分布式部署并且节点宕机后将任务转移到其他节点。如果要加上这些的话，这件事就变得复杂起来了，而Kafka Connect 已经为我们造好这些轮子。 Kafka Connect 如何工作？ Kafka Connect 特性如下： Kafka 连接器的通用框架：Kafka Connect 标准化了其他数据系统与Kafka的集成，从而简化了连接器的开发，部署和管理 支持分布式模式和单机模式部署 Rest API：通过简单的Rest API管理连接器 偏移量管理：针对Source和Sink都有相应的偏移量（Offset）管理方案，程序员无须关心Offset 的提交 分布式模式可扩展的，支持故障转移 Kafka Connect Concepts这里简单介绍下Kafka Connect 的概念与组成更多细节请参考 👉 https://docs.confluent.io/platform/current/connect/concepts.html Connectors连接器，分为两种 Source（从源数据库拉取数据写入Kafka），Sink（从Kafka消费数据写入目标数据） 连接器其实并不参与实际的数据copy，连接器负责管理Task。连接器中定义了对应Task的类型，对外提供配置选项（用户创建连接器时需要提供对应的配置信息）。并且连接器还可以决定启动多少个Task线程。 用户可以通过Rest API 启停连接器，查看连接器状态 Confluent 已经提供了许多成熟的连接器，传送门👉 https://www.confluent.io/product/connectors/ Task实际进行数据传输的单元，和连接器一样同样分为 Source和Sink Task的配置和状态存储在Kafka的Topic中，config.storage.topic和status.storage.topic。我们可以随时启动，停止任务，以提供弹性、可扩展的数据管道 Worker刚刚我们讲的Connectors 和Task 属于逻辑单元，而Worker 是实际运行逻辑单元的进程，Worker 分为两种模式，单机模式和分布式模式 单机模式：比较简单，但是功能也受限，只有一些特殊的场景会使用到，例如收集主机的日志，通常来说更多的是使用分布式模式 分布式模式：为Kafka Connect提供了可扩展和故障转移。相同group.id的Worker，会自动组成集群。当新增Worker，或者有Worker挂掉时，集群会自动协调分配所有的Connector 和 Task（这个过程称为Rebalance） 当使用Worker集群时，创建连接器，或者连接器Task数量变动时，都会触发Rebalance 以保证集群各个Worker节点负载均衡。但是当Task 进入Fail状态的时候并不会触发 Rebalance，只能通过Rest Api 对Task进行重启 ConvertersKafka Connect 通过 Converter 将数据在Kafka（字节数组）与Task（Object）之间进行转换 默认支持以下Converter AvroConverter io.confluent.connect.avro.AvroConverter: 需要使用 Schema Registry ProtobufConverter io.confluent.connect.protobuf.ProtobufConverter: 需要使用 Schema Registry JsonSchemaConverter io.confluent.connect.json.JsonSchemaConverter: 需要使用 Schema Registry JsonConverter org.apache.kafka.connect.json.JsonConverter (无需 Schema Registry): 转换为json结构 StringConverter org.apache.kafka.connect.storage.StringConverter: 简单的字符串格式 ByteArrayConverter org.apache.kafka.connect.converters.ByteArrayConverter: 不做任何转换 Converters 与 Connector 是解耦的，下图展示了在Kafka Connect中，Converter 在何时进行数据转换 Transforms连接器可以通过配置Transform 实现对单个消息（对应代码中的Record）的转换和修改，可以配置多个Transform 组成一个链。例如让所有消息的topic加一个前缀、sink无法消费source 写入的数据格式，这些场景都可以使用Transform 解决 Transform 如果配置在Source 则在Task之后执行，如果配置在Sink 则在Task之前执行 Dead Letter Queue与其他MQ不同，Kafka 并没有死信队列这个功能。但是Kafka Connect提供了这一功能。 当Sink Task遇到无法处理的消息，会根据errors.tolerance配置项决定如何处理，默认情况下(errors.tolerance=none) Sink 遇到无法处理的记录会直接抛出异常，Task进入Fail 状态。开发人员需要根据Worker的错误日志解决问题，然后重启Task，才能继续消费数据 设置 errors.tolerance=all，Sink Task 会忽略所有的错误，继续处理。Worker中不会有任何错误日志。可以通过配置errors.deadletterqueue.topic.name = &lt;dead-letter-topic-name&gt; 让无法处理的消息路由到 Dead Letter Topic 快速上手下面我来实战一下，如何使用Kafka Connect，我们先定一个小目标 将MySQL中的全量数据同步到Redis 新建文件 docker-compose.yaml123456789101112131415161718192021version: &apos;3.7&apos;services: zookeeper: image: wurstmeister/zookeeper container_name: zk ports: - 2182:2181 kafka: image: wurstmeister/kafka:2.13-2.7.0 container_name: kafka ports: - 9092:9092 environment: KAFKA_BROKER_ID: 0 # 宿主机ip KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://192.168.3.21:9092 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092 depends_on: - zookeeper 在终端上执行 docker-compose -f docker-compose.yaml up -d 启动docker容器 准备连接器，这里我是自己写了一个简单的连接器😄。下载地址：https://github.com/TavenYin/kafka-connect-example/blob/master/release/kafka-connector-example-bin.jar 12# 将连接器上传到kafka 容器中docker cp kafka-connector-example-bin.jar kafka:/opt/connectors 修改配置并启动Worker 12345#在配置文件末尾追加 plugin.path=/opt/connectorsvi /opt/kafka/config/connect-distributed.properties# 启动Workerbin/connect-distributed.sh -daemon config/connect-distributed.properties 准备MySQL 由于我宿主机里已经安装了MySQL，我就直接使用了，使用如下Sql创建表。创建之后随便造几条数据12345CREATE TABLE `test_user` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`)) ; 创建连接器 新建 source.json1234567891011&#123; &quot;name&quot; : &quot;example-source&quot;, &quot;config&quot; : &#123; &quot;connector.class&quot; : &quot;com.github.taven.source.ExampleSourceConnector&quot;, &quot;tasks.max&quot; : &quot;1&quot;, &quot;database.url&quot; : &quot;jdbc:mysql://192.168.3.21:3306/test?useUnicode=true&amp;characterEncoding=UTF-8&amp;autoReconnect=true&amp;useSSL=false&amp;zeroDateTimeBehavior=convertToNull&amp;serverTimezone=UTC&amp;rewriteBatchedStatements=true&quot;, &quot;database.username&quot; : &quot;root&quot;, &quot;database.password&quot; : &quot;root&quot;, &quot;database.tables&quot; : &quot;test_user&quot; &#125;&#125; 向Worker 发送请求，创建连接器curl -i -X POST -H &quot;Accept:application/json&quot; -H &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @source.json source.json 中，有一些属性是Kafka Connect 提供的，例如上述文件中 name, connector.class, tasks.max，剩下的属性可以在开发Connector 时自定义。关于Kafka Connect Configuration 相关请阅读这里 👉 https://docs.confluent.io/platform/current/installation/configuration/connect/index.html 确认数据是否写入Kafka 首先查看一下Worker中的运行状态，如果Task的state = RUNNING，代表Task没有抛出任何异常，平稳运行123bash-4.4# curl -X GET localhost:8083/connectors/example-source/status&#123;&quot;name&quot;:&quot;example-source&quot;,&quot;connector&quot;:&#123;&quot;state&quot;:&quot;RUNNING&quot;,&quot;worker_id&quot;:&quot;172.21.0.3:8083&quot;&#125;,&quot;tasks&quot;:[&#123;&quot;id&quot;:0,&quot;state&quot;:&quot;RUNNING&quot;,&quot;worker_id&quot;:&quot;172.21.0.3:8083&quot;&#125;],&quot;type&quot;:&quot;source&quot;&#125; 查看kafka 中Topic 是否创建123456bash-4.4# bin/kafka-topics.sh --list --zookeeper zookeeper:2181__consumer_offsetsconnect-configsconnect-offsetsconnect-statustest_user 这些Topic 都存储了什么？ __consumer_offsets: 记录所有Kafka Consumer Group的Offset connect-configs: 存储连接器的配置，对应Connect 配置文件中config.storage.topic connect-offsets: 存储Source 的Offset，对应Connect 配置文件中offset.storage.topic connect-status: 连接器与Task的状态，对应Connect 配置文件中status.storage.topic 查看topic中数据，此时说明MySQL数据已经成功写入Kafka1234bash-4.4# bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test_user --from-beginning&#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;type&quot;:&quot;int64&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;name&quot;&#125;],&quot;optional&quot;:false,&quot;name&quot;:&quot;test_user&quot;&#125;,&quot;payload&quot;:&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;yyyyyy&quot;&#125;&#125;&#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;type&quot;:&quot;int64&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;name&quot;&#125;],&quot;optional&quot;:false,&quot;name&quot;:&quot;test_user&quot;&#125;,&quot;payload&quot;:&#123;&quot;id&quot;:2,&quot;name&quot;:&quot;qqqq&quot;&#125;&#125;&#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;type&quot;:&quot;int64&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;name&quot;&#125;],&quot;optional&quot;:false,&quot;name&quot;:&quot;test_user&quot;&#125;,&quot;payload&quot;:&#123;&quot;id&quot;:3,&quot;name&quot;:&quot;eeee&quot;&#125;&#125; 数据结构为Json，可以回顾一下上面我们修改的connect-distributed.properties，默认提供的Converter 为JsonConverter，所有的数据包含schema 和 payload 两项是因为配置文件中默认启动了key.converter.schemas.enable=true和value.converter.schemas.enable=true两个选项 启动 Sink 新建sink.json123456789101112&#123; &quot;name&quot; : &quot;example-sink&quot;, &quot;config&quot; : &#123; &quot;connector.class&quot; : &quot;com.github.taven.sink.ExampleSinkConnector&quot;, &quot;topics&quot; : &quot;test_user, test_order&quot;, &quot;tasks.max&quot; : &quot;1&quot;, &quot;redis.host&quot; : &quot;192.168.3.21&quot;, &quot;redis.port&quot; : &quot;6379&quot;, &quot;redis.password&quot; : &quot;&quot;, &quot;redis.database&quot; : &quot;0&quot; &#125;&#125; 创建Sink Connectorcurl -i -X POST -H &quot;Accept:application/json&quot; -H &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @sink.json 然后查看Sink Connector Status，这里我发现由于我的Redis端口只对localhost开发，所以这里我的Task Fail了，修改了Redis配置之后，重启Task curl -X POST localhost:8083/connectors/example-sink/tasks/0/restart 在确认了Sink Status 为RUNNING 后，可以确认下Redis中是否有数据 关于Kafka Connect Rest api 文档，请参考👉https://docs.confluent.io/platform/current/connect/references/restapi.html 如何查看Sink Offset消费情况 使用命令bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group connect-example-sink 下图代表 test_user topic 三条数据已经全部消费 Kafka Connect 高级功能我们的小目标已经达成了。现在两个Task无事可做，正好借此机会我们来体验一下可扩展和故障转移 集群扩展我启动了开发环境中的Kafka Connect Worker，根据官方文档所示通过注册同一个Kafka 并且使用相同的 group.id=connect-cluster 可以自动组成集群 启动我开发环境中的Kafka Connect，之后检查两个连接器状态 12345bash-4.4# curl -X GET localhost:8083/connectors/example-source/status&#123;&quot;name&quot;:&quot;example-source&quot;,&quot;connector&quot;:&#123;&quot;state&quot;:&quot;RUNNING&quot;,&quot;worker_id&quot;:&quot;172.23.176.1:8083&quot;&#125;,&quot;tasks&quot;:[&#123;&quot;id&quot;:0,&quot;state&quot;:&quot;RUNNING&quot;,&quot;worker_id&quot;:&quot;172.23.176.1:8083&quot;&#125;],&quot;type&quot;:&quot;source&quot;&#125;bash-4.4#bash-4.4# curl -X GET localhost:8083/connectors/example-sink/status&#123;&quot;name&quot;:&quot;example-sink&quot;,&quot;connector&quot;:&#123;&quot;state&quot;:&quot;RUNNING&quot;,&quot;worker_id&quot;:&quot;172.21.0.3:8083&quot;&#125;,&quot;tasks&quot;:[&#123;&quot;id&quot;:0,&quot;state&quot;:&quot;RUNNING&quot;,&quot;worker_id&quot;:&quot;172.21.0.3:8083&quot;&#125;],&quot;type&quot;:&quot;sink&quot;&#125; 观察worker_id 可以发现，两个Connectors 已经分别运行在两个Worker上了 故障转移此时我们通过kill pid结束docker中的Worker进程观察是否宕机之后自动转移，但是发现Task并没有转移到仅存的Worker中，Task 状态变为UNASSIGNED，这是为啥呢？难道是有什么操作错了？ 在网上查阅了一番得知，Kafka Connect 的集群扩展与故障转移机制是通过Kafka Rebalance 协议实现的（Consumer也是该协议），当Worker节点宕机时间超过 scheduled.rebalance.max.delay.ms 时，Kafka才会将其踢出集群。踢出后将该节点的连接器和任务分配给其他Worker，scheduled.rebalance.max.delay.ms默认值为五分钟。 后来经测试发现，五分钟之后查看连接器信息，已经转移到存活的Worker节点了 本来还计划写一下如何开发连接器和Kafka Rebalance，但是这篇已经够长了，所以计划后续更新这两篇文章]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>Kafka Connect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[问题分析：Kafka Connect 引入了Fastjson后，Rest API响应为{}]]></title>
    <url>%2Fpost%2F11f7961b.html</url>
    <content type="text"><![CDATA[前言最近在学习Kafka Connect，写了个连接器的demo。在demo提交了几个版本之后，突然发现Kafka Connect Rest API 无法正常响应了，明明有正在运行的连接器，查询status，居然返回{} 问题分析对 Rest API 进行debug后，确认是有数据的，但是数据返回不到客户端，很奇怪。因为我记得之前是好用的，所以我回滚了代码版本，逐一排查之后发现当引入Fastjson 依赖之后，会导致Connect Rest API 不可用 如果懒一点的话，到这里就已经结束了，直接删除Fastjson依赖，使用其他Json包。但是我很好奇，在我的理解里，Fastjson 这种库就是个工具包，如果我们程序没有主动调用的时候，是不会对我们产生任何影响的。 百度谷歌一通之后，一筹莫展之际，点开了Fastjson的源码包，在这里发现了Fastjson为JAXRS提供的SPI扩展 JAXRS：Java API for RESTful Web Services，JavaEE提供的Web服务接口。Jersey 实现了JAXRS，而Kafka Connect 引用了Jersey 。SPI：Service Provider Interface ，是JDK内置的一种服务提供发现机制，可以参考我之前的博客 Java SPI 实战 打开javax.ws.rs.ext.MessageBodyWriter 文件，可以看到提供的实现类是com.alibaba.fastjson.support.jaxrs.FastJsonProvider，定位到FastJsonProvider下writeTo方法，该方法会把object写入到OutputStream中，看起来很靠谱，debug试一下 果然，说明Fastjson果然参与了Rest API的响应。为什么使用Fastjson就响应不了数据呢，看了下源码，这里要求被序列化的Bean必须标记Fastjson相关的注解，而实际的Bean使用的是Jackson的注解，所以Fastjson无法序列化数据。 接下来可以根据调用栈和全局搜索找一下，看看FastJsonProvider是在什么时机加载的，能否干掉他。 调用栈并没有找到什么有用的信息，通过全局搜索MessageBodyWriter找到了FastJsonProvider的加载位置，MessageBodyFactory::initialize 上图字面意思理解，使用 injectionManager (注入管理器)，找到MessageBodyWriter的可用实现 这里的 customMbws size = 2，分别是FastJson和Jackson的实现。但是FastJson在前，而每次需要做JSON序列化的时候，会遍历writers，如果找到支持application/json的MessageBodyWriter则直接返回，所以每次使用的都是FastJson的实现。 至此已经明白了，为什么Fastjson 会影响Kafka Connect了，接下来就是想办法解决了 这个时候还是没有找到Fastjson是在哪加载的，在Fastjson的 wiki 中找到了些灵感，发现Fastjson 在Jersey 中并不是通过SPI的方式进行的扩展，而是通过FastJsonAutoDiscoverable，向Jersey 的 context中注册FastJsonProvider 最后，我们在java 进程启动时指定参数 -Dfastjson.auto.discoverable=false，禁用 FastJsonProvider 参考https://github.com/alibaba/fastjson/wiki/Integrate-Fastjson-in-JAXRS]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式事务 Seata AT模式原理与实战]]></title>
    <url>%2Fpost%2F6fe3d525.html</url>
    <content type="text"><![CDATA[Seata 是阿里开源的基于Java的分布式事务解决方案 AT，XA，TCC，SagaSeata 提供四种模式解决分布式事务场景，AT，XA，TCC，Saga。简单叨咕叨咕我对这几种模式的理解 AT 这是Seata的一大特色，AT对业务代码完全无侵入性，使用非常简单，改造成本低。我们只需要关注自己的业务SQL，Seata会通过分析我们业务SQL，反向生成回滚数据 AT 包含两个阶段 一阶段，所有参与事务的分支，本地事务Commit 业务数据和回滚日志（undoLog） 二阶段，事务协调者根据所有分支的情况，决定本次全局事务是Commit 还是 Rollback（二阶段是完全异步） XA也是我们常说的二阶段提交，XA要求数据库本身提供对规范和协议的支持。XA用起来的话，也是对业务代码无侵入性的。 上述其他三种模式，都是属于补偿型，无法保证全局一致性。啥意思呢，例如刚刚说的AT模式，我们是可能读到这一次分布式事务的中间状态，而XA模式不会。 补偿型 事务处理机制构建在 事务资源（数据库） 之上（要么在中间件层面，要么在应用层面），事务资源 本身对分布式事务是无感知的，这也就导致了补偿型事务无法做到真正的 全局一致性 。比如，一条库存记录，处在 补偿型 事务处理过程中，由 100 扣减为 50。此时，仓库管理员连接数据库，查询统计库存，就看到当前的 50。之后，事务因为意外回滚，库存会被补偿回滚为 100。显然，仓库管理员查询统计到的 50 就是 脏 数据。如果是XA的话，中间态数据库存 50 由数据库本身保证，不会被仓库管理员读到（当然隔离级别需要 读已提交 以上） 但是全局一致性带来的结果就是数据的锁定（AT模式也是存在全局锁的，但是隔离级别无法保证，后边我们会详细说），例如全局事务中有一条update语句，其他事务想要更新同一条数据的话，只能等待全局事务结束 传统XA模式是存在一些问题的，Seata也是做了相关的优化，更多关于Seata XA的内容，传送门👉http://seata.io/zh-cn/blog/seata-xa-introduce.html TCC TCC 模式同样包含两个阶段 Try 阶段 ：所有参与分布式事务的分支，对业务资源进行检查和预留 二阶段 Confirm：所有分支的Try全部成功后，执行业务提交 二阶段 Cancel：取消Try阶段预留的业务资源 对比AT或者XA模式来说，TCC模式需要我们自己抽象并实现Try，Confirm，Cancel三个接口，编码量会大一些，但是由于事务的每一个阶段都由开发人员自行实现。而且相较于AT模式来说，减少了SQL解析的过程，也没有全局锁的限制，所以TCC模式的性能是优于AT 、XA模式。PS：果然简单和高效难以两全的 Saga Saga 是长事务解决方案，每个参与者需要实现事务的正向操作和补偿操作。当参与者正向操作执行失败时，回滚本地事务的同时，会调用上一阶段的补偿操作，在业务失败时最终会使事务回到初始状态 Saga与TCC类似，同样没有全局锁。由于相比缺少锁定资源这一步，在某些适合的场景，Saga要比TCC实现起来更简单。由于Saga和TCC都需要我们手动编码实现，所以在开发时我们需要参考一些设计上的规范，由于不是本文重点，这里就不多说了，可以参考分布式事务 Seata 及其三种模式详解 在我们了解完四种分布式事务的原理之后，我们回到本文重点AT模式 AT 如何使用模拟需求：以下订单为例，在分布式的电商场景中，订单服务和库存服务可能是两个数据库 我们先来看看AT模式下的代码是什么样的，这里忽略了Seata的相关配置，只看业务部分 在需要开启分布式事务的方法上标记@GlobalTransactional，然后执行分别执行扣减库存和创建订单操作，事务的参与者可以是本地的数据源，或者RPC的远程调用（远程调用的话需要携带全局事务ID，也就是上图的xid） AT 一阶段之前说过AT模式分为两个阶段，第一阶段包括提交业务数据和回滚日志（undoLog），第一阶段具体流程如下图 GlobalTransactional 切面标记@GlobalTransactional的方法通过AOP实现了，开启全局事务和提交全局事务两个操作，与Spring 事务机制类似，当 GlobalTransactionalInterceptor 在事务执行过程中捕获到Throwable时，会发起全局事务回滚 0.1 步骤中会生成一个全局事务ID 0.2 所有事务参与者执行结束后，一阶段事务提交 undoLog我们先来看看 Seata undoLog 的结构1234567891011// 省略了相关方法public class SQLUndoLog &#123; // insert, update ... private SQLType sqlType; private String tableName; private TableRecords beforeImage; private TableRecords afterImage;&#125; Seata 在执行业务SQL前后，会生成beforeImage和afterImage，在需要回滚时，根据SQLType，决定具体的回滚策略，例如SQLType=update时，将数据回滚到beforeImage的状态，如果SQLType=insert，则根据afterImage删除数据 如2.4所示，每条业务SQL，执行成功后，会为这条SQL生成LockKey，格式为tableName:PrimaryKey 注册分支事务在3.1步骤注册分支事务时，client会把所有的LockKey 拼到一起作为全局锁发送给Seata-server。如果注册成功，写入undoLog，并提交本地事务，一阶段结束，等待二阶段反馈 如果当前有其他分支事务已经持有了相同的锁（即其他事务也在处理相同表的同一行），则client 注册事务分支失败。client会根据客户端定义的重发时间和重发次数进行不断的尝试，如果重试结束仍然没有获得锁，则一阶段失败，本地事务回滚。如果该全局事务存在已经注册成功分支事务，Seata-server 进行二阶段回滚 全局锁会在分支事务二阶段结束后释放 Seata 全局锁的设计是为了什么？以扣减库存场景为例，TX1 完成库存扣减的一阶段，库存从100扣减为99，正在等待二阶段的通知。TX2也要扣减同一商品的库存，如果没有全局锁的限制，TX2库存从99扣减为98，这时如果TX1接收到回滚通知，进行回滚把库存从98回滚到100。因为没有全局锁，造成了脏写 AT 二阶段二阶段是完全异步化的并且完全由Seata控制，Seata根据所有事务参与者的提交情况决定二阶段如何处理 如果所有事务提交成功，则二阶段的任务就是删除一阶段生成 的undoLog，并释放全局锁 如果部分事务参与者提交失败，则需要根据undoLog对已经注册的事务分支进行回滚，并释放全局锁 对Seata提出的疑问至此我们已经初步了解了Seata的AT模式是如何实现的了 如果你也和我一样，仔细思考了上述过程，可能会提出一些问题，这边我列举一下我在学习Seata时，遇到的问题，以及我得出的结论 问题1. Seata如何做到无侵入的分析业务SQL生成undoLog，注册事务分支等操作？ Seata 代理了DataSource，我们可以通过在代码注入一个DataSource来验证我的说法，目前的DataSource 是 io.seata.rm.datasource.DataSourceProxy 所有的Java持久化框架，最终在操作数据库时都会通过DataSource接口获取Connection，通过Connection 实现对数据库的增删改查，事务控制。 Seata 通过代理Connection的方式，做到了无侵入的生成undoLog，注册事务分支，具体源码可以查看io.seata.rm.datasource.ConnectionProxy 问题2. ConnectionProxy 如何判断当前事务是全局事务，还是本地事务？ 通过当前线程是否绑定了全局事务id，在进行全局事务之前，需要调用RootContext.bind(xid); 问题3. 全局事务并发更新 还是以下订单扣减库存的场景为例，如果TX1和TX2同时扣减product_id为1的库存，这时Seata会不会生成相同的beforeImage？ 举个例子，TX1读库存为100，TX1扣减库存1，此时BeforeImage为100紧接着 如果TX2读库存也为100，那么就有问题了，不管TX2扣减多少库存，如果TX1回滚那么相当于覆盖了TX2扣减的库存，出现了脏写 Seata是如何解决这个问题的？ 源码位置：io.seata.rm.datasource.exec.AbstractDMLBaseExecutor::executeAutoCommitFalse 可以看到这里的逻辑和我上面画的图一致，证明我没有瞎说 😄 我们来看一下beforeImage()，这是一个抽象方法，看一下他的子类UpdateExecutor是如何实现的 通过Debug，可以看出Seata这边也是确实考虑了这个问题，直接简单而有效的解决了这个问题 回到我们的例子，由于SELECT FOR UPDATE的存在，TX2如果也想读同一条数据的话，只能等到TX1 提交事务后，才能读到。所以问题解决 问题4. 全局事务外的更新 我们现在可以确认在Seata的保证下，全局事务，不会造成数据的脏写，但是全局事务外会！ 什么意思呢？ 还以库存为例 用户正在抢购，用户A完成了1阶段的库存扣减，这个时候库存为99。 此时库存管理员上线了，他查了一下库存为99。嗯…太少了，我加100个，库存管理员把库存更新为200。 而此时seata给用户A生成beforeImage为100，如果此时用户A的全局事务失败了，发生了回滚，再次将库存更新为100… 再次出现脏写 Seata 针对这个问题，提供了@GlobalLock注解，标记该注解时，会像全局事务一样进行SQL分析，竞争全局锁，就不会出现上述问题了 关于这个问题可以参考Seata的FAQ文档 http://seata.io/zh-cn/docs/overview/faq.html 问题5. @GlobalTransactional 和 @Transactional 同时使用会怎么样 我们上文中已经说过了 @GlobalTransactional 的作用了，他是负责开启全局事务/提交事务1阶段，说白了@GlobalTransactional 只和Seata-server 交互，而 @Transactional 管理的是本地数据库的事务，所以二者不发生冲突。 但是需要注意 @GlobalTransactional AOP 覆盖范围一定要大于 @Transactional 问题6. 如果其中某一个事务分支超时未提交，会发生什么 这个我并没有看源码，而是通过跑demo，验证的 例如现在有A，B两个事务分支 A 正常提交，并向Seata注册分支成功 B 2分钟后提交事务，并向Seata发起注册 Seata的全局事务超时时间，默认是1分钟，Seata-server 在检测到有超时的全局事务时，会向所有已提交的分支，发起回滚。而超时提交的事务，向Seata-server发起分支注册时，响应结果为事务已超时，或者事务不存在，也会回滚本地事务 问题7. Seata-client 如何接收Seata-server发起的通知 Seata-client 包含了Netty服务，在启动时Netty会监听端口，并向Seata-server 发起注册。server中存储了client 的调用地址。 总结我们学习了Seata的AT模式是如何工作的，可以看出Seata模式在开发上是非常简单的，但是Seata的背后为了维持分布式事务的数据一致性，做了大量的工作，AT模式非常适合现有的业务模型直接迁移。 但是他的缺点也很明显，性能并不是那么的优秀。例如我们刚刚看到的全局锁的问题，为了数据不会发生脏写，Seata牺牲了业务的并发能力。在非常要求性能的场景，可能还是需要考虑TCC，SAGA，可靠消息等方案 在使用Seata开发前，建议大家先去阅读一下FAQ文档，避免踩坑 https://seata.io/zh-cn/docs/overview/faq.html DEMOhttps://github.com/TavenYin/taven-springboot-learning/tree/master/springboot-seata 参考Seata是什么 - http://seata.io/zh-cn/docs/overview/what-is-seata.htmlSeata常见问题 - http://seata.io/zh-cn/docs/overview/faq.html分布式事务中间件 Seata 的设计原理 - http://seata.io/zh-cn/blog/seata-at-mode-design.html分布式事务 Seata 及其三种模式详解 - http://seata.io/zh-cn/blog/seata-at-tcc-saga.html分布式事务如何实现？深入解读 Seata 的 XA 模式 - http://seata.io/zh-cn/blog/seata-xa-introduce.html]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>Seata</tag>
        <tag>分布式事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅入浅出 Spring 事务传播实现原理]]></title>
    <url>%2Fpost%2Fbef5c7fd.html</url>
    <content type="text"><![CDATA[本文和大家一起刨析 Spring 事务的相关源码，篇幅较长，代码片段较多，建议使用电脑阅读 本文目标 理解Spring事务管理核心接口 理解Spring事务管理的核心逻辑 理解事务的传播类型及其实现原理 版本SpringBoot 2.3.3.RELEASE 什么是事务的传播？Spring 除了封装了事务控制之外，还抽象出了 事务的传播 这个概念，事务的传播并不是关系型数据库所定义的，而是Spring在封装事务时做的增强扩展，可以通过@Transactional 指定事务的传播，具体类型如下 事务传播行为类型 说明 PROPAGATION_REQUIRED 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。Spring的默认事务传播类型 PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY 使用当前的事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起（暂停）。 PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 举个栗子以嵌套事务为例123456789101112131415161718192021222324252627282930313233343536@Servicepublic class DemoServiceImpl implements DemoService &#123; @Autowired private JdbcTemplate jdbcTemplate; @Autowired private DemoServiceImpl self; @Transactional @Override public void insertDB() &#123; String sql = "INSERT INTO sys_user(`id`, `username`) VALUES (?, ?)"; jdbcTemplate.update(sql, uuid(), "taven"); try &#123; // 内嵌事务将会回滚，而外部事务不会受到影响 self.nested(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; @Transactional(propagation = Propagation.NESTED) @Override public void nested() &#123; String sql = "INSERT INTO sys_user(`id`, `username`) VALUES (?, ?)"; jdbcTemplate.update(sql, uuid(), "nested"); throw new RuntimeException("rollback nested"); &#125; private String uuid() &#123; return UUID.randomUUID().toString(); &#125;&#125; 上述代码中，nested()方法标记了事务传播类型为嵌套，如果nested()中抛出异常仅会回滚nested()方法中的sql，不会影响到insertDB()方法中已经执行的sql 注意：service 调用内部方法时，如果直接使用this调用，事务不会生效。因此使用this调用相当于跳过了外部的代理类，所以AOP不会生效，无法使用事务 思考众所周知，Spring 事务是通过AOP实现的，如果是我们自己写一个AOP控制事务，该怎么做呢？1234567891011121314151617// 伪代码public Object invokeWithinTransaction() &#123; // 开启事务 connection.beginTransaction(); try &#123; // 反射执行方法 Object result = invoke(); // 提交事务 connection.commit(); return result; &#125; catch(Exception e) &#123; // 发生异常时回滚 connection.rollback(); throw e; &#125; &#125; 在这个基础上，我们来思考一下如果是我们自己做的话，事务的传播该如何实现 以PROPAGATION_REQUIRED为例，这个似乎很简单，我们判断一下当前是否有事务（可以考虑使用ThreadLocal存储已存在的事务对象），如果有事务，那么就不开启新的事务。反之，没有事务，我们就创建新的事务 如果事务是由当前切面开启的，则提交/回滚事务，反之不做处理 那么事务传播中描述的挂起（暂停）当前事务，和内嵌事务是如何实现的？ 源码入手要阅读事务传播相关的源码，我们先来了解下Spring 事务管理的核心接口与类 TransactionDefinition该接口定义了事务的所有属性（隔离级别，传播类型，超时时间等等），我们日常开发中经常使用的 @Transactional 其实最终会被转化为 TransactionDefinition TransactionStatus事务的状态，以最常用的实现 DefaultTransactionStatus 为例，该类存储了当前的事务对象，savepoint，当前挂起的事务，是否完成，是否仅回滚等等 TransactionManager这是一个空接口，直接继承他的 interface 有 PlatformTransactionManager（我们平时用的就是这个，默认的实现类DataSourceTransactionManager）以及ReactiveTransactionManager（响应式事务管理器，由于不是本文重点，我们不多说） 从上述两个接口来看，TransactionManager 的主要作用 通过TransactionDefinition开启一个事务，返回TransactionStatus 通过TransactionStatus 提交、回滚事务（实际开启事务的Connection通常存储在TransactionStatus中） 123456789101112public interface PlatformTransactionManager extends TransactionManager &#123; TransactionStatus getTransaction(@Nullable TransactionDefinition definition) throws TransactionException; void commit(TransactionStatus status) throws TransactionException; void rollback(TransactionStatus status) throws TransactionException;&#125; TransactionInterceptor事务拦截器，事务AOP的核心类（支持响应式事务，编程式事务，以及我们常用的标准事务），由于篇幅原因，本文只讨论标准事务的相关实现 下面我们从事务逻辑的入口 TransactionInterceptor 入手，来看下Spring事务管理的核心逻辑以及事务传播的实现 TransactionInterceptorTransactionInterceptor 实现了MethodInvocation（这是实现AOP的一种方式），其核心逻辑在父类TransactionAspectSupport 中，方法位置：TransactionInterceptor::invokeWithinTransaction 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950protected Object invokeWithinTransaction(Method method, @Nullable Class&lt;?&gt; targetClass, final InvocationCallback invocation) throws Throwable &#123; // If the transaction attribute is null, the method is non-transactional. TransactionAttributeSource tas = getTransactionAttributeSource(); // 当前事务的属性 TransactionAttribute extends TransactionDefinition final TransactionAttribute txAttr = (tas != null ? tas.getTransactionAttribute(method, targetClass) : null); // 事务属性中可以定义当前使用哪个事务管理器 // 如果没有定义就去Spring上下文找到一个可用的 TransactionManager final TransactionManager tm = determineTransactionManager(txAttr); // 省略了响应式事务的处理 ... PlatformTransactionManager ptm = asPlatformTransactionManager(tm); final String joinpointIdentification = methodIdentification(method, targetClass, txAttr); if (txAttr == null || !(ptm instanceof CallbackPreferringPlatformTransactionManager)) &#123; // Standard transaction demarcation with getTransaction and commit/rollback calls. TransactionInfo txInfo = createTransactionIfNecessary(ptm, txAttr, joinpointIdentification); Object retVal; try &#123; // This is an around advice: Invoke the next interceptor in the chain. // This will normally result in a target object being invoked. // 如果有下一个拦截器则执行，最终会执行到目标方法，也就是我们的业务代码 retVal = invocation.proceedWithInvocation(); &#125; catch (Throwable ex) &#123; // target invocation exception // 当捕获到异常时完成当前事务 （提交或者回滚） completeTransactionAfterThrowing(txInfo, ex); throw ex; &#125; finally &#123; cleanupTransactionInfo(txInfo); &#125; if (retVal != null &amp;&amp; vavrPresent &amp;&amp; VavrDelegate.isVavrTry(retVal)) &#123; // Set rollback-only in case of Vavr failure matching our rollback rules... TransactionStatus status = txInfo.getTransactionStatus(); if (status != null &amp;&amp; txAttr != null) &#123; retVal = VavrDelegate.evaluateTryFailure(retVal, txAttr, status); &#125; &#125; // 根据事务的状态提交或者回滚 commitTransactionAfterReturning(txInfo); return retVal; &#125; // 省略了编程式事务的处理 ...&#125; 这里代码很多，根据注释的位置，我们可以把核心逻辑梳理出来 获取当前事务属性，事务管理器（以注解事务为例，这些都可以通过@Transactional来定义） createTransactionIfNecessary，判断是否有必要创建事务 invocation.proceedWithInvocation 执行拦截器链，最终会执行到目标方法 completeTransactionAfterThrowing当抛出异常后，完成这个事务，提交或者回滚，并抛出这个异常 commitTransactionAfterReturning 从方法命名来看，这个方法会提交事务。但是深入源码中会发现，该方法中也包含回滚逻辑，具体行为会根据当前TransactionStatus的一些状态来决定（也就是说，我们也可以通过设置当前TransactionStatus，来控制事务回滚，并不一定只能通过抛出异常），详见AbstractPlatformTransactionManager::commit 我们继续，来看看createTransactionIfNecessary做了什么 TransactionAspectSupport::createTransactionIfNecessary1234567891011121314151617181920212223242526272829protected TransactionInfo createTransactionIfNecessary(@Nullable PlatformTransactionManager tm, @Nullable TransactionAttribute txAttr, final String joinpointIdentification) &#123; // If no name specified, apply method identification as transaction name. if (txAttr != null &amp;&amp; txAttr.getName() == null) &#123; txAttr = new DelegatingTransactionAttribute(txAttr) &#123; @Override public String getName() &#123; return joinpointIdentification; &#125; &#125;; &#125; TransactionStatus status = null; if (txAttr != null) &#123; if (tm != null) &#123; // 通过事务管理器开启事务 status = tm.getTransaction(txAttr); &#125; else &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Skipping transactional joinpoint [" + joinpointIdentification + "] because no transaction manager has been configured"); &#125; &#125; &#125; return prepareTransactionInfo(tm, txAttr, joinpointIdentification, status);&#125; createTransactionIfNecessary中的核心逻辑 通过PlatformTransactionManager（事务管理器）开启事务 prepareTransactionInfo 准备事务信息，这个具体做了什么我们稍后再讲 继续来看PlatformTransactionManager::getTransaction，该方法只有一个实现 AbstractPlatformTransactionManager::getTransaction 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public final TransactionStatus getTransaction(@Nullable TransactionDefinition definition) throws TransactionException &#123; // Use defaults if no transaction definition given. TransactionDefinition def = (definition != null ? definition : TransactionDefinition.withDefaults()); // 获取当前事务，该方法有继承 AbstractPlatformTransactionManager 的子类自行实现 Object transaction = doGetTransaction(); boolean debugEnabled = logger.isDebugEnabled(); // 如果目前存在事务 if (isExistingTransaction(transaction)) &#123; // Existing transaction found -&gt; check propagation behavior to find out how to behave. return handleExistingTransaction(def, transaction, debugEnabled); &#125; // Check definition settings for new transaction. if (def.getTimeout() &lt; TransactionDefinition.TIMEOUT_DEFAULT) &#123; throw new InvalidTimeoutException("Invalid transaction timeout", def.getTimeout()); &#125; // 传播类型PROPAGATION_MANDATORY, 要求当前必须有事务 // No existing transaction found -&gt; check propagation behavior to find out how to proceed. if (def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_MANDATORY) &#123; throw new IllegalTransactionStateException( "No existing transaction found for transaction marked with propagation 'mandatory'"); &#125; // PROPAGATION_REQUIRED, PROPAGATION_REQUIRES_NEW, PROPAGATION_NESTED 不存在事务时创建事务 else if (def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRED || def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW || def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; SuspendedResourcesHolder suspendedResources = suspend(null); if (debugEnabled) &#123; logger.debug("Creating new transaction with name [" + def.getName() + "]: " + def); &#125; try &#123; // 开启事务 return startTransaction(def, transaction, debugEnabled, suspendedResources); &#125; catch (RuntimeException | Error ex) &#123; resume(null, suspendedResources); throw ex; &#125; &#125; else &#123; // Create "empty" transaction: no actual transaction, but potentially synchronization. if (def.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT &amp;&amp; logger.isWarnEnabled()) &#123; logger.warn("Custom isolation level specified but no actual transaction initiated; " + "isolation level will effectively be ignored: " + def); &#125; boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); return prepareTransactionStatus(def, null, true, newSynchronization, debugEnabled, null); &#125;&#125; 代码很多，重点关注注释部分即可 doGetTransaction获取当前事务 如果存在事务，则调用handleExistingTransaction处理，这个我们稍后会讲到 接下来，会根据事务的传播决定是否开启事务 如果事务传播类型为PROPAGATION_MANDATORY，且不存在事务，则抛出异常 如果传播类型为 PROPAGATION_REQUIRED, PROPAGATION_REQUIRES_NEW, PROPAGATION_NESTED，且当前不存在事务，则调用startTransaction创建事务 当不满足 3、4时，例如 PROPAGATION_NOT_SUPPORTED，此时会执行事务同步，但是不会创建真正的事务 Spring 事务同步在之前一篇博客中有讲到，传送门👉https://www.jianshu.com/p/7880d9a98a5f Spring 如何管理当前的事务接下来讲讲上面提到的doGetTransaction、handleExistingTransaction，这两个方法是由不同的TransactionManager自行实现的 我们以SpringBoot默认的TransactionManager，DataSourceTransactionManager为例123456789101112131415@Overrideprotected Object doGetTransaction() &#123; DataSourceTransactionObject txObject = new DataSourceTransactionObject(); txObject.setSavepointAllowed(isNestedTransactionAllowed()); ConnectionHolder conHolder = (ConnectionHolder) TransactionSynchronizationManager.getResource(obtainDataSource()); txObject.setConnectionHolder(conHolder, false); return txObject;&#125;@Overrideprotected boolean isExistingTransaction(Object transaction) &#123; DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; return (txObject.hasConnectionHolder() &amp;&amp; txObject.getConnectionHolder().isTransactionActive());&#125; 结合 AbstractPlatformTransactionManager::getTransaction 一起来看，doGetTransaction 其实获取的是当前的Connection。判断当前是否存在事务，是判断DataSourceTransactionObject 对象中是否包含connection，以及connection是否开启了事务。 我们继续来看下TransactionSynchronizationManager.getResource(obtainDataSource())获取当前connection的逻辑 TransactionSynchronizationManager::getResource12345678910111213141516171819202122232425262728293031323334353637383940private static final ThreadLocal&lt;Map&lt;Object, Object&gt;&gt; resources = new NamedThreadLocal&lt;&gt;("Transactional resources");@Nullable// TransactionSynchronizationManager::getResourcepublic static Object getResource(Object key) &#123; // DataSourceTransactionManager 调用该方法时，以数据源作为key // TransactionSynchronizationUtils::unwrapResourceIfNecessary 如果key为包装类，则获取被包装的对象 // 我们可以忽略该逻辑 Object actualKey = TransactionSynchronizationUtils.unwrapResourceIfNecessary(key); Object value = doGetResource(actualKey); if (value != null &amp;&amp; logger.isTraceEnabled()) &#123; logger.trace("Retrieved value [" + value + "] for key [" + actualKey + "] bound to thread [" + Thread.currentThread().getName() + "]"); &#125; return value;&#125;/** * Actually check the value of the resource that is bound for the given key. */@Nullableprivate static Object doGetResource(Object actualKey) &#123; Map&lt;Object, Object&gt; map = resources.get(); if (map == null) &#123; return null; &#125; Object value = map.get(actualKey); // Transparently remove ResourceHolder that was marked as void... if (value instanceof ResourceHolder &amp;&amp; ((ResourceHolder) value).isVoid()) &#123; map.remove(actualKey); // Remove entire ThreadLocal if empty... if (map.isEmpty()) &#123; resources.remove(); &#125; value = null; &#125; return value;&#125; 看到这里，我们能明白DataSourceTransactionManager是如何管理线程之间的Connection，ThreadLocal 中存储一个Map，key为数据源对象，value为该数据源在当前线程的Connection DataSourceTransactionManager 在开启事务后，会调用TransactionSynchronizationManager::bindResource将指定数据源的Connection绑定到当前线程 AbstractPlatformTransactionManager::handleExistingTransaction我们继续回头看，如果存在事务的情况，如何处理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899private TransactionStatus handleExistingTransaction( TransactionDefinition definition, Object transaction, boolean debugEnabled) throws TransactionException &#123; // 如果事务的传播要求以非事务方式执行 抛出异常 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NEVER) &#123; throw new IllegalTransactionStateException( "Existing transaction found for transaction marked with propagation 'never'"); &#125; // PROPAGATION_NOT_SUPPORTED 如果存在事务，则挂起当前事务，以非事务方式执行 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NOT_SUPPORTED) &#123; if (debugEnabled) &#123; logger.debug("Suspending current transaction"); &#125; // 挂起当前事务 Object suspendedResources = suspend(transaction); boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); // 构建一个无事务的TransactionStatus return prepareTransactionStatus( definition, null, false, newSynchronization, debugEnabled, suspendedResources); &#125; // PROPAGATION_REQUIRES_NEW 如果存在事务，则挂起当前事务，新建一个事务 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW) &#123; if (debugEnabled) &#123; logger.debug("Suspending current transaction, creating new transaction with name [" + definition.getName() + "]"); &#125; SuspendedResourcesHolder suspendedResources = suspend(transaction); try &#123; return startTransaction(definition, transaction, debugEnabled, suspendedResources); &#125; catch (RuntimeException | Error beginEx) &#123; resumeAfterBeginException(transaction, suspendedResources, beginEx); throw beginEx; &#125; &#125; // PROPAGATION_NESTED 内嵌事务，就是我们开头举得例子 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; if (!isNestedTransactionAllowed()) &#123; throw new NestedTransactionNotSupportedException( "Transaction manager does not allow nested transactions by default - " + "specify 'nestedTransactionAllowed' property with value 'true'"); &#125; if (debugEnabled) &#123; logger.debug("Creating nested transaction with name [" + definition.getName() + "]"); &#125; // 非JTA事务管理器都是通过savePoint实现的内嵌事务 // savePoint：关系型数据库中事务可以创建还原点，并且可以回滚到还原点 if (useSavepointForNestedTransaction()) &#123; // Create savepoint within existing Spring-managed transaction, // through the SavepointManager API implemented by TransactionStatus. // Usually uses JDBC 3.0 savepoints. Never activates Spring synchronization. DefaultTransactionStatus status = prepareTransactionStatus(definition, transaction, false, false, debugEnabled, null); // 创建还原点 status.createAndHoldSavepoint(); return status; &#125; else &#123; // Nested transaction through nested begin and commit/rollback calls. // Usually only for JTA: Spring synchronization might get activated here // in case of a pre-existing JTA transaction. return startTransaction(definition, transaction, debugEnabled, null); &#125; &#125; // 如果执行到这一步传播类型一定是，PROPAGATION_SUPPORTS 或者 PROPAGATION_REQUIRED // Assumably PROPAGATION_SUPPORTS or PROPAGATION_REQUIRED. if (debugEnabled) &#123; logger.debug("Participating in existing transaction"); &#125; // 校验目前方法中的事务定义和已存在的事务定义是否一致 if (isValidateExistingTransaction()) &#123; if (definition.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT) &#123; Integer currentIsolationLevel = TransactionSynchronizationManager.getCurrentTransactionIsolationLevel(); if (currentIsolationLevel == null || currentIsolationLevel != definition.getIsolationLevel()) &#123; Constants isoConstants = DefaultTransactionDefinition.constants; throw new IllegalTransactionStateException("Participating transaction with definition [" + definition + "] specifies isolation level which is incompatible with existing transaction: " + (currentIsolationLevel != null ? isoConstants.toCode(currentIsolationLevel, DefaultTransactionDefinition.PREFIX_ISOLATION) : "(unknown)")); &#125; &#125; if (!definition.isReadOnly()) &#123; if (TransactionSynchronizationManager.isCurrentTransactionReadOnly()) &#123; throw new IllegalTransactionStateException("Participating transaction with definition [" + definition + "] is not marked as read-only but existing transaction is"); &#125; &#125; &#125; boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); // 构建一个TransactionStatus，但不开启事务 return prepareTransactionStatus(definition, transaction, false, newSynchronization, debugEnabled, null);&#125; 这里代码很多，逻辑看上述注释即可。这里终于看到了期待已久的挂起事务和内嵌事务了，我们还是看一下DataSourceTransactionManager的实现 挂起事务：通过TransactionSynchronizationManager::unbindResource 根据数据源获取当前的Connection，并在resource中移除该Connection。之后会将该Connection存储到TransactionStatus对象中 1234567// DataSourceTransactionManager::doSuspend@Overrideprotected Object doSuspend(Object transaction) &#123; DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; txObject.setConnectionHolder(null); return TransactionSynchronizationManager.unbindResource(obtainDataSource());&#125; 在事务提交或者回滚后，调用 AbstractPlatformTransactionManager::cleanupAfterCompletion会将TransactionStatus 中缓存的Connection重新绑定到resource中 内嵌事务：通过关系型数据库的savePoint实现，提交或回滚的时候会判断如果当前事务为savePoint则释放savePoint或者回滚到savePoint，具体逻辑参考AbstractPlatformTransactionManager::processRollback 和 AbstractPlatformTransactionManager::processCommit 至此，事务的传播源码分析结束 prepareTransactionInfo上文留下了一个问题，prepareTransactionInfo 方法做了什么，我们先来看下TransactionInfo的结构123456789101112131415161718protected static final class TransactionInfo &#123; @Nullable private final PlatformTransactionManager transactionManager; @Nullable private final TransactionAttribute transactionAttribute; private final String joinpointIdentification; @Nullable private TransactionStatus transactionStatus; @Nullable private TransactionInfo oldTransactionInfo; // ...&#125; 该类在Spring中的作用，是为了内部传递对象。ThreadLocal中存储了最新的TransactionInfo，通过当前TransactionInfo可以找到他的oldTransactionInfo。每次创建事务时会新建一个TransactionInfo（无论有没有真正的事务被创建）存储到ThreadLocal中，在每次事务结束后，会将当前ThreadLocal中的TransactionInfo重置为oldTransactionInfo，这样的结构形成了一个链表，使得Spring事务在逻辑上可以无限嵌套下去 如果觉得有收获，可以关注我的公众号，你的点赞和关注就是对我最大的支持]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>SpringBoot</tag>
        <tag>事务传播</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java SPI 实战]]></title>
    <url>%2Fpost%2Fe971632b.html</url>
    <content type="text"><![CDATA[SPI 全称为 (Service Provider Interface) ，是JDK内置的一种服务提供发现机制，可以轻松实现面向服务的注册与发现，完成服务提供与使用的解耦，并且可以实现动态加载 SPI 能做什么利用SPI机制，sdk的开发者可以为使用者提供扩展点，使用者无需修改源码，有点类似Spring @ConditionalOnMissingBean 的意思 动手实现一个SPI例如我们要正在开发一个sdk其中有一个缓存的功能，但是用户很可能不想使用我们的缓存实现，用户想要自定义缓存的实现，此时使用spi就非常的合适了 新建一个maven工程命名为sdk Cache 接口12345678910111213import java.util.ServiceLoader;public interface Cache &#123; String getName(); static Cache load() &#123; // ServiceLoader 实现了 Iterable，可以加载到Cache接口的多个实现类 ServiceLoader&lt;Cache&gt; cacheServiceLoader = ServiceLoader.load(Cache.class); return cacheServiceLoader.iterator().next(); &#125;&#125; ServiceLoader 是Java提供服务发现工具类，这是我们实现SPI的关键 CacheDefaultImpl12345public class CacheDefaultImpl implements Cache &#123; public String getName() &#123; return &quot;defaultImpl&quot;; &#125;&#125; 除此之外，ServiceLoader 还需要在classpath:META-INF/services 下找到以该接口全名命名的文件，这里我们直接在resource 目录下创建META-INF/services/ com.github.tavenyin.Cache文件即可，文件中指定Cache的实现类 12# 此处可以指定多个实现类com.github.tavenyin.CacheDefaultImpl Run我们建立一个新的maven子工程，并引入sdk模块，执行测试代码12System.out.println(Cache.load().getName()) # 输出结果为 defaultImpl 使用者定制化那么如果sdk的使用者不想使用我们的CacheDefaultImpl了怎么办，没关系使用者只需要覆盖 classpath:META-INF/services/com.github.tavenyin.Cache 就可以了 （使用者在同样在resource下创建即可覆盖） 我们再来运行一下测试代码，输出结果为 newImpl ServiceLoader 实现原理ServiceLoader 的实现原理还是比较简单的，试想一下，如果我们自己实现一个ServiceLoader，我们会怎么做？ 通过指定的文件加载出所有的类名 通过反射构建这些对象 没错，ServiceLoader 就是这么做的，我们来简单看一下源码 入口 ServiceLoader::iterator::next 1234567891011121314151617181920212223242526272829303132// Cached providers, in instantiation orderprivate LinkedHashMap&lt;String,S&gt; providers = new LinkedHashMap&lt;&gt;();// The current lazy-lookup iteratorprivate LazyIterator lookupIterator; // ServiceLoader::iteratorpublic Iterator&lt;S&gt; iterator() &#123; return new Iterator&lt;S&gt;() &#123; Iterator&lt;Map.Entry&lt;String,S&gt;&gt; knownProviders = providers.entrySet().iterator(); public boolean hasNext() &#123; if (knownProviders.hasNext()) return true; return lookupIterator.hasNext(); &#125; // ServiceLoader::iterator::next public S next() &#123; if (knownProviders.hasNext()) return knownProviders.next().getValue(); return lookupIterator.next(); &#125; public void remove() &#123; throw new UnsupportedOperationException(); &#125; &#125;;&#125; 从providers 初始为一个空的LinkedHashMap，我们无需关注，所以knownProviders::hasNext 一定返回false，我们直奔knownProviders::next knownProviders::next 中核心逻辑在nextService() 中1234567891011121314151617181920212223242526272829303132private S nextService() &#123; // hasNextService 中做了两件事 // 1. 判断是否还有服务的提供者 // 2. 通过 &quot;META-INF/services/&quot; + 接口全名 加载所有提供者ClassName if (!hasNextService()) throw new NoSuchElementException(); String cn = nextName; nextName = null; Class&lt;?&gt; c = null; try &#123; // 通过ClassName 创建Class c = Class.forName(cn, false, loader); &#125; catch (ClassNotFoundException x) &#123; fail(service, &quot;Provider &quot; + cn + &quot; not found&quot;); &#125; if (!service.isAssignableFrom(c)) &#123; fail(service, &quot;Provider &quot; + cn + &quot; not a subtype&quot;); &#125; try &#123; // 反射创建实现类实例 S p = service.cast(c.newInstance()); providers.put(cn, p); return p; &#125; catch (Throwable x) &#123; fail(service, &quot;Provider &quot; + cn + &quot; could not be instantiated&quot;, x); &#125; throw new Error(); // This cannot happen&#125; 与我们上述分析的实现过程一致，更多细节感兴趣的童鞋可自行阅读 ServiceLoader 如何实现动态加载同一个 ServiceLoader 对象的话，不会重新加载META-INF/services/下的信息。如果我们需要动态加载的话，可以考虑每次重新创建新的ServiceLoader 对象，或者调用 ServiceLoader::reload demo 地址https://github.com/TavenYin/java-spi.git 如果觉得有收获，可以关注我的公众号【殷天文】，你的点赞和关注就是对我最大的支持]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot Websocket 实战]]></title>
    <url>%2Fpost%2Fa77ef6c7.html</url>
    <content type="text"><![CDATA[Websocket 是一种在单个TCP连接上进行全双工通信的协议。WebSocket连接成功后，服务端与客户端可以双向通信。在需要消息推送的场景，Websocket 相对于轮询能更好的节省服务器资源和带宽，并且能够更实时地进行通讯。 与 HTTP 协议有着良好的兼容性。默认端口也是80和443，并且握手阶段采用 HTTP 协议，因此握手时不容易屏蔽，能通过各种 HTTP 代理服务器。 依赖于TCP协议 数据格式比较轻量，性能开销小，通信高效。 可以发送文本，也可以发送二进制数据。 没有同源限制，客户端可以与任意服务器通信。 协议标识符是ws（如果加密，则为wss），服务器网址就是 URL。 SpringBoot 中使用 Websocket在简单了解Websocket 之后，我们来动手实践一下。SpringBoot 中有多种方式可以实现Websocket Server，这里我选择使用Tomcat 中 javax.websocket.server 的api来实现，结尾会给出demo地址 引入Maven依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-websocket&lt;/artifactId&gt;&lt;/dependency&gt; 创建一个Bean用于处理Websocket 请求，通过ServerEndpoint 声明当前Bean 接受的Websocket URL 这里为什么声明的是 @Controller，后文会解释 1234567891011121314151617181920212223242526272829303132333435363738394041import org.springframework.stereotype.Controller;import javax.websocket.*;import javax.websocket.server.ServerEndpoint;@ServerEndpoint(value = &quot;/message_websocket&quot;)@Controllerpublic class MsgWebsocketController &#123; @OnOpen public void onOpen(Session session) &#123; // 先鉴权，如果鉴权通过则存储WebsocketSession，否则关闭连接，这里省略了鉴权的代码 WebSocketSupport.storageSession(session); System.out.println(&quot;session open. ID:&quot; + session.getId()); &#125; /** * 连接关闭调用的方法 */ @OnClose public void onClose(Session session) &#123; System.out.println(&quot;session close. ID:&quot; + session.getId()); &#125; /** * 收到客户端消息后调用的方法 */ @OnMessage public void onMessage(String message, Session session) &#123; System.out.println(&quot;get client msg. ID:&quot; + session.getId() + &quot;. msg:&quot; + message); &#125; /** * 发生错误时调用 */ @OnError public void onError(Session session, Throwable error) &#123; error.printStackTrace(); &#125;&#125; 声明 ServerEndpointExporter 123456789@Configurationpublic class WebsocketConfig &#123; @Bean public ServerEndpointExporter serverEndpointExporter() &#123; return new ServerEndpointExporter(); &#125;&#125; 至此，Websocket Server 已经搭建完成，客户端已经可以和服务端通信了 服务端 向客户端推送消息 通过 session.getBasicRemote().sendText(message); 即可 源码浅析我们来看下上述的短短几行代码是如何为我们构建 Websocket Server ServerEndpointExporter 重点关注下红框中的内容 ServerEndpointExporter 实现了 SmartInitializingSingleton，会在bean 实例化结束后调用 afterSingletonsInstantiated 从Spring上下文中获取所有标记@ServerEndpoint的Bean的name 其实 我们声明的 MsgWebsocketController 中并不是只能标记@Controller，只是为了将其注册到Spring容器中，方便ServerEndpoint的注册而已，标记 @Controller 更符合Spring的开发规范 通过ServerContainer 将所有标记@ServerEndpoint的Bean 注册 ServerContainer 默认的实现类为 WsServerContainer，会对我们的ServerEndpoint做一个映射，URL =&gt; 对应的class，然后针对不同的事件调用指定的方法（例如建立连接时调用标记@Onopen的方法），这有点Spring DispatcherServlet 那味，感兴趣的同学可以自己看下 在了解了 Spring 为我们做了什么后，我们来完善一下我们的Demo 建立一个SessionManager当我们想向客户端推送消息的时候，首先我们需要找到客户端与服务端建立的连接，也就是WebscoketSession WsServerContainer 中虽然已经存储了 WebscoketSession，但是并没有办法直接通过SessionId，或者我们的业务Id 直接定位到指定的Session，所以我们需要实现一个自己的SessionManager 1final ConcurrentHashMap&lt;Object, Session&gt; sessionPool = new ConcurrentHashMap&lt;&gt;(); 使用 ConcurrentHashMap 管理即可 分布式推送解决 如图，用户1与服务器A建立Webscoket，用户2与服务器B建立Webscoket，那么用户1如果想向用户2推送一条消息，该如何实现？ WebscoketSession 实际上是网络连接，并不像我们传统应用的Session可以序列化到Redis，只能每个服务器管理自己的WebscoketSession，所以此时服务器A通知服务器B，你要给用户2推送一条消息。 一个比较简单有效的实现方法，利用消息队列，如下图 这个方案优点是实现简单，缺点是每台服务器都需要判断一遍当前是否存在指定的WebscoketSession ，方案细化的话则需要维护用户Session与每台服务器的关系，这样直接将消息推送给指定服务器即可 其他问题测试时发现，当客户端断网后，服务端检测不到客户端失去连接的情况，依然可以调用Session的推送方法，服务端会一直持有这个无效的Session 目前想到的解决方案：设置WebsocketSession的最大空闲时间（session.setMaxIdleTimeout(milliseconds);），当超过这个时间时，服务端会关闭Session。前端定期发送一条心跳包，用于维持Session，当出现上述情况时，服务端也不会一直持有Session了 完整demo地址关于demo的细节参考项目地址中Readme Github 👉 https://github.com/TavenYin/taven-springboot-learning/tree/master/sp-websocket Gitee 👉 https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/sp-websocket 参考http://www.ruanyifeng.com/blog/2017/05/websocket.html 部分代码参考了一位兄弟的博客，但是由于时间有点长，找不到了，在此说一声抱歉 如果觉得有收获，可以关注我的公众号【殷天文】，第一时间接收到我的更新]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>Websocket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis能否保证数据高可靠性]]></title>
    <url>%2Fpost%2Ff4cc44a4.html</url>
    <content type="text"><![CDATA[记录下工作中关于Redis的一些思考，主要关于Redis的事务，脚本，持久化 本文讨论的问题： Redis的事务或者执行lua脚本可以像关系型数据库事务那样，要么全部提交，要么全部回滚吗？ 当脚本或者事务执行过程中发生宕机Redis中的数据会丢失吗？ 原子性在Redis的开发文档中可以了解到，Redis的事务以及Redis执行lua脚本都可以保证原子性（Redis的每条命令也是原子的），那么原子性可以保证什么？能否解决我们的问题？先来看下原子性的定义 来自维基百科对关系型数据库事务原子性的定义 事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行 来自百度百科原子操作的定义 原子操作是不可分割的，在执行完毕之前不会被任何其它任务或事件中断 Redis 所保证的原子性正是如此 不可分割，保证事务和脚本执行过程中，不会被其他客户端的命令打断 事务和脚本中的命令，要么都执行，要么都不执行 Redis Transactions如何使用Redis 事务12345&gt; MULTI // 开启事务OK&gt; SET KEY VALUE // 执行命令，此时的命令只是入队，会在 EXEC 之后原子性的执行事务中所有命令QUEUED&gt; EXEC / DISCARD // 执行或者取消事务 Redis的事务保证 在Redis事务的执行过程中，永远不会执行另一个客户端发出的请求 所有命令都会执行，或者不执行。如果在执行 EXEC 之前，客户端与服务端失去响应，那么事务中所有的命令都不会执行，相反如果执行 EXEC 保证所有的命令都执行。 事务中可能发生的错误 EXEC 之前：命令无法Queued，例如语法错误（错误的参数数量，错误的命令），或者一些其他错误，例如内存不足（Redis使用了maxmemory 限制最大内存） EXEC 之后：命令也可能会失败，例如针对string类型使用list的命令 EXEC 之前的错误，客户端可以通过检查服务端的响应来解决，当发生入队失败时（Redis响应值非QUEUED），客户端DISCARD当前事务 从Redis 2.6.5开始，Redis 会记录事务期间发生的错误，并拒绝执行事务，在执行EXEC时返回错误信息并自动丢弃该事务 但是EXEC 之后的错误，即使导致部分命令执行失败，Redis还是会执行其他的剩余命令 Redis 事务的不回滚机制虽然Redis保证了原子性，但是他的事务并不会像关系型数据库那样，在Redis事务中如果某条命令发生了错误，其他的命令会依旧执行，这点相比较关系型数据库来说不免有些”奇怪”了 Redis开发文档中给出的解释如下 只有语法错误才可能导致命令执行失败，大多数情况都是编程错误导致的 因为不需要回滚，使得 Redis 更简单 更高效 关于事务的更多内容👉 https://redis.io/topics/transactions Redis lua2.6.0 版本后，Redis 增加了对lua脚本的支持，脚本和Redis事务一样保证了原子性，执行脚本时不会执行其他脚本或Redis命令（所以不要让脚本运行时间过长），同样脚本也没有回滚机制，当脚本中出现lua的异常，或者Redis命令错误，也无法保证全部执行成功 Redis lua 和事务有点类似，但是有些场景使用事务是无法做到的，例如我想对Redis中的数据先读，然后根据原有数据变更，整个过程想要保证原子性，由于事务在EXEC之前无法获取返回值，使用lua 就非常合适 关于Redis lua 更多内容 👉 https://redis.io/commands/eval Redis 执行命令时宕机数据会丢失吗看到了这，第一个问题我们已经清楚了，Redis并没有回滚的能力，但是通常情况下，这些需要回滚的场景都是编码错误，我们是可以避免的。我们继续探寻第二个问题的答案 Redis是基于内存的数据库，所以当发生宕机或者停止后重新启动时，Redis会使用磁盘上的持久化文件来恢复数据，所以是否能恢复数据，能恢复多少数据，取决于使用哪一种持久化策略 简单说下两种持久化策略 RDB按指定的时间间隔将内存中数据集写入快照 AOFAOF会记录服务器接收的每个写入操作。当Redis命令执行成功后，命令会被传播到AOF程序中。AOF 的三种同步策略 appendfsync always ：每次有数据修改发生时都会同步到AOF文件 appendfsync everysec ：每秒钟同步一次，AOF的默认策略 appendfsync no ：将数据同步将给操作系统管理，通常linux系统30s会同步一次数据，但这取决于操作系统 即使使用always 也无法保证写入的每一条命令都被持久化，从命令执行成功到数据保存到硬盘之间，还是有一段非常小的间隔 回到我们的问题上，如果使用RDB，毫无疑问，数据只能恢复到上次的备份 即使使用AOF的话，如果在Redis事务执行期间宕机，那么这次事务还是相当于”没有执行”，由于命令还没来及写入AOF，在服务恢复后更不可能恢复数据。对于Redis客户端而言，会收到服务端的异常响应 写入AOF的过程也是会被打断的，Redis 文档中提到，如果Redis服务器突然崩溃，导致出现了”半写状态”的AOF文件时，服务器重新启动时，会检测到这种情况，并且退出提示用户使用 redis-check-aof 修复 AOF 文件。”半写状态”的事务或者命令会被删除，服务器可以重新启动。 如果写入AOF过程被打断，对于客户端而言可能是毫无感知的（看了下Redis命令执行相关，AOF应该是发生在响应客户端之后） 所以第二个问题，我们也清楚了，Redis 并不能保证我们写入的数据都安全的持久化 关于持久化的更多内容👉 https://redis.io/topics/persistence 扩展：脚本如何持久化这里说的是AOF的情况 在Redis 5 之前，默认是将脚本本身传播到AOF中。这种传播方式的好处，不需要将脚本转成Redis命令，在写入AOF或者其他Redis实例时不会占用过多的带宽和CPU 复制脚本不允许脚本中出现随机性的写入，因为这会导致通过AOF恢复数据时，数据不一致，在这点上Redis做了一些限制，由于不是本文重点就不多说了，可以参考Redis开发文档 从Redis 3.2 开始新增了一种脚本复制方式 script effects replication（Redis 5 开始默认使用这种方式处理脚本）。这种模式下，Redis 会收集脚本中所有修改数据的命令。当脚本执行完成后，这些命令被包装成一个事务，传播到AOF 和其他实例。 这种方式的好处 当脚本执行很慢的时候，会影响加载AOF重建数据的时间，这种情况使用 script effects replication 效率更高 这种方式会允许脚本中存在随机性的写入 使用方式12-- 在执行Redis命令前调用，成功启用 script effects replication 返回trueredis.replicate_commands() 总结所以说Redis无论是事务还是脚本，并不能做到像关系型数据事务一样，所以针对数据一致性要求较高的业务场景，并不适合使用Redis 而且从持久化方面来考虑，这也不是Redis的强项，Redis的优势正是基于内存，所以读写性能高。虽然宕机的可能性看似很极端，通常我们使用了某个服务后，我们会尽可能的保证它的高可用，但是我们需要知道Redis的”持久化”并不能保证我们的数据绝对安全，所以当我们的业务场景对数据一致性，持久化要求很高的时候，关系型数据库依旧是很好的选择 参考Redis 设计与实现 AOFRedis 设计与实现 事务Redis 命令执行过程(上)Redis 命令执行过程(下)]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java ThreadLocal 实现原理]]></title>
    <url>%2Fpost%2F764a84a5.html</url>
    <content type="text"><![CDATA[ThreadLocal 线程本地变量，算是Java开发中比较常用的API了，今天我们来一探究竟 使用场景ThreadLocal 适用于每个线程需要自己独立的实例且该实例需要在多个方法中被使用，也就是变量在线程间隔离，而在同一线程共享的场景。例如管理Connection，我们希望每个线程只使用一个Connection实例，这个时候用ThreadLocal就很合适。 12345678910111213141516public class ThreadLocalDemo &#123; private static final ThreadLocal&lt;Object&gt; threadLocal = new ThreadLocal&lt;&gt;(); public static void main(String[] args) &#123; threadLocal.set(new Object()); someMethod(); &#125; static void someMethod() &#123; // 获取在threadLocal中存储的对象 threadLocal.get(); // 清除ThreadLocal中数据 threadLocal.remove(); &#125;&#125; 还有之前写过的一篇动态切换数据源 https://www.jianshu.com/p/0a485c965b8b，AOP 通过 ThreadLocal 保存当前线程需要访问的数据源的key，AbstractRoutingDataSource 再通过 ThreadLocal 中的数据切换到指定的数据源，对业务代码毫无入侵 原理在我们了解了如何使用之后，来看下 ThreadLocal 是如何实现的 ThreadLocal.get()我们从get方法来分析，可以看到方法中获取当前线程，并通过当前线程得到一个 ThreadLocalMap，我们可以暂时把这个ThreadLocalMap 理解为我们熟悉的HashMap，然后通过 this（当前ThreadLocal对象）作为key，从Map中获取Entry 我们再来看下，ThreadLocalMap 以及ThreadLocalMap.Entry 中的核心成员变量，ThreadLocalMap 中实现了一个简单的hash表 看到这里你可能还不是很清晰，结合下面这张图理解一下，每个线程（Thread对象）中有一个ThreadLocalMap，使线程之间的数据天然隔离，ThreadLocalMap 有一张hash表 Entry[]，每个 Entry 中对应存储着一个ThreadLocal实例 - value，这样使得不同的ThreadLocal 对象之间也形成了隔离 ThreadLocalMap 中的hash表我们通过 ThreadLocalMap.set() 来了解下内部的hash表是如何实现的 线性探测是指当发生hash冲突时，利用固定的算法寻找一定步长的下个位置（ThreadLocal中发生hash冲突时，index+1），依次判断，直至找到能够存放的位置 如果线程中操作了大量的 ThreadLocal 对象，势必会造成hash冲突，这是没有必要的性能开销，如果可以的话，我们可以只保留一个ThreadLocal对象 关于 ThreadLocal 的一些思考 为什么要使用弱引用 图3中，我们看到hash表中会出现 key == null的Entry，这是因为 ThreadLocalMap.Entry 的key （Entry 对ThreadLocal设置了弱引用，可以回顾一下图2） 弱引用的对象拥有更短暂的生命周期。在GC时，一旦发现了对象只具有弱引用，这个对象一定被回收 这么做的原因：如果ThreadLocal 对象需要被回收时（此时并没有调用ThreadLocal.remove），线程中的ThreadLocalMap 一直强引用着 ThreadLocal对象，这会让 ThreadLocal对象 以及对应的value对象内存无法释放，导致内存泄漏。这算是ThreadLocal的一种容错机制，这样做使得了ThreadLocal对象得到了回收，但是value的内存并没有释放，所以ThreadLocalMap 的get、set方法中都会去尝试清理ThreadLocal已经被回收的entry。 使用过后不及时remove会怎么样 很多博客中都强调了，ThreadLocal.remove的重要性。举个例子，我们新启了一个线程在这个线程中使用了ThreadLocal，我们并没有调用remove，这会导致存储的value对象一直没有办法被回收，直到线程被销毁 线程池中也需要remove吗 以web线程池为例，如果每次都在过滤器中操作同一个ThreadLocal.set，然后业务代码中get，似乎没什么问题。计算出的hash值都是一样的，槽位也是一样的会覆盖上一次的值。确实业务不会有问题，但是还是推荐大家在使用完之后remove，因为这样会让无用的value对象早点被回收，在很多java源码中都会看到，对一些不再使用的对象进行如下的help GC操作1object = null // help GC 所以我们也需要让无用的对象失去引用，帮助GC 综上所述 ThreadLocal 使用过后要及时remove，帮助JVM释放内存 参考https://www.jianshu.com/p/98b68c97df9b]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Thread</tag>
        <tag>多线程</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电商技术--库存设计指北]]></title>
    <url>%2Fpost%2Fc4f13c61.html</url>
    <content type="text"><![CDATA[前言最近在解决一套老电商系统的库存”超卖”问题。一直以为超卖问题，最难解决的是库存扣减，实则不然，我们的系统在解决了库存扣减问题之后，还会一直有“超卖”现象？这一切的背后到底是道德的沦丧，还是人性的扭曲，欢迎收看本期走近科学 本文带你解决以下电商场景问题 保证库存线程安全的扣减 防止库存的多次扣减、回滚 超时未支付被取消的订单（取消会回滚库存）， 如果收到了支付回调怎么办 如何线程安全的扣减库存先来说说库存扣减的问题，这是我们原来老系统的逻辑，注意！这里是错误的示例 123456789// 以下是伪代码，错误的示例// 查询出Goods对象$goods = selectGoodsById($id);if ($goods-&gt;num - $order_num &gt; 0) &#123; // 计算出扣减后的库存 $goods-&gt;num = $goods-&gt;num - $order_num; // 保存 save($goods);&#125; 上述代码犯了大忌，并发情况会导致多个线程读到相同的库存数，然后扣减，然后保存到DB，下面我们来说下正确的姿势 正确的做法利用MySQL update 会持有当前记录锁的特点，保证线程安全的扣减 SQL 示例：1update kucun set num = num - ? where id = ? and num - ? &gt;= 0 我们的这条记录根据主键更新，当事务A update 这条记录时，会持有当前记录的锁，当事务A未提交时，其他想要更新这条记录的事务只能等待锁释放 关于MySQL update 锁的细节，本文不讨论，可以参考MySQL文档 https://dev.mysql.com/doc/refman/8.0/en/innodb-locks-set.htmlhttps://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.htmlhttps://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html 虽然MySQL可以保证数据的准确性，但是大并发量场景下，大量的锁竞争，导致库存的扣减可能成为系统性能的瓶颈 使用 Redis 库存扣减使用Redis的优势很多，单线程的文件事件处理器保证了并发下可以线程的安全扣减、回滚库存， 以及Redis高性能。 虽然Redis解决了线程安全和性能的问题，但是Redis并不能做到像MySQL那样一条SQL命令完成库存扣减，我们需要先读出已有库存，再和当前下单库存做一个判断是否可以库存扣减。所以最佳的实现方案是通过Redis 执行lua脚本，保证整个逻辑处理期间，不会有其他客户端插进来 12345678910111213141516171819202122232425262728293031/** * * 扣减库存Lua脚本 * 库存（stock）-1：表示不限库存 * 库存（stock）0：表示没有库存 * 库存（stock）大于0：表示剩余库存 * * @params 库存key * @return * -3:库存未初始化 * -2:库存不足 * -1:不限库存 * 大于等于0:剩余库存（扣减之后剩余的库存） */const SUB_STOCK_LUA = &quot; if (redis.call(&apos;exists&apos;, KEYS[1]) == 1) then local stock = tonumber(redis.call(&apos;get&apos;, KEYS[1])); local num = tonumber(ARGV[1]); if (stock == -1) then return -1; end; if (stock &gt;= num) then return redis.call(&apos;incrby&apos;, KEYS[1], 0 - num); end; return -2; end; return -3;&quot;; 注意：当对一个订单中的 good_list 扣减库存的时候，需要注意，当某一个商品库存扣减失败时，之前的扣减的商品库存需要回滚。这会涉及到对redis的多次操作，你可以把整体逻辑写到一个lua脚本中 使用Redis 做库存扣减会有一个问题（伪代码如下），Redis数据和MySQL数据并不能保证强一致性，因为Redis的数据相当于直接写进去了，如果在需要回滚的时候，Redis不可用了导致数据无法回滚，最终会造成MySQL没有写入订单数据，Redis却扣减了库存 123456789101112try &#123; $db-&gt;beginTransaction(); $db-&gt;saveOrder(); $redis-&gt;reduceStock(); $db-&gt;commit(); &#125; catch (Exception $e) &#123; $db-&gt;rollback(); $redis-&gt;rollbackStock();&#125; 这种情况并没有什么好的解决办法，这是一个几率非常小的故障，首先我们肯定要尽可能的保证Redis的高可用性，其次在发生这种情况后，我们要想办法恢复Redis中的数据，例如我们可以在整个逻辑之后，选择异步的方式（例如MQ）向MySQL中同步库存，当发生故障后，以MySQL数据为准恢复数据 所以Redis是一把双刃剑，提升性能的同时，也带来了问题 AliSQL这是后来我在网上看到的方案，AliSQL 是阿里自研 MySQL 分支，AliSQL 针对并发修改同一记录的情况，使用数据库层面的缓冲队列，避免大量争锁的代价。感兴趣的同学可以试下（阿里云MySQL 8.0 集成了这一功能），如果AliSQL解决了性能问题的话，那么这个方案相比Redis要更好 关于库存多次扣减的问题当订单的提交和库存的扣减同步进行的时候，不需要考虑这个问题。 举例：订单系统生成订单之后，通过MQ通知库存系统，库存系统异步扣减库存，这个时候库存系统可能会多次消费，这个时候就需要考虑这个问题了。 或者我们上面说的通过MQ同步MySQL库存也需要考虑可能发生多次扣减 解决方案如图，通过订单做为唯一索引保证流水记录的唯一性，从而保证只能有一次成功的扣减 库存回滚问题多数博客对于超卖的讲解只在于库存的扣减，但是库存扣减安全了，真的就可以保证不超卖吗？我们的系统在解决了库存扣减问题后，还是出现成交订单 &gt; 库存的问题，为此我也是绞尽脑汁，抓破了头 在对下单进行压力测试之后，我坚信下单不会出现超卖的问题，后来我怀疑问题出在了库存回滚，如果一个订单回滚了两次库存（取消超时未支付订单的线程和用户线程同时取消一个订单），同样也会出现超卖的现象。 解决方法：和防止多次扣减一样，采用写入订单回滚流水的方式，个人认为这种方法比较加锁要好，数据有迹可循 超时未支付被取消的订单收到了支付回调在解决了库存回滚问题之后，超卖问题还没有解决，最后通过日志定位到了这个问题。 问题描述：用户在系统即将自动取消订单的前一瞬间完成了支付，系统取消了该订单并回滚了库存，同时系统收到了该订单的支付回调，该订单的状态更改为已支付，因为不该出现的库存回滚导致了“超卖” 下面说下我们的解决方案，以微信支付为例 我们的系统在提交订单之后，会调用微信的统一下单接口，这时候微信收到了我们的商户订单号（微信已经生成订单），用户选择不支付。超时自动取消逻辑处理之前，先调用微信的关闭订单接口，如果关闭成功，则这个时候用户后续无法对该订单发起支付。如果返回订单已支付，则无需处理该订单，该订单会收到微信支付的回调 参考https://www.jianshu.com/p/76bc0e963172https://www.zhihu.com/question/268937734https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=9_3 如果觉得文章有帮助，欢迎点赞、转发、关注我的公众号，你的支持就是我最大的动力]]></content>
      <categories>
        <category>电商技术</category>
      </categories>
      <tags>
        <tag>电商</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教你如何使用MySQL8递归.]]></title>
    <url>%2Fpost%2F2e0c1b4e.html</url>
    <content type="text"><![CDATA[之前写过一篇 MySQL通过自定义函数的方式，递归查询树结构，从MySQL 8.0 开始终于支持了递归查询的语法 CTE首先了解一下什么是 CTE，全名 Common Table Expressions12345WITH cte1 AS (SELECT a, b FROM table1), cte2 AS (SELECT c, d FROM table2)SELECT b, d FROM cte1 JOIN cte2WHERE cte1.a = cte2.c; cte1, cte2 为我们定义的CTE，可以在当前查询中引用 可以看出 CTE 就是一个临时结果集，和派生表类似，二者的区别这里不细说，可以参考下MySQL开发文档：https://dev.mysql.com/doc/refman/8.0/en/with.html#common-table-expressions-recursive-examples 递归查询先来看下递归查询的语法1234567WITH RECURSIVE cte_name AS( SELECT ... -- return initial row set UNION ALL / UNION DISTINCT SELECT ... -- return additional row sets)SELECT * FROM cte; 定义一个CTE，这个CTE 最终的结果集就是我们想要的 ”递归得到的树结构”，RECURSIVE代表当前 CTE 是递归的 第一个SELECT 为 “初始结果集” 第二个SELECT 为递归部分，利用 “初始结果集/上一次递归返回的结果集” 进行查询得到 “新的结果集” 直到递归部分结果集返回为null，查询结束 最终UNION ALL 会将上述步骤中的所有结果集合并（UNION DISTINCT 会进行去重），再通过 SELECT * FROM cte; 拿到所有的结果集 递归部分不能包括： 聚合函数例如 SUM() GROUP BY ORDER BY LIMIT DISTINCT 上面的讲解可能有点抽象，通过例子慢慢来理解12345678910111213141516171819WITH RECURSIVE cte (n) AS -- 这里定义的n相当于结果集的列名，也可在下面查询中定义( SELECT 1 UNION ALL SELECT n + 1 FROM cte WHERE n &lt; 5)SELECT * FROM cte;-- result+------+| n |+------+| 1 || 2 || 3 || 4 || 5 |+------+ 初始结果集为 n =1 这时候看递归部分，第一次执行 CTE结果集即是 n =1，条件发现并不满足 n &lt; 5，返回 n + 1 第二次执行递归部分，CTE结果集为 n = 2，递归… 直至条件不满足 最后合并结果集 EXAMPLE最后来看一个树结构的例子123456CREATE TABLE `c_tree` ( `id` int(11) NOT NULL AUTO_INCREMENT, `cname` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL, `parent_id` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=13 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; 1234567891011121314151617mysql&gt; select * from c_tree;+----+---------+-----------+| id | cname | parent_id |+----+---------+-----------+| 1 | 1 | 0 || 2 | 2 | 0 || 3 | 3 | 0 || 4 | 1-1 | 1 || 5 | 1-2 | 1 || 6 | 2-1 | 2 || 7 | 2-2 | 2 || 8 | 3-1 | 3 || 9 | 3-1-1 | 8 || 10 | 3-1-2 | 8 || 11 | 3-1-1-1 | 9 || 12 | 3-2 | 3 |+----+---------+-----------+ 1234567891011121314151617mysql&gt; WITH RECURSIVE tree_cte as( select * from c_tree where parent_id = 3 UNION ALL select t.* from c_tree t inner join tree_cte tcte on t.parent_id = tcte.id)SELECT * FROM tree_cte;+----+---------+-----------+| id | cname | parent_id |+----+---------+-----------+| 8 | 3-1 | 3 || 12 | 3-2 | 3 || 9 | 3-1-1 | 8 || 10 | 3-1-2 | 8 || 11 | 3-1-1-1 | 9 |+----+---------+-----------+ 初始结果集R0 = select * from c_tree where parent_id = 3 递归部分，第一次 R0 与 c_tree inner join 得到 R1 R1 再与 c_tree inner join 得到 R2 … 合并所有结果集 R0 + … + Ri 更多信息https://dev.mysql.com/doc/refman/8.0/en/with.html]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redisson源码解析，如何利用Redis实现分布式可重入锁]]></title>
    <url>%2Fpost%2Fd5899455.html</url>
    <content type="text"><![CDATA[最开始使用Redisson 的api的时候，我觉得哇，这个api 太牛逼了居然有分布式的可重入锁，正好最近研究了下Redisson的源码，和大家分享一下 前言首先我们先回顾一下 Java 中的 ReentrantLock 是如何实现的？ 这里我先简单介绍一下ReentrantLock 实现的思路 锁标识：通过AQS的state变量作为锁标识，利用Java的CAS保证多线程竞争锁时的线程安全问题 队列：未竞争到锁的线程进入AQS的队列并挂起，等待解锁时被唤醒（或者超时） 如何设计分布式可重入锁首先锁标识，这个在Redis中很容易实现，可以用lock name 作为key，当前线程生成一个uuid，作为value，加上Redis 单线程模型，实现线程安全的锁竞争 这种方式在之前的博客里也提到过，可以参考下 Redis分布式锁的正确实现方式 但是如何基于Redis 做一个队列，像Java那样可以挂起唤醒线程呢？这点我在看源码之前一直没有想到… 那么Redisson 是如何做的呢？ 答案：利用Redis的发布订阅，加上Java的Semaphore（信号量，不了解Semaphore的小伙伴可以Google一下） Redisson 分布式锁实现思路锁标识：Hash 数据结构，key 为锁的名字，filed 当前竞争锁成功线程的“唯一标识”，value 重入次数 队列：所有竞争锁失败的线程，会订阅当前锁的解锁事件，利用 Semaphore 实现线程的挂起和唤醒 源码分析我们来看一下tryLock方法的源码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 public boolean tryLock(long waitTime, long leaseTime, TimeUnit unit) throws InterruptedException &#123; long time = unit.toMillis(waitTime); long current = System.currentTimeMillis(); long threadId = Thread.currentThread().getId(); // 尝试获取锁，返回null 代表获取锁成功，当获取锁失败时返回当前锁的释放时间 Long ttl = tryAcquire(leaseTime, unit, threadId); // lock acquired if (ttl == null) &#123; return true; &#125; // 如果此时已经超过等待时间则获取锁失败 time -= System.currentTimeMillis() - current; if (time &lt;= 0) &#123; acquireFailed(threadId); return false; &#125; current = System.currentTimeMillis(); // 订阅解锁事件 RFuture&lt;RedissonLockEntry&gt; subscribeFuture = subscribe(threadId); // 等待订阅成功，成功后唤醒当前线程 if (!await(subscribeFuture, time, TimeUnit.MILLISECONDS)) &#123; if (!subscribeFuture.cancel(false)) &#123; subscribeFuture.onComplete((res, e) -&gt; &#123; if (e == null) &#123; unsubscribe(subscribeFuture, threadId); &#125; &#125;); &#125; acquireFailed(threadId); return false; &#125; try &#123; // 再次判断一下是否超时 time -= System.currentTimeMillis() - current; if (time &lt;= 0) &#123; acquireFailed(threadId); return false; &#125; while (true) &#123; long currentTime = System.currentTimeMillis(); // 尝试获取锁 ttl = tryAcquire(leaseTime, unit, threadId); // lock acquired if (ttl == null) &#123; return true; &#125; time -= System.currentTimeMillis() - currentTime; if (time &lt;= 0) &#123; acquireFailed(threadId); return false; &#125; // waiting for message currentTime = System.currentTimeMillis(); if (ttl &gt;= 0 &amp;&amp; ttl &lt; time) &#123; // 等待解锁消息，此处利用Semaphore，锁未释放时，permits=0，线程处于挂起状态 // 当发布解锁消息时，当前的Semaphore对象的release() permits=1 // 所有的客户端都会有一个线程被唤醒，去尝试竞争锁 getEntry(threadId).getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS); &#125; else &#123; getEntry(threadId).getLatch().tryAcquire(time, TimeUnit.MILLISECONDS); &#125; time -= System.currentTimeMillis() - currentTime; if (time &lt;= 0) &#123; acquireFailed(threadId); return false; &#125; &#125; &#125; finally &#123; unsubscribe(subscribeFuture, threadId); &#125;// return get(tryLockAsync(waitTime, leaseTime, unit)); &#125; tryAcquire(leaseTime, unit, threadId); 这个方法我们下面会分析，现在我们只需要知道这个方法是用来获取锁就可以了 这个时候我们已经可以理清Redisson可重入锁的思路了 获取锁 如果获取锁失败，订阅解锁事件 之后是一个无限循环12345678910while(true) &#123; // 尝试获取锁 // 判断是否超时 // 等待解锁消息释放信号量 //（此时每个Java客户端都可能会有多个线程被挂起，但是只有一个线程会被唤醒） // 判断是否超时&#125; 利用信号量，合理控制线程对锁的竞争，合理利用系统资源，可以说做的灰常的奈斯了 需要注意：!await(subscribeFuture, time, TimeUnit.MILLISECONDS) ，这里很多博客都解释错了，这里并不是等待发布解锁消息，只要订阅事件成功后，就会往下执行，真正等待解锁消息的是 getEntry(threadId).getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS); 这里你可能不信，为什么我说的就对啊，debug一下你就知道 tryLockInnerAsynctryAcquire 内部依靠 tryLockInnerAsync 来实现获取锁的逻辑，我们来看下源码123456789101112131415161718192021222324252627&lt;T&gt; RFuture&lt;T&gt; tryLockInnerAsync(long leaseTime, TimeUnit unit, long threadId, RedisStrictCommand&lt;T&gt; command) &#123; internalLockLeaseTime = unit.toMillis(leaseTime); return commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, command, // 是否存在锁 "if (redis.call('exists', KEYS[1]) == 0) then " + // 不存在则创建 "redis.call('hset', KEYS[1], ARGV[2], 1); " + // 设置过期时间 "redis.call('pexpire', KEYS[1], ARGV[1]); " + // 竞争锁成功 返回null "return nil; " + "end; " + // 如果锁已经被当前线程获取 "if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " + // 重入次数加1 "redis.call('hincrby', KEYS[1], ARGV[2], 1); " + "redis.call('pexpire', KEYS[1], ARGV[1]); " + "return nil; " + "end; " + // 锁被其他线程获取，返回锁的过期时间 "return redis.call('pttl', KEYS[1]);", // 下面三个参数分别为 KEYS[1], ARGV[1], ARGV[2] // 即锁的name，锁释放时间，当前线程唯一标识 Collections.&lt;Object&gt;singletonList(getName()), internalLockLeaseTime, getLockName(threadId));&#125; tryLockInnerAsync 中利用lua脚本 和 Redis 单线程的特点来实现锁的竞争 这里可以看到锁的结构，和我们上文所说的一样，Hash 数据结构，key 为锁的name，filed 当前竞争锁成功线程的”唯一标识”，value 重入次数 unlockInnerAsync接下来我们再来看解锁的核心代码123456789101112131415161718192021222324252627282930protected RFuture&lt;Boolean&gt; unlockInnerAsync(long threadId) &#123; return commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN, // 用锁的name和线程唯一标识去判断是否存在这样的键值对 // 解铃还须系铃人，不存在则无权解锁，返回null "if (redis.call('hexists', KEYS[1], ARGV[3]) == 0) then " + "return nil;" + "end; " + // 解锁逻辑 // 冲入次数-1 "local counter = redis.call('hincrby', KEYS[1], ARGV[3], -1); " + // 如果大于0 代表当前线程重入锁多次无法解锁，更新锁的有效时间 "if (counter &gt; 0) then " + "redis.call('pexpire', KEYS[1], ARGV[2]); " + "return 0; " + "else " + // 解锁，删除key "redis.call('del', KEYS[1]); " + // 发布解锁消息 "redis.call('publish', KEYS[2], ARGV[1]); " + "return 1; "+ "end; " + "return nil;", // KEYS[1]，KEYS[2] // 锁的name，发布订阅的Channel Arrays.&lt;Object&gt;asList(getName(), getChannelName()), // ARGV[1] ~ ARGV[3] // 解锁消息，释放时间，当前线程唯一标识 LockPubSub.UNLOCK_MESSAGE, internalLockLeaseTime, getLockName(threadId));&#125; 发布解锁消息后，会调用到LockPubSub 的 onMessage，释放信号量，唤醒等待锁的线程1234567891011121314151617181920212223242526272829303132333435363738public class LockPubSub extends PublishSubscribe&lt;RedissonLockEntry&gt; &#123; public static final Long UNLOCK_MESSAGE = 0L; public static final Long READ_UNLOCK_MESSAGE = 1L; public LockPubSub(PublishSubscribeService service) &#123; super(service); &#125; @Override protected RedissonLockEntry createEntry(RPromise&lt;RedissonLockEntry&gt; newPromise) &#123; return new RedissonLockEntry(newPromise); &#125; @Override protected void onMessage(RedissonLockEntry value, Long message) &#123; if (message.equals(UNLOCK_MESSAGE)) &#123; Runnable runnableToExecute = value.getListeners().poll(); if (runnableToExecute != null) &#123; runnableToExecute.run(); &#125; // 释放信号量 value.getLatch().release(); &#125; else if (message.equals(READ_UNLOCK_MESSAGE)) &#123; while (true) &#123; Runnable runnableToExecute = value.getListeners().poll(); if (runnableToExecute == null) &#123; break; &#125; runnableToExecute.run(); &#125; value.getLatch().release(value.getLatch().getQueueLength()); &#125; &#125;&#125; 参考 慢谈 Redis 实现分布式锁 以及 Redisson 源码解析 https://www.programcreek.com/java-api-examples/?code=rollenholt-SourceReading/redisson/redisson-master/src/main/java/org/redisson/RedissonLock.java 欢迎点赞、转发。你的支持就是对我最大的帮助]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>锁</tag>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>源码</tag>
        <tag>Redisson</tag>
        <tag>ReentrantLock</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL优化实战--索引篇]]></title>
    <url>%2Fpost%2F1e0091db.html</url>
    <content type="text"><![CDATA[关于SQL优化，这个问题，相信大家过多过少都有过一些了解。最近我也在研究SQL优化方面的东西，分享一些经验。 首先简单介绍下索引，“索引” 是SQL优化中很重要的一部分（但是索引并不是优化的唯一选项） 索引原理简述如何理解索引？索引其实就是一种数据结构，用于快速定位和访问数据库中的数据。 通常来说索引使用的数据结构是 B-Tree / B+Tree。以B-Tree为例，假设每个节点存储100个Key，三层的B-Tree 可存储一百万数据，如果将根节点存入内存中的话，只需要读取两次磁盘就可以从100万数据中找到指定数据 关于B-Tree 推荐阅读这篇 https://www.geeksforgeeks.org/introduction-of-b-tree-2/ 包含B-Tree的查询，新增，删除操作如何实现 MySQL 执行计划SQL优化中查看执行计划是必不可少的一项，通过 explain 关键字可以查看MySQL中的执行计划 注：\G 含义是纵向显示结果 如果之前没有了解的 EXPLAIN 的同学，看到这个列表肯定是一脸懵逼。没关系我们先来挑几个重要的属性认识一下。 type：ALL 代表全表扫描 key：代表使用的索引，NULL 代表没有使用索引 rows：扫描行数 关于explain 再扩展一下，先执行 explain extended ...; ，再执行 SHOW WARNINGS 可以看到MySQL优化器对我们的SQL做了什么优化。如下图所示 利用索引来优化SQL 使用索引的优点：减少服务器扫描的数据量、避免排序和临时表、将随机I/O变为顺序I/O 通过下图，我们可以看到，添加了索引之后扫描行数从三十万行降到了1，性能提升可想而知 生产环境要注意，创建索引是一个非常耗时的操作，并且会阻塞其他操作。 生产环境添加索引有没有什么完美方案？有的，如果你的MySQL使用主从策略的时候，可以像Nginx不停机升级web服务那样，先移除一个节点为该节点执行 ALTER TABLE 操作，然后巴拉巴拉，因为具体我也没操作过就不细说了，感兴趣大家可以Google一下，动手尝试一下。如果是单机部署的话，只能用户少的时候在执行这种操作了 使用索引连接表索引也可以提高表连接的性能，下面是个例子，用户表左连订单表，对user_id 添加索引的前后对比 like优化 通过上述例子，我们可以看出，如果模糊查询时以%开头的话，MySQL无法使用索引，但是通常来说模糊查询时我们的匹配方式都会是 %xxx%，那么如何优化呢？ 这里可以通过存“反值”的方式巧妙的解决这个问题，例如我现在在数据库加一列 reverse_order_no 存储订单号的反值（并添加索引），匹配的时候再通过 REVERSE(‘%910’) 函数将参数取反。 这里也可以使用 or，如下图，查看执行计划会发现Extra 属性返回 &quot;Using sort_union(order_no,reverse_order_no); Using where&quot; 这里代表MySQL发生了索引合并，后文我们会讲到 排序以及多列索引排序需要加索引！相信大家可能知道这个道理，但是如下图所示，user_id 和 addtime 两列都建立了索引，那么下面这条查询排序使用索引了吗？ 答案是：并没有！为什么？注意 Extra 中的 using filesort，代表MySQL 使用了内部文件排序算法对结果集进行了排序。MySQL 通常在一个表上只选择一个索引（有例外的情况），这种情况如果我们希望排序使用索引的话，可以建立一个多列索引，如下图所示 而且多列索引最左边的列，可以当作单列索引来使用 MySQL 优化器特性我们刚刚说过 MySQL 通常在一个表上只选择一个索引，如何理解？例如索引A和索引B 一个需要扫描十万行，一个需要扫描五万行，那么MySQL一定选择开销最小的索引方式。 在一些特殊情况下，MySQL 会选择 Index Merge（索引合并），即在一个表上使用多个索引 Union：两个基数很高的索引执行OR操作时 Sort-Union：与上述类似，一旦or的左右两边出现范围查询，会使用该算法，区别是Sort-Union会进行排序 intersect：针对唯一值不多的索引列，例如在 is_pay（0-未支付，1-支付），is_send（0-未发货，1-发货） 两列建立索引，查询已支付并且未发货的订单，如下图所示 根据MySQL 5.7开发文档所示，还有一种会使用intersect，InnoDB 主键上的任何范围搜索 关于Index Merge的更多信息，参考MySQL开发文档https://dev.mysql.com/doc/refman/5.7/en/index-merge-optimization.html 索引的影响添加索引虽然可以提升我们的SQL性能，但是随之而来也会带来一定的开销 数据插入和更新的性能，因为需要构建索引的原因，在数据量大的时候会比较明显，下图是 《Effective MySQL之SQL语句最优化》中对添加索引前后的插入性能对比 磁盘空间的影响，同样也是来自于书中的测试 可以看到在添加了索引之后，空间占用是原来的7倍，在数据量庞大时，这是一个需要关注的点。 还有需要注意的一点是，在MySQL Innodb 中有聚簇索引和二级索引，一般来说主键就是聚簇索引，而其他的索引都是二级索引。 二级索引所存储的值是聚簇索引。所以当使用二级索引来进行检索时，MySQL 会先通过该索引找到对应的聚簇索引，再通过该聚簇索引找到对应的数据。这时使用占用字节更小的类型来做主键会更好，会节省索引占用空间 参考 Effective MySQL之SQL语句最优化]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>索引</tag>
        <tag>sql优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security实战(二) 动态权限]]></title>
    <url>%2Fpost%2Fb718a659.html</url>
    <content type="text"><![CDATA[好消息好消息！Security系列终于有了第二期，最近在看项目源码忍不住又搞起来Spring Security，来给大家分享一下，虽然和上一节说好的内容不同🤭 回顾上节我们介绍了如何进行简单的权限配置，包括url权限和方法权限，还有如何授予用户权限。12345678910111213141516171819protected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers("/", "/home").permitAll() // 测试配置URL权限 .antMatchers("/match/**").hasAuthority("sys:match") // 对某URL添加多个权限，可以多次配置 .antMatchers("/match/**").hasAuthority("sys:mm") .anyRequest().authenticated() .and() .formLogin() .loginPage("/login") .permitAll() .and() .logout() .permitAll() .and() ;&#125; 但是如果现在你的业务系统要求动态权限呢？ 比如用户权限变更了，我们可以重新构建Security上下文中Authentication 对象，这还好说。如果说某个接口的权限修改了，如果按照上述的方法来做的话，是不可能实现动态修改的。 本节我们来介绍一下Spring Security 如何实现动态权限 实现原理FilterSecurityInterceptor 负责 Security中的权限控制，其核心代码在父类AbstractSecurityInterceptor中，我们来看一下 这里我删了一些与核心逻辑无关的代码，我们只需要关注红框里的内容 这时候聪明的你应该已经明白了FilterSecurityInterceptor是如何管理权限的，我们完全可以自己实现上面的AccessDecisionManager和SecurityMetadataSource来实现我们的动态权限 但是先别急，先看看AccessDecisionManager的默认实现AffirmativeBased 这代码写的有意思了，通过遍历所有的Voter，每个Voter实现具体的判断逻辑，返回 1，0，-1（分别代表同意、弃权、拒绝），当存在拒绝时直接抛出AccessDeniedException 非常的民主，我们只需要实现一个Voter即可 注：AccessDecisionManager 还有其他的默认实现，感兴趣的同学可以自行查看源码 CodingOK，首先我们先捋一下思路 实现SecurityMetadataSource，提供当前资源要求的权限 实现AccessDecisionVoter，用于判断当前用户是否有权限访问 将我们自己的实现注册到FilterSecurityInterceptor中 OK，可以开搞了 SecurityService实现一个Service，用于从数据库加载数据1234567891011121314151617181920212223242526@Servicepublic class SecurityService &#123; @Autowired private JdbcTemplate jdbcTemplate; /** * 加载资源要求的权限 * * @param resource * @return */ public List&lt;String&gt; getPermByResource(String resource) &#123; return jdbcTemplate.queryForList(Sql.getPermByResource, String.class, resource); &#125; /** * 当前用户的权限 * * @param username * @return */ public List&lt;String&gt; getPermByUsername(String username) &#123; return jdbcTemplate.queryForList(Sql.getPermByUsername, String.class, username); &#125;&#125; 注：这里两个方法都可以加上缓存，由于demo演示，我就没有这么做 实现SecurityMetadataSourceMySecurityMetadataSource 很简单，就是通过SecurityService加载一下数据12345678910111213141516171819202122232425262728public class MySecurityMetadataSource implements FilterInvocationSecurityMetadataSource &#123; private SecurityService securityService; public MySecurityMetadataSource(SecurityService securityService) &#123; this.securityService = securityService; &#125; @Override public Collection&lt;ConfigAttribute&gt; getAttributes(Object object) throws IllegalArgumentException &#123; String uri = ((FilterInvocation) object).getHttpRequest().getRequestURI(); List&lt;String&gt; list = securityService.getPermByResource(uri); if (list != null &amp;&amp; list.size() != 0) &#123; return SecurityConfig.createList(list.toArray(new String[0])); &#125; return null; &#125; @Override public Collection&lt;ConfigAttribute&gt; getAllConfigAttributes() &#123; return null; &#125; @Override public boolean supports(Class&lt;?&gt; clazz) &#123; return FilterInvocation.class.isAssignableFrom(clazz); &#125;&#125; 实现AccessDecisionVoter这里加载一下当前用户的权限，判断用户是否满足当前资源所要求的权限12345678910111213141516171819202122232425262728293031323334public class MyAccessDecisionVoter implements AccessDecisionVoter&lt;Object&gt; &#123; private SecurityService securityService; public MyAccessDecisionVoter(SecurityService securityService) &#123; this.securityService = securityService; &#125; @Override public boolean supports(ConfigAttribute attribute) &#123; return true; &#125; @Override public boolean supports(Class&lt;?&gt; clazz) &#123; return true; &#125; @Override public int vote(Authentication authentication, Object object, Collection&lt;ConfigAttribute&gt; attributes) &#123; Object principal = authentication.getPrincipal(); if ("anonymousUser".equals(principal)) &#123; // 当前用户未登录，如果不要求权限-&gt;允许访问，否则拒绝访问 return CollectionUtils.isEmpty(attributes) ? ACCESS_GRANTED : ACCESS_DENIED; &#125; else &#123; // 这里我的逻辑是，当前资源的要求权限，用户必须全部满足时才可以访问 User user = (User) principal; List&lt;String&gt; permitList = securityService.getPermByUsername(user.getUsername()); List&lt;String&gt; stringAttributes = attributes.stream().map(ConfigAttribute::getAttribute).collect(Collectors.toList()); return permitList.containsAll(stringAttributes) ? ACCESS_GRANTED : ACCESS_DENIED; &#125; &#125;&#125; 注册到FilterSecurityInterceptor我们核心的业务已经实现完了，现在需要把MySecurityMetadataSource和MyAccessDecisionVoter注册到FilterSecurityInterceptor中 需要注意的是，FilterSecurityInterceptor并不可以通过@Bean的方式来声明，该对象是在WebSecurityConfigurerAdapter的初始化方法中默认创建的 但是Spring Security为我们提供了ObjectPostProcessor，用于解决上述问题，具体用法如下12345678910111213http .authorizeRequests() .antMatchers("/", "/home", "/403").permitAll() .anyRequest().authenticated() .withObjectPostProcessor(new ObjectPostProcessor&lt;FilterSecurityInterceptor&gt;() &#123; @Override public &lt;O extends FilterSecurityInterceptor&gt; O postProcess( O fsi) &#123; fsi.setSecurityMetadataSource(new MySecurityMetadataSource(securityService)); fsi.setAccessDecisionManager(new AffirmativeBased(getDecisionVoters())); return fsi; &#125; &#125;) 完整DemoGithub 👉 https://github.com/TavenYin/security-example/tree/master/dynamic-permissions]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Spring Cloud 分布式配置中心]]></title>
    <url>%2Fpost%2Fbca00fad.html</url>
    <content type="text"><![CDATA[Spring Cloud Config 能做什么？我们可以将分布式系统的配置文件托管在Git仓库或者数据库中，Config Server 负责管理配置文件，以Restful的形式提供给其他服务，可以在任何其他语言的应用程序中使用，不依赖Spring Cloud全家桶。 本节目标使用Spring Cloud 基于Git仓库搭建分布式配置中心 版本Spring Cloud Greenwich.SR2Spring Boot 2.1.7.RELEASE Git仓库在你喜欢的Git平台上建立一个仓库，创建一个目录，并建立几个配置文件（建议网络较慢的同学选择Gitee） 搭建Config Serverpom.xml1234567891011121314151617181920212223&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; ...&lt;/dependencies&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Greenwich.SR2&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; application.properties12345678910server.port = 9001spring.application.name = config-server#表示配置中心所在仓库的位置spring.cloud.config.server.git.uri = https://gitee.com/yintianwen7/cloud-config.git#仓库路径下的的相对搜索位置，可以配置多个spring.cloud.config.server.git.search-paths = demo#git的用户名spring.cloud.config.server.git.username = yourusername#git的密码spring.cloud.config.server.git.password = yourpassword ConfigServerApplication.java123456789@EnableConfigServer@SpringBootApplicationpublic class ConfigServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigServerApplication.class, args); &#125;&#125; 至此，ConfigServer 搭建完成，我们可以访问ConfigServer 查看 cloud-config/demo 下的配置文件 访问方式如下: /{application}/{profile}[/{label}] /{application}-{profile}.yml /{label}/{application}-{profile}.yml /{application}-{profile}.properties /{label}/{application}-{profile}.properties 这里 label 为git 分支，默认master 例如访问 application-dev.yml，直接请求 localhost:9001/application/dev即可 可以尝试修改一下Git仓库中的配置文件，再访问Config Server，这时你会发现Config Server中的数据是实时的。 搭建 Client 应用访问Config Serverpom.xml1234567891011121314151617181920212223&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; ...&lt;/dependencies&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Greenwich.SR2&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; bootstrap.properties，注意这里是 bootstrap.properties，不然你会发现你请求的URI一直是localhost:888812345spring.profiles.active=simplespring.application.name=applicationspring.cloud.config.label=master# 拼接规则: uri/name/profile/labelspring.cloud.config.uri=http://localhost:9001 CloudConfig.java 配置文件的映射实体，同理也可以@Value 或者 Environment 对象读取属性12345678@ConfigurationProperties("cloud.config")public class CloudConfig &#123; private String a; private String b; private String c; // 省略 get set&#125; ClientApplication.java123456789101112131415161718@RestController@SpringBootApplication@EnableConfigurationProperties(CloudConfig.class)public class ClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ClientApplication.class, args); &#125; @Autowired private CloudConfig cloudConfig; // 刷新配置时 POST /actuator/refresh @GetMapping("config") public Object config() &#123; return cloudConfig; &#125;&#125; 启动 Client，查看日志，可以看到请求配置中心的URL 请求 localhost:8080/config ，验证配置文件是否读取成功 刷新客户端配置Client 中读取的配置文件，并不是实时的，我们可以通过修改Git仓库中的文件来验证这点。那么如何刷新配置呢？ pom.xml 引入1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; bootstrap.properties 追加1management.endpoints.web.exposure.include=* 对需要刷新的Bean，添加注解@RefreshScope1234@RefreshScope@ConfigurationProperties("cloud.config")public class CloudConfig &#123;&#125; POST /actuator/refresh，即可刷新配置 Config Server 加密解密允许数据以加密形式存储在Git仓库，配置中心负责对加密的数据进行解密，然后提供给客户端应用。由于篇幅问题，这里不讲了，感兴趣的小伙伴参考 Spring Cloud构建微服务架构：分布式配置中心（加密解密） 配置中心高可用方案1：使用传统负载均衡器 方案2：将client、config server 注册到Spring Cloud注册中心，通过注册中心访问配置中心，具体代码参考👇 Demo本文demo👉：Github， Gitee 参考http://blog.didispace.com/spring-cloud-starter-dalston-3-2/https://segmentfault.com/a/1190000012908853]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
        <tag>分布式配置中心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ 集群搭建]]></title>
    <url>%2Fpost%2F4c62e394.html</url>
    <content type="text"><![CDATA[本文记录RabbitMQ集群搭建过程中遇到的问题 环境 vmware12 虚拟机，CentOS7本文以两台CentOS 为例，IP 192.168.32.128，192.168.32.129 磁盘节点和内存节点集群中，每一个RabbitMQ实例都是一个节点，而节点分为磁盘节点和内存节点 内存节点：将Rabbit中的元数据（Queue, Exchange, binding, vhost等）存储在内存中，持久化的 Message 依旧保存在磁盘中，内存节点的性能只能体现在资源管理上，消息的发送和接收和磁盘节点没有区别 磁盘节点：元数据存储在磁盘中，一个Rabbit集群要求至少有一个磁盘节点，因为内存节点中不存储元数据，所以每次内存节点启动，都会从其他节点中同步元数据 另外如果唯一磁盘的磁盘节点崩溃了，不能进行如下操作： 不能创建队列 不能创建交换器 不能创建绑定 不能添加用户 不能更改权限 不能添加和删除集群几点 RabbitMQ集群的几种类型 单一模式：仅有一个rabbit实例 普通模式：默认集群模式，每个节点各自维护自己的数据，两个节点仅存有相同的元数据。例如RabbitA 和 RabbitB，A中存在 QueueA，消费者可以从RabbitB实例中，读取QueueA的消息，这时RabbitB会从A中读取消息，返回给消费者。但是如果RabbitA 宕机，这时就无法获取QueueA的数据了 镜像模式：Rabbit 会将数据同步到其他节点中，这固然提高了可用性，但是随之而来的问题是，系统的性能会降低。节点之间消息的传递会占用带宽，而每个节点存储的数据量会变大 普通模式我们先来搭建普通模式的集群，在RabbitMQ中，每个节点的名必须是唯一的，默认以 rabbit@hostname为节点name 而每台虚拟机中默认的host是localhost，这就导致了每个节点的name都是 rabbit@localhost 1. 修改hosts修改两台主机的hosts文件，以下是129的配置，我们把128定义为F，129定义为G 123456cat /etc/hosts127.0.0.1 G::1 G192.168.32.129 G192.168.32.128 F 保证F和G可以ping通 2. 安装RabbitMQ可以参考 https://www.linuxprobe.com/install-rabbitmq-on-centos-7.html 当时安装完一直发现无法访问 Rabbit的web控制台，除了开启插件，和添加一个用户之外，还需要放开防火墙的端口12345678firewall-cmd --add-port=4369/tcp --permanentfirewall-cmd --add-port=25672/tcp --permanentfirewall-cmd --add-port=5671-5672/tcp --permanentfirewall-cmd --add-port=15672/tcp --permanentfirewall-cmd --add-port=61613-61614/tcp --permanentfirewall-cmd --add-port=1883/tcp --permanentfirewall-cmd --add-port=8883/tcp --permanentfirewall-cmd --reload # 重载配置 3. 同步Erlang cookieErlang VM将尝试在RabbitMQ服务器启动时创建一个随机生成的值（Erlang Cookie）。集群环境中，所有节点的Erlang Cookie必须一致。 .erlang.cookie 通常在 $HOME下或者/var/lib/rabbitmq下1234567# 找到其中一台主机的 .erlang.cookie# 修改权限chmod 777 /var/lib/rabbitmq/.erlang.cookie# 拷贝到另一台主机scp /var/lib/rabbitmq/.erlang.cookie G:/var/lib/rabbitmq/# 再把权限修改回来chmod 400 /var/lib/rabbitmq/.erlang.cookie 4. 检查节点名 如果两个节点都是 rabbit@localhost，这是无法建立集群的 我当时就是遇到了这个问题，需要修改节点name，参考 https://ubuntuqa.com/article/6619.html 我采取的方法：1234567vi /etc/rabbitmq/rabbitmq-env.conf# 添加如下配置NODENAME=rabbit@G# 保存后，重启rabbitmq 生效service rabbitmq-server restart 5. 组成集群上述步骤都成功后，我们可以组成集群了 这个时候我们先启动rabbitmq@F，然后rabbit@G执行以下操作，加入F，组成集群1234rabbitmqctl stop_app # 停止rabbitmq服务rabbitmqctl reset # 清空节点状态rabbitmqctl join_cluster rabbit@F # 加入F，组成集群rabbitmqctl start_app 集群中，任意节点停机后，执行 rabbitmqctl start_app 即可再次加入集群 查看集群状态 rabbitmqctl cluster_status 镜像模式任意rabbit节点输入命令，将所有队列，同步到所有节点中1rabbitmqctl set_policy ha-all "^" '&#123;"ha-mode":"all"&#125;' 12345678910111213rabbitmqctl set_policy [-p Vhost] Name Pattern Definition [Priority]-p Vhost： 可选参数，针对指定vhost下的queue进行设置Name: policy的名称Pattern: queue的匹配模式(正则表达式)Definition：镜像定义，包括三个部分ha-mode, ha-params, ha-sync-mode ha-mode:指明镜像队列的模式，有效值为 all/exactly/nodes all：表示在集群中所有的节点上进行镜像 exactly：表示在指定个数的节点上进行镜像，节点的个数由ha-params指定 nodes：表示在指定的节点上进行镜像，节点名称通过ha-params指定 ha-params：ha-mode模式需要用到的参数 ha-sync-mode：进行队列中消息的同步方式，有效值为automatic和manualpriority：可选参数，policy的优先级 关于镜像模式策略的更多请参考官方文档：https://www.rabbitmq.com/ha.html 集群的负载均衡为什么有了集群还需要负载均衡？ 这里我纠结了好几天，原因是，在我的理解里 集群 == 负载均衡，这样的理解是有问题的。正解：分布式集群保证的是高可用，而并不是负载均衡。 rabbitmq文档中，也建议我们使用TCP负载均衡器，这样也不需要在我们应用程序里管理集群的地址 Connecting to Clusters from ClientsA client can connect as normal to any node within a cluster. If that node should fail, and the rest of the cluster survives, then the client should notice the closed connection, and should be able to reconnect to some surviving member of the cluster. Generally, it’s not advisable to bake in node hostnames or IP addresses into client applications: this introduces inflexibility and will require client applications to be edited, recompiled and redeployed should the configuration of the cluster change or the number of nodes in the cluster change. Instead, we recommend a more abstracted approach: this could be a dynamic DNS service which has a very short TTL configuration, or a plain TCP load balancer, or some sort of mobile IP achieved with pacemaker or similar technologies. In general, this aspect of managing the connection to nodes within a cluster is beyond the scope of RabbitMQ itself, and we recommend the use of other technologies designed specifically to solve these problems. 网上比较多的方案是使用 HAProxy 作为负载均衡器，这里大家可以参考一下这一篇 HAProxy从零开始到掌握 具体安装配置的细节本文就不说了 这里我又添加了一台 130 的虚拟机来跑 HAProxy 贴一下我的 haproxy.cfg，仅供参考 1234567891011121314151617181920212223242526272829globaldaemonpidfile /home/ha/haproxy/conf/haproxy.pidlog 127.0.0.1 local2defaultsmode tcpmaxconn 10000timeout connect 5stimeout client 100stimeout server 100sfrontend http-in bind *:5670 maxconn 30000 default_backend default_serversbackend default_servers balance roundrobin server F 192.168.32.128:5672 check inter 2000 rise 2 fall 3 weight 1 server G 192.168.32.129:5672 check inter 2000 rise 2 fall 3 weight 1listen stats bind *:1936 mode http stats refresh 30s #每30秒更新监控数据 stats uri /stats #访问监控页面的uri stats realm HAProxy\ Stats #监控页面的认证提示 stats auth admin:admin 做完负载均衡之后，可以跑一下程序，查看一下负载均衡的效果，这里我的10个 Connection 按照权重 1:1 分配在了两台 Rabbit 上 参考 https://www.cnblogs.com/knowledgesea/p/6535766.htmlhttps://www.rabbitmq.com/clustering.html]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
        <tag>集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ 理解 Exchange]]></title>
    <url>%2Fpost%2Fea1358e6.html</url>
    <content type="text"><![CDATA[本节目标 了解 AMQP 模型 了解 Exchange 建议大家在学习MQ之前，先了解一下 生产者消费者模型，可以参考我之前的一篇 Java 理解生产者-消费者设计模式 AMQP 简介RabbitMQ 是实现了 AMQP 协议的消息中间件，所以说下面讲到这些概念不仅限于RabbitMQ，所有实现AMQP 协议的消息中间件都具备这些 AMQP 模型图如下 Publisher（发布者）和 Consumer（消费者）可以理解为是我们的应用程序 Exchange（交换机），我们的Publisher 会将消息先推送到Exchange ，Exchange 类似邮局，他会将消息再路由到 Queue（队列）中，Queue会将消息推送给Consumer Exchange 和 Queue 之间通过 Binding （绑定）将两者关联到一起，一个Exchange 可以绑定多个Queue，一个Queue 可以绑定多个Exchange Exchange刚刚我们说Exchange类似邮局。邮局的话，当然会有很多的邮递方式，所以我们的Exchange有多种类型，每种类型都有自己的策略 AMQP 提供四种Exchange类型 name 默认预设名 Direct exchange (Empty string) and amq.direct Fanout exchange amq.fanout Topic exchange amq.topic Headers exchange amq.match (and amq.headers in RabbitMQ) Direct ExchangeRabbit 文档中给出的定义翻译过来是这样的 Direct Exchange基于RoutingKey 将消息传递到队列 通俗点讲是啥意思呢，直接看代码吧 12// 第一个参数为Exchange，第二个参数就是RoutingKey rabbitTemplate.convertAndSend("user_exchange", "user_routingKey", message); 我们再来看绑定的时候，也会关联一个RoutingKey12// 绑定的时候 也会关联一个RoutingKeyBindingBuilder.bind(queue()).to(userExchange()).with("user_routingKey") 千言万语都在图里 Default ExchangeDefault Exchange 其实是AMQP中预先声明的，并且它属于 Direct Exchange，通常来说每个Exchange都会有自己的名，Default Exchange 的名是 &quot;&quot;. 他有一个特殊的属性，当你手动创建一个队列时，MQ会自动将这个队列绑定到Default Exchange 上，绑定时 RoutingKey 与队列名称相同 说的好像挺绕的，来看一下代码吧12345// 声明一个 Queue，这个时候我们不主动Binding的话，test_queue 会和Default Exchange 绑定，此时 routingKey = test_queueQueue queue = new Queue("test_queue");// 向默认交换机发布消息rabbitTemplate.convertAndSend("", "test_queue", message); Fanout ExchangeFanout Exchange 很简单，适合发布订阅场景。他不使用RoutingKey，他将消息路由到所有与其绑定的队列 Topic ExchangeTopic Exchange 又是基于RoutingKey来路由转发的，但是有些不同，上图 在使用 Topic 的时候，routingKey 可以是 *.orange.*,lazy.# 这种表达式 * (星) 代表一个词. # (hash) 代表0或多个词. 同理，当发布消息时的 routingKey 与其匹配时，会路由到相应的队列 Headers Exchange基于Header中的属性来匹配路由，由于不是很常用，这里就不说了 参考 https://www.rabbitmq.com/tutorials/amqp-concepts.html]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
        <tag>Exchange</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 源码解析 同步事务的实现原理]]></title>
    <url>%2Fpost%2Fff158ca0.html</url>
    <content type="text"><![CDATA[在最开始学习Spring AOP的时候，那时候只知道事务是利用AOP管理的，但是并没有翻看过源码，后来发现Spring不光可以管理关系型数据的事务。甚至可以同步管理Redis等其他数据库的事务，这是怎么做到的呢？本节我们一起探索一下Spring的事务 版本SpringBoot 2.1.6.RELEASE 分析首先如果我们自己写一个AOP来管理事务的话，会怎么做？ 执行代理方法前开启事务 执行代理的方法 执行代理方法后回滚或者提交事务 如何实现Redis事务的同步提交和回滚？ Spring中关于事务的管理要比我们想象的复杂的多，本文只关注同步事务的实现，下面我们来看源码 TransactionAspectSupport在翻看源码后找到这个类，事务切面支持，这个类实现了事务管理的核心逻辑，我们来看一下核心代码 我们通常在项目中会使用默认的DataSourceTransactionManager事务管理器 + 注解式事务（也就是声明式事务）CallbackPreferringPlatformTransactionManager 是Spring提供的回调式事务管理器（用于编程式事务），这里我们不讨论。 可以看到声明式事务的处理逻辑，和我们上述分析的基本一致，那么他是在什么时候同步处理了其他数据的事务呢？继续深入源码 源码跟踪我们以completeTransactionAfterThrowing() 这个方法为例（上图红框框），跟踪下去 可以看到，当我们异常需要回滚时，会调用 TransactionManager.rollback()，我们继续来跟踪这个方法 这里代码逻辑还是比较多的，我们只关注红框里的代码块，doRollback中只做了数据源的回滚，那么像Redis事务的提交是在哪里实现的？ 我们来看一下 triggerAfterCompletion() 方法，答案马上就知道了 同步事务 TransactionSynchronizationManager 中会从ThreadLocal 中获取出当前线程中 “同步事务”接口集合 List，然后接下来的操作就是遍历所有的TransactionSynchronization，执行synchronization.afterCompletion(completionStatus); 我们可以在TransactionSynchronization接口下找到一个Redis的实现 代码逻辑很简单，事务提交则Redis事务执行，否则取消Redis事务 同步事务的注册 我们RedisTemplate开启事务后，会执行到上图的debug处，RedisConnection 开启事务，并将当前Connection注册到TransactionSynchronizationManager 中，这样在triggerAfterCompletion 就可以同步的管理我们Redis的事务了]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>Spring</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解消息队列 - 使用场景简介]]></title>
    <url>%2Fpost%2F81e2e5c7.html</url>
    <content type="text"><![CDATA[在之前的 Java 理解生产者-消费者设计模式 一文中，我们学习了生产者-消费者模式，生产者和消费者之间通过一个缓冲队列进行通讯，即做到了异步消息吞吐，又使得程序解耦 可以说，使用 MQ（消息队列） 可以在分布式环境中实现生产者-消费者模式 适合MQ使用的场景 消息的发送者和消费者需要解耦的情况 异步处理 多个消费者（消息的关注者不止一个） 消费者的处理结果不返回给发送者 （以RabbitMQ为例，虽然可以做到RPC调用，但是这种调用会带来更多的麻烦） 削峰填谷 下面我跟详细分析一下各种使用场景 解耦 其实很好理解，因为生产者和消费者通过MQ进行通讯，可以说两者没有什么直接的瓜葛，必然解耦 异步 &amp;&amp; 多个消费者生产者只负责把消息发布，并在意谁处理消息，可能会有多个消费者处理该消息 例如用户注册，我们会给用户发送注册邮件和短信（可以把邮件和短信理解为多个消费者服务） 可以看到在使用了MQ，即可以通过异步处理的方式，降低响应时间，又可以降低业务之间的耦合度 消费者的处理结果不返回给发送者线程池和MQ都可以看成 生产者-消费者模式的实现，而这两者也都可以获得消费者的返回值。但是如果你这样做的话，你的生产者线程会阻塞（等待消费者的返回），没法做到完全异步的处理，有些违背了生产者-消费者模式的初衷。而且还要考虑等待超时后的处理。但是不能说这种用法是不对的，具体业务具体分析 削峰填谷何为削峰填谷，可以理解为流量控制。例如在抢票，秒杀等业务场景时，可能会突然有大量的请求涌入。但是我们的服务器资源有限，并不能处理这么多的请求，此时可以使用 MQ进行流量控制 因为，请求都缓存在了MQ中，下游服务可以根据自己的速度进行处理。 参考 https://www.kancloud.cn/cosmicyang/rabbitmq/824050]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>消息队列</tag>
        <tag>生产者消费者</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 理解 ThreadPoolExecutor 实现原理]]></title>
    <url>%2Fpost%2Fa5233273.html</url>
    <content type="text"><![CDATA[使用线程池（ThreadPoolExecutor）的好处是减少在创建和销毁线程上所花的时间以及系统资源的开销，解决资源不足的问题。如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题。 – 阿里Java开发手册 版本JDK 1.8 本节目标 理解线程池核心参数 理解线程池工作原理 理解线程池核心方法 线程池的核心参数和构造方法ctl12345678910111213141516171819202122232425262728293031323334353637// 线程池核心变量，包含线程池的运行状态和有效线程数，利用二进制的位掩码实现private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bits// 线程池状态private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// Packing and unpacking ctl// 获取当前线程池运行状态private static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;// 获取当前线程池有效线程数private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;// 打包ctl变量private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125;/* * Bit field accessors that don't require unpacking ctl. * These depend on the bit layout and on workerCount being never negative. */private static boolean runStateLessThan(int c, int s) &#123; return c &lt; s;&#125;private static boolean runStateAtLeast(int c, int s) &#123; return c &gt;= s;&#125;private static boolean isRunning(int c) &#123; return c &lt; SHUTDOWN;&#125; JDK7 以后，线程池的状态和有效线程数通过 ctl 这个变量表示（使用二进制的位掩码来实现，这里我们不深究），理解上述几个方法作用即可，不影响下面的源码阅读 关于线程池的五种状态 RUNNING：接受新任务并处理队列中的任务 SHUTDOWN ：不接受新任务，但处理队列中的任务 STOP ：不接受新任务，不处理队列中的任务，并中断正在进行的任务（中断并不是强制的，只是修改了Thread的状态，是否中断取决于Runnable 的实现逻辑） TIDYING ：所有任务都已终止，workerCount为0时，线程池会过度到该状态，并即将调用 terminate() TERMINATED ：terminated() 调用完成；线程池中止 线程池状态的转换 RUNNING =&gt; SHUTDOWN ：调用 shutdown() RUNNING / SHUTDOWN =&gt; STOP ：调用 shutdownNow() （该方法会返回队列中未执行的任务） SHUTDOWN =&gt; TIDYING： 当线程池和队列都为空时 STOP =&gt; TIDYING：当线程池为空时 TIDYING =&gt; TERMINATED：当 terminated() 调用完成时 构造方法线程池最终都是调用如下构造方法123456789public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; // 省略&#125; 核心参数我们来看一下线程池中的核心参数都是什么作用123456789101112131415161718192021222324private final BlockingQueue&lt;Runnable&gt; workQueue; // 阻塞队列，用于缓存任务private final ReentrantLock mainLock = new ReentrantLock(); // 线程池主锁private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;(); // 工作线程集合private final Condition termination = mainLock.newCondition(); // awaitTermination() 方法的等待条件private int largestPoolSize; // 记录最大线程池大小private long completedTaskCount; //用来记录线程池中曾经出现过的最大线程数private volatile ThreadFactory threadFactory; // 线程工厂，用于创建线程private volatile RejectedExecutionHandler handler; // 任务拒绝时的策略private volatile long keepAliveTime; // 线程存活时间 // 当线程数超过核心池数时，或允许核心池线程超时，该参数会起作用。否则一直会等待新的任务private volatile boolean allowCoreThreadTimeOut; // 是否允许核心池线程超时private volatile int corePoolSize; // 核心线程池数量private volatile int maximumPoolSize; // 最大线程池数量 workQueue这个队列的作用，和之前的 Java 理解生产者-消费者设计模式 中讲到的缓冲队列，作用很相似，或者说线程池就是生产者消费者模式的一种实现。 关于 handler ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy：当前任务自己决定 corePoolSize 和 maximumPoolSize如果你对这两个参数有疑问，看完下面的栗子你会清晰很多 下面我们来举个栗子来更好的理解一下线程池 理解线程池工作原理假如有一个工厂，工厂里面有10个工人，每个工人同时只能做一件任务。 因此只要当10个工人中有工人是空闲的，来了任务就分配给空闲的工人做； 当10个工人都有任务在做时，如果还来了任务，就把任务进行排队等待； 每个工人做完自己的任务后，会去任务队列中领取新的任务； 如果说新任务数目增长的速度远远大于工人做任务的速度（任务累积过多时），那么此时工厂主管可能会想补救措施，比如重新招4个临时工人进来； 然后就将任务也分配给这4个临时工人做； 如果说着14个工人做任务的速度还是不够，此时工厂主管可能就要考虑不再接收新的任务或者抛弃前面的一些任务了。 当这14个工人当中有人空闲时，而新任务增长的速度又比较缓慢，工厂主管可能就考虑辞掉4个临时工了，只保持原来的10个工人，毕竟请额外的工人是要花钱的。 开始工厂的10个工人，就是 corePoolSize (核心池数量)； 当10个人都在工作时 (核心池达到 corePoolSize)，任务排队等待时，会缓存到 workQueue 中； 当任务累积过多时(达到 workQueue 最大值时)，找临时工； 14个临时工，就是 maximumPoolSize (数量)； 如果此时工作速度还是不够，线程池这时会考虑拒绝任务，具体由拒绝策略决定 理解线程池核心方法execute()线程池中所有执行任务的方法有关的方法，都会调用 execute()。如果你理解了上述的小例子，再来看这个会清晰很多123456789101112131415161718192021222324252627282930313233343536373839public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); /* * Proceed in 3 steps: * * 1. If fewer than corePoolSize threads are running, try to * start a new thread with the given command as its first * task. The call to addWorker atomically checks runState and * workerCount, and so prevents false alarms that would add * threads when it shouldn't, by returning false. * * 2. If a task can be successfully queued, then we still need * to double-check whether we should have added a thread * (because existing ones died since last checking) or that * the pool shut down since entry into this method. So we * recheck state and if necessary roll back the enqueuing if * stopped, or start a new thread if there are none. * * 3. If we cannot queue task, then we try to add a new * thread. If it fails, we know we are shut down or saturated * and so reject the task. */ int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; else if (!addWorker(command, false)) reject(command);&#125; 分析execute() step 1 1）首先检查当前有效线程数 是否小于 核心池数量if (workerCountOf(c) &lt; corePoolSize) 2）如果满足上述条件，则尝试向核心池添加一个工作线程 （addWorker() 第二个参数决定了是添加核心池，还是最大池）if (addWorker(command, true)) 3）如果成功则退出方法，否则将执行 step2 step 2 1）如果当前线程池处于运行状态 &amp;&amp; 尝试向缓冲队列添加任务if (isRunning(c) &amp;&amp; workQueue.offer(command)) 2）如果线程池正在运行并且缓冲队列添加任务成功，进行 double check（再次检查） 3）如果此时线程池非运行状态 =&gt; 移除队列 =&gt; 拒绝当前任务，退出方法（这么做是为了，当线程池不可用时及时回滚） 12if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); 4）如果当前有效线程数为0，则创建一个无任务的工作线程（此时这个线程会去队列中获取任务） step 3 1）当无法无法向核心池和队列中添加任务时，线程池会再尝试向最大池中添加一个工作线程，如果失败则拒绝该任务 12else if (!addWorker(command, false)) reject(command); 图解execute()根据上述的步骤画了如下的这个图，希望能帮助大家更好的理解 addWorker()在分析execute() 方法时，我们已经知道了 addWorker() 的作用了，可以向核心池或者最大池添加一个工作线程。我们来看一下这个方法都做了什么 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; 这个方法代码看似很复杂，没关系，我们一步一步来分析 step 1先看第一部分 12345678910111213141516171819202122232425retry:for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125;&#125; 这一部分代码，主要是判断，是否可以添加一个工作线程。 在execute()中已经判断过if (workerCountOf(c) &lt; corePoolSize)了，为什么还要再判断？ 因为在多线程环境中，当上下文切换到这里的时候，可能线程池已经关闭了，或者其他线程提交了任务，导致workerCountOf(c) &gt; corePoolSize 1）首先进入第一个无限for循环，获取ctl对象，获取当前线程的运行状态，然后判断12345if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; 这个判断的意义为，当线程池运行状态 &gt;= SHUTDOWN 时，向添加一个工作线程必须同时满足 rs == SHUTDOWN firstTask == null ! workQueue.isEmpty()三个条件，否则添加线程失败 所以当线程状态为SHUTDOWN时，线程池允许添加一个无任务的工作线程去执行队列中的任务。 2）进入第二个无限for循环 123456789101112for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop&#125; 获取当前有效线程数，if 有效线程数 &gt;= 容量 || 有效线程数 &gt;= 核心池数量/最大池数量，则return false; 添加线程失败 如果有效线程数在合理范围之内，尝试使用 CAS 自增有效线程数 （CAS 是Java中的乐观锁，不了解的小伙伴可以Google一下），乐观锁自增成功，代表当前无其他线程竞争，相当于获取到锁了 如果自增成功，break retry; 跳出这两个循环，执行下面的代码 自增失败，检查线程池状态，如果线程池状态发生变化，回到第一个for 继续执行；否则继续在第二个for 中； step 2下面这部分就比较简单了 1234567891011121314151617181920212223242526272829303132333435363738boolean workerStarted = false;boolean workerAdded = false;Worker w = null;try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125;&#125; finally &#123; if (! workerStarted) addWorkerFailed(w);&#125;return workerStarted; 1）创建工作线程对象Worker； 2）加锁，判断当前线程池状态是否允许启动线程；如果可以，将线程加入workers（这个变量在需要遍历所有工作线程时会用到），记录最大值，启动线程； 3）如果线程启动失败，执行addWorkerFailed（从workers中移除该对象，有效线程数减一，尝试中止线程池） WorkerWorker对象是线程池中的内部类，线程的复用、线程超时都是在这实现的 123456789private final class Worker extends AbstractQueuedSynchronizer implements Runnable&#123; // 这里我们只关心Run()，省略了其他源码，感兴趣的同学可以自己看一下源码 public void run() &#123; runWorker(this); &#125;&#125; Worker 实现了 Runnable，我们这里只关心 Worker 的run方法中做了什么，关于 AbstractQueuedSynchronizer 有关的不在本文讨论 下面我们分析一下runWorker()1234567891011121314151617181920212223242526272829303132333435363738394041424344final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; // 通过该变量判断是用户任务抛出异常结束，还是线程池自然结束 completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125;&#125; 1）123 while (task != null || (task = getTask()) != null) &#123;// ...&#125; 不对地通过getTask() 从队列中获取任务，可以间接通过getTask()的返回值控制线程的结束 2）123456789// If pool is stopping, ensure thread is interrupted;// if not, ensure thread is not interrupted. This// requires a recheck in second case to deal with// shutdownNow race while clearing interruptif ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); 接下来这个判断，其实我是没有太理解的，暂且认为是保证当线程池STOP时，线程一定会被打断 3）执行Runnable12345678910111213141516171819try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125;&#125; finally &#123; task = null; w.completedTasks++; w.unlock();&#125; beforeExecute(wt, task); 和 afterExecute(task, thrown); 默认是没有实现的，我们可以自己扩展 4）最后是当跳出while循环后（getTask() == null或者用户任务抛出异常），会去执行processWorkerExit(w, completedAbruptly);线程退出工作（该方法会根据线程池状态，尝试中止线程池。然后会考虑是结束当前线程，还是再新建一个工作线程，这里就不细说了） 我们再来看一下 getTask() 方法12345678910111213141516171819202122232425262728293031323334353637private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; int wc = workerCountOf(c); // Are workers subject to culling? boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; &#125; catch (InterruptedException retry) &#123; timedOut = false; &#125; &#125;&#125; 1） 第一段不做解释，满足该条件时，return null; 退出线程12345// Check if queue empty only if necessary.if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null;&#125; 2） 下面这段很有意思12345678910111213141516int wc = workerCountOf(c);// Are workers subject to culling?// 是否允许线程超时// 当我们设置了允许核心池超时 或者 有效线程数 &gt; 核心池数量的时候// 线程池会考虑为我们清除掉一些线程boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize;// (有效线程数 &gt; 最大线程池数量 || (允许超时 &amp;&amp; 超时) ) // &amp;&amp; (有效线程数 &gt; 1 || 或者队列为空时)if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) // timedOut 表示当前线程超时，下文会说到 &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue;&#125; 我在第一次看这段代码的时候，傻傻的以为 timedOut 不是永远为false吗，我以为JDK源码怎么写出这么个Bug。别忘了当前的getTask()方法也是在一个无限循环里 3）12345678910try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true;&#125; catch (InterruptedException retry) &#123; timedOut = false;&#125; 根据 timed，决定调用使用poll() 或者 take()。 poll 在队列为空时会等待指定时间，如果这期间没有获取到元素，则return null take 则在队列为空时会一直等待，直至队列中被添加新的任务，或者被打断；这两个方法都会被shutdown() 或者 shutdownNow的 thread.interrupt()打断；如果被打断则回到第一步 至此 execute() 方法所涉及的逻辑我们差不多分析完了 备注线程池使用1234567891011121314public class Test &#123; public static void main(String[] args) &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 10, 60, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(5)); executor.execute(() -&gt; &#123; // 业务逻辑 &#125;); executor.shutdown(); &#125;&#125; 合理配置线程池的大小一般需要根据任务的类型来配置线程池大小： 如果是CPU密集型任务，参考值可以设为 N+1 （N 为CPU核心数） 如果是IO密集型任务，参考值可以设置为2*N 当然，这只是一个参考值，具体的设置还需要根据实际情况进行调整，比如可以先将线程池大小设置为参考值，再观察任务运行情况和系统负载、资源利用率来进行适当调整。 参考 JDK1.8 https://www.cnblogs.com/dolphin0520/p/3932921.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>源码</tag>
        <tag>ThreadPoolExecutor</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 源码解析 @RequestBody @ResponseBody 的来龙去脉]]></title>
    <url>%2Fpost%2Fb75109d6.html</url>
    <content type="text"><![CDATA[@RequestBody 和 @ResponseBody 是实际开发中很常用的两个注解，通常用来解析和响应JSON，用起来十分的方便，这两个注解的背后是如何实现的？ 源码版本SpringBoot 2.1.3.RELEASE RequestResponseBodyMethodProcessor Resolves method arguments annotated with @RequestBody and handles return values from methods annotated with @ResponseBody by reading and writing to the body of the request or response with an HttpMessageConverter.An @RequestBody method argument is also validated if it is annotated with @javax.validation.Valid. In case of validation failure, MethodArgumentNotValidException is raised and results in an HTTP 400 response status code if DefaultHandlerExceptionResolver is configured. 简单来说，这个类用来解析@RequestBody的参数和处理 @ResponseBody返回值，通过 HttpMessageConverter 这个接口来实现。 如果@RequestBody标记的参数包含@Valid，还会对这个参数进行校验。 继承关系 HandlerMethodArgumentResolver 和 HandlerMethodReturnValueHandler 分别是Spring的参数处理器和返回值处理器 HandlerMethodArgumentResolver 12345678public interface HandlerMethodArgumentResolver &#123; boolean supportsParameter(MethodParameter parameter); Object resolveArgument(MethodParameter parameter, @Nullable ModelAndViewContainer mavContainer, NativeWebRequest webRequest, @Nullable WebDataBinderFactory binderFactory) throws Exception;&#125; Spring的参数解析器接口，supportsParameter() 方法用于判断解析器是否支持当前Controller方法的参数，resolveArgument() 则是将Request解析为Controller方法对应的参数Bean HandlerMethodReturnValueHandler 12345678public interface HandlerMethodReturnValueHandler &#123; boolean supportsReturnType(MethodParameter returnType); void handleReturnValue(@Nullable Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception;&#125; 同理这个接口将Controller方法返回的对象，封装为Response 我们在实际开发时，也可以实现这两个接口自定义自己的参数解析和响应处理，RequestResponseBodyMethodProcessor 实现了这两个接口，既做了参数解析器也做了响应处理器。 RequestResponseBodyMethodProcessor 源码分析我们来看一下 RequestResponseBodyMethodProcessor 是如何工作的，以解析参数为例 resolveArgument 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Overridepublic boolean supportsParameter(MethodParameter parameter) &#123; // 支持标记@RequestBody的参数 return parameter.hasParameterAnnotation(RequestBody.class);&#125;@Overridepublic Object resolveArgument(MethodParameter parameter, @Nullable ModelAndViewContainer mavContainer, NativeWebRequest webRequest, @Nullable WebDataBinderFactory binderFactory) throws Exception &#123; parameter = parameter.nestedIfOptional(); // 通过HttpMessageConverters 将请求体, 封装为@RequestBody所标记的XXBean Object arg = readWithMessageConverters(webRequest, parameter, parameter.getNestedGenericParameterType()); String name = Conventions.getVariableNameForParameter(parameter); if (binderFactory != null) &#123; WebDataBinder binder = binderFactory.createBinder(webRequest, arg, name); if (arg != null) &#123; // 如果存在@Valid 对参数进行校验 validateIfApplicable(binder, parameter); if (binder.getBindingResult().hasErrors() &amp;&amp; isBindExceptionRequired(binder, parameter)) &#123; throw new MethodArgumentNotValidException(parameter, binder.getBindingResult()); &#125; &#125; if (mavContainer != null) &#123; mavContainer.addAttribute(BindingResult.MODEL_KEY_PREFIX + name, binder.getBindingResult()); &#125; &#125; return adaptArgumentIfNecessary(arg, parameter);&#125;@Overrideprotected &lt;T&gt; Object readWithMessageConverters(NativeWebRequest webRequest, MethodParameter parameter, Type paramType) throws IOException, HttpMediaTypeNotSupportedException, HttpMessageNotReadableException &#123; HttpServletRequest servletRequest = webRequest.getNativeRequest(HttpServletRequest.class); Assert.state(servletRequest != null, "No HttpServletRequest"); ServletServerHttpRequest inputMessage = new ServletServerHttpRequest(servletRequest); Object arg = readWithMessageConverters(inputMessage, parameter, paramType); if (arg == null &amp;&amp; checkRequired(parameter)) &#123; throw new HttpMessageNotReadableException("Required request body is missing: " + parameter.getExecutable().toGenericString(), inputMessage); &#125; return arg;&#125; 作为参数解析器，RequestResponseBodyMethodProcessor 支持所有标记@RequestBody的参数。在resolveArgument()方法中，通过调用readWithMessageConverters() 将 Request 转为对应 arg。我们来看一下 readWithMessageConverters() 到底做了什么 readWithMessageConverters 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980protected &lt;T&gt; Object readWithMessageConverters(HttpInputMessage inputMessage, MethodParameter parameter, Type targetType) throws IOException, HttpMediaTypeNotSupportedException, HttpMessageNotReadableException &#123; // 当前请求的contentType MediaType contentType; boolean noContentType = false; try &#123; contentType = inputMessage.getHeaders().getContentType(); &#125; catch (InvalidMediaTypeException ex) &#123; throw new HttpMediaTypeNotSupportedException(ex.getMessage()); &#125; if (contentType == null) &#123; noContentType = true; contentType = MediaType.APPLICATION_OCTET_STREAM; &#125; // Controller参数的Class Class&lt;?&gt; contextClass = parameter.getContainingClass(); Class&lt;T&gt; targetClass = (targetType instanceof Class ? (Class&lt;T&gt;) targetType : null); if (targetClass == null) &#123; ResolvableType resolvableType = ResolvableType.forMethodParameter(parameter); targetClass = (Class&lt;T&gt;) resolvableType.resolve(); &#125; // 当前请求方式 HttpMethod httpMethod = (inputMessage instanceof HttpRequest ? ((HttpRequest) inputMessage).getMethod() : null); Object body = NO_VALUE; EmptyBodyCheckingHttpInputMessage message; try &#123; message = new EmptyBodyCheckingHttpInputMessage(inputMessage); // 遍历所有的HttpMessageConverter， for (HttpMessageConverter&lt;?&gt; converter : this.messageConverters) &#123; Class&lt;HttpMessageConverter&lt;?&gt;&gt; converterType = (Class&lt;HttpMessageConverter&lt;?&gt;&gt;) converter.getClass(); GenericHttpMessageConverter&lt;?&gt; genericConverter = (converter instanceof GenericHttpMessageConverter ? (GenericHttpMessageConverter&lt;?&gt;) converter : null); // 如果当前的HttpMessageConverter可以解析对应的 class和contentType if (genericConverter != null ? genericConverter.canRead(targetType, contextClass, contentType) : (targetClass != null &amp;&amp; converter.canRead(targetClass, contentType))) &#123; if (message.hasBody()) &#123; HttpInputMessage msgToUse = getAdvice().beforeBodyRead(message, parameter, targetType, converterType); // 将Http报文转换为对应的class body = (genericConverter != null ? genericConverter.read(targetType, contextClass, msgToUse) : ((HttpMessageConverter&lt;T&gt;) converter).read(targetClass, msgToUse)); body = getAdvice().afterBodyRead(body, msgToUse, parameter, targetType, converterType); &#125; else &#123; body = getAdvice().handleEmptyBody(null, message, parameter, targetType, converterType); &#125; break; &#125; &#125; &#125; catch (IOException ex) &#123; throw new HttpMessageNotReadableException("I/O error while reading input message", ex, inputMessage); &#125; if (body == NO_VALUE) &#123; if (httpMethod == null || !SUPPORTED_METHODS.contains(httpMethod) || (noContentType &amp;&amp; !message.hasBody())) &#123; return null; &#125; throw new HttpMediaTypeNotSupportedException(contentType, this.allSupportedMediaTypes); &#125; MediaType selectedContentType = contentType; Object theBody = body; LogFormatUtils.traceDebug(logger, traceOn -&gt; &#123; String formatted = LogFormatUtils.formatValue(theBody, !traceOn); return "Read \"" + selectedContentType + "\" to [" + formatted + "]"; &#125;); return body;&#125; 上述代码核心逻辑就是遍历当前解析中配置的所有 HttpMessageConverter，如果某个Converter可以解析当前的 contentType，就把转换工作交给他去进行。 之前做过将默认解析替换为fastjson，当时就是添加一个FastJson实现的HttpMessageConverter，但是那时候并不理解这么做是为了什么，现在才恍然大悟… handleReturnValue RequestResponseBodyMethodProcessor 的Response处理逻辑和解析逻辑类似，找到一个支持的HttpMessageConverter，把响应工作交给他，感兴趣的童鞋可以自己找下源码。 RequestResponseBodyMethodProcessor 是怎么被调用的上面讲了 RequestResponseBodyMethodProcessor 做了参数解析和响应处理的工作，那么他在Spring框架中是怎么被调用的，我们来看一下 如图，RequestMappingHandlerAdapter 的resolvers（Request解析器）、handlers（Response处理器）还有 ExceptionHandlerExceptionResolver 的handlers 调用了 RequestResponseBodyMethodProcessor RequestMappingHandlerAdapter我们只分析一下 RequestMappingHandlerAdapter ，该类对所有标记 @RequestMapping的注解进行解析和响应 在WebMvcConfigurationSupport中，配置了该Bean，将其加入到Spring容器中，我们自定义的参数解析、响应解析、和HttpMessageConvert 通过上图的方法set到 RequestMappingHandlerAdapter 中。 123456789101112131415161718192021222324252627282930@Beanpublic RequestMappingHandlerAdapter requestMappingHandlerAdapter() &#123; RequestMappingHandlerAdapter adapter = createRequestMappingHandlerAdapter(); adapter.setContentNegotiationManager(mvcContentNegotiationManager()); // 获取所有HttpMessageConverter，包括我们自定义的配置 adapter.setMessageConverters(getMessageConverters()); adapter.setWebBindingInitializer(getConfigurableWebBindingInitializer()); // 自定义的参数解析器 adapter.setCustomArgumentResolvers(getArgumentResolvers()); // 自定义的响应处理器 adapter.setCustomReturnValueHandlers(getReturnValueHandlers()); if (jackson2Present) &#123; adapter.setRequestBodyAdvice(Collections.singletonList(new JsonViewRequestBodyAdvice())); adapter.setResponseBodyAdvice(Collections.singletonList(new JsonViewResponseBodyAdvice())); &#125; AsyncSupportConfigurer configurer = new AsyncSupportConfigurer(); configureAsyncSupport(configurer); if (configurer.getTaskExecutor() != null) &#123; adapter.setTaskExecutor(configurer.getTaskExecutor()); &#125; if (configurer.getTimeout() != null) &#123; adapter.setAsyncRequestTimeout(configurer.getTimeout()); &#125; adapter.setCallableInterceptors(configurer.getCallableInterceptors()); adapter.setDeferredResultInterceptors(configurer.getDeferredResultInterceptors()); return adapter;&#125; 继续说 RequestMappingHandlerAdapter ，getDefaultArgumentResolvers() 封装了SpringBoot中的默认参数解析器，其中就有我们的本节所讲的 RequestResponseBodyMethodProcessor ，在afterPropertiesSet() 方法中调用了该方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667@Overridepublic void afterPropertiesSet() &#123; // Do this first, it may add ResponseBody advice beans initControllerAdviceCache(); if (this.argumentResolvers == null) &#123; List&lt;HandlerMethodArgumentResolver&gt; resolvers = getDefaultArgumentResolvers(); this.argumentResolvers = new HandlerMethodArgumentResolverComposite().addResolvers(resolvers); &#125; if (this.initBinderArgumentResolvers == null) &#123; List&lt;HandlerMethodArgumentResolver&gt; resolvers = getDefaultInitBinderArgumentResolvers(); this.initBinderArgumentResolvers = new HandlerMethodArgumentResolverComposite().addResolvers(resolvers); &#125; if (this.returnValueHandlers == null) &#123; List&lt;HandlerMethodReturnValueHandler&gt; handlers = getDefaultReturnValueHandlers(); this.returnValueHandlers = new HandlerMethodReturnValueHandlerComposite().addHandlers(handlers); &#125;&#125;private List&lt;HandlerMethodArgumentResolver&gt; getDefaultArgumentResolvers() &#123; List&lt;HandlerMethodArgumentResolver&gt; resolvers = new ArrayList&lt;&gt;(); // Annotation-based argument resolution resolvers.add(new RequestParamMethodArgumentResolver(getBeanFactory(), false)); resolvers.add(new RequestParamMapMethodArgumentResolver()); resolvers.add(new PathVariableMethodArgumentResolver()); resolvers.add(new PathVariableMapMethodArgumentResolver()); resolvers.add(new MatrixVariableMethodArgumentResolver()); resolvers.add(new MatrixVariableMapMethodArgumentResolver()); resolvers.add(new ServletModelAttributeMethodProcessor(false)); // 添加RequestResponseBodyMethodProcessor resolvers.add(new RequestResponseBodyMethodProcessor(getMessageConverters(), this.requestResponseBodyAdvice)); // 省略，详见源码... return resolvers;&#125;private List&lt;HandlerMethodReturnValueHandler&gt; getDefaultReturnValueHandlers() &#123; List&lt;HandlerMethodReturnValueHandler&gt; handlers = new ArrayList&lt;&gt;(); // Single-purpose return value types handlers.add(new ModelAndViewMethodReturnValueHandler()); handlers.add(new ModelMethodProcessor()); handlers.add(new ViewMethodReturnValueHandler()); handlers.add(new ResponseBodyEmitterReturnValueHandler(getMessageConverters(), this.reactiveAdapterRegistry, this.taskExecutor, this.contentNegotiationManager)); handlers.add(new StreamingResponseBodyReturnValueHandler()); handlers.add(new HttpEntityMethodProcessor(getMessageConverters(), this.contentNegotiationManager, this.requestResponseBodyAdvice)); handlers.add(new HttpHeadersReturnValueHandler()); handlers.add(new CallableMethodReturnValueHandler()); handlers.add(new DeferredResultMethodReturnValueHandler()); handlers.add(new AsyncTaskMethodReturnValueHandler(this.beanFactory)); // Annotation-based return value types handlers.add(new ModelAttributeMethodProcessor(false)); // 添加RequestResponseBodyMethodProcessor handlers.add(new RequestResponseBodyMethodProcessor(getMessageConverters(), this.contentNegotiationManager, this.requestResponseBodyAdvice)); // 省略，详见源码... return handlers;&#125; RequestResponseBodyMethodProcessor 何时被调用上面铺垫了这么多，终于来了 RequestMappingHandlerAdapter 的 invokeHandlerMethod 中 构建了 invocableMethod 对象并将所有的解析器和处理器封装到该对象，通过invocableMethod.invokeAndHandle() 进行对请求的解析，对controller的调用，以及响应的处理 invocableMethod.invokeAndHandle() 中是怎么样实现的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public void invokeAndHandle(ServletWebRequest webRequest, ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123; // 参数解析，并反射调用controller方法，获取方法返回值 Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs); // 下面就是对Response的处理 setResponseStatus(webRequest); if (returnValue == null) &#123; if (isRequestNotModified(webRequest) || getResponseStatus() != null || mavContainer.isRequestHandled()) &#123; mavContainer.setRequestHandled(true); return; &#125; &#125; else if (StringUtils.hasText(getResponseStatusReason())) &#123; mavContainer.setRequestHandled(true); return; &#125; mavContainer.setRequestHandled(false); Assert.state(this.returnValueHandlers != null, "No return value handlers"); try &#123; this.returnValueHandlers.handleReturnValue( returnValue, getReturnValueType(returnValue), mavContainer, webRequest); &#125; catch (Exception ex) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(formatErrorForReturnValue(returnValue), ex); &#125; throw ex; &#125;&#125;public Object invokeForRequest(NativeWebRequest request, @Nullable ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123; // 调用参数解析器获取调用controller 所需的参数 Object[] args = getMethodArgumentValues(request, mavContainer, providedArgs); if (logger.isTraceEnabled()) &#123; logger.trace("Arguments: " + Arrays.toString(args)); &#125; // 反射调用 controller return doInvoke(args);&#125;protected Object[] getMethodArgumentValues(NativeWebRequest request, @Nullable ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123; if (ObjectUtils.isEmpty(getMethodParameters())) &#123; return EMPTY_ARGS; &#125; MethodParameter[] parameters = getMethodParameters(); Object[] args = new Object[parameters.length]; // 遍历解析参数 for (int i = 0; i &lt; parameters.length; i++) &#123; MethodParameter parameter = parameters[i]; parameter.initParameterNameDiscovery(this.parameterNameDiscoverer); args[i] = findProvidedArgument(parameter, providedArgs); if (args[i] != null) &#123; continue; &#125; // 这里的 resolvers 是一个封装了所有参数解析器的包装类，遍历所有解析器，如果不能找到支持当前参数的，抛出异常 // 如果找到当前参数对应的解析器，则缓存起来，在下面的 resolvers.resolveArgument 时，直接使用 if (!this.resolvers.supportsParameter(parameter)) &#123; throw new IllegalStateException(formatArgumentError(parameter, "No suitable resolver")); &#125; try &#123; // 调用参数解析器 args[i] = this.resolvers.resolveArgument(parameter, mavContainer, request, this.dataBinderFactory); &#125; catch (Exception ex) &#123; // Leave stack trace for later, exception may actually be resolved and handled.. if (logger.isDebugEnabled()) &#123; String error = ex.getMessage(); if (error != null &amp;&amp; !error.contains(parameter.getExecutable().toGenericString())) &#123; logger.debug(formatArgumentError(parameter, error)); &#125; &#125; throw ex; &#125; &#125; return args;&#125; invokeAndHandle() 里做了三件事 将请求中解析为Controller中指定的参数 用解析好的参数反射调用 Controller 方法 处理响应 一次Http请求经历了什么回过头来再看，这时候我们发一个请求，在 RequestMappingHandlerAdapter 的 invokeHandlerMethod()中 debug一下，看一下线程栈是什么样的 简单画一张图来表示一下 END 欢迎各位给出意见和指正]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发编程-volatile]]></title>
    <url>%2Fpost%2Feb0b8344.html</url>
    <content type="text"><![CDATA[原文：Java并发编程：volatile关键字解析 原文讲解了很多东西作为铺垫，在原文内容有些删减和更改。如读者觉得不妥，或者理解不到位，建议阅读原文。 内存模型相关概念大家都知道，计算机在执行程序时，每条指令都是在CPU中执行的，而执行指令过程中，势必涉及到数据的读取和写入。由于程序运行过程中的临时数据是存放在主存（物理内存）当中的，这时就存在一个问题，由于CPU执行速度很快，而从内存读取数据和向内存写入数据的过程跟CPU执行指令的速度比起来要慢的多，因此如果任何时候对数据的操作都要通过和内存的交互来进行，会大大降低指令执行的速度。因此在CPU里面就有了高速缓存。 也就是，当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。举个简单的例子，比如下面的这段代码： 1i = i + 1; 当线程执行这个语句时，会先从主存当中读取i的值，然后复制一份到高速缓存当中，然后CPU执行指令对i进行加1操作，然后将数据写入高速缓存，最后将高速缓存中i最新的值刷新到主存当中。 这个代码在单线程中运行是没有任何问题的，但是在多线程中运行就会有问题了。在多核CPU中，每条线程可能运行于不同的CPU中，因此每个线程运行时有自己的高速缓存（对单核CPU来说，其实也会出现这种问题，只不过是以线程调度的形式来分别执行的）。本文我们以多核CPU为例。 比如同时有2个线程执行这段代码，假如初始时i的值为0，那么我们希望两个线程执行完之后i的值变为2。但是事实会是这样吗？ 可能存在下面一种情况：初始时，两个线程分别读取i的值存入各自所在的CPU的高速缓存当中，然后线程1进行加1操作，然后把i的最新值1写入到内存。此时线程2的高速缓存当中i的值还是0，进行加1操作之后，i的值为1，然后线程2把i的值写入内存。 最终结果i的值是1，而不是2。这就是著名的缓存一致性问题。通常称这种被多个线程访问的变量为共享变量。 也就是说，如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致的问题。 为了解决缓存不一致性问题，通常来说有以下2种解决方法： 1）通过在总线加LOCK#锁的方式 2）通过缓存一致性协议 这2种方式都是硬件层面上提供的方式。 在早期的CPU当中，是通过在总线上加LOCK#锁的形式来解决缓存不一致的问题。因为CPU和其他部件进行通信都是通过总线来进行的，如果对总线加LOCK#锁的话，也就是说阻塞了其他CPU对其他部件访问（如内存），从而使得只能有一个CPU能使用这个变量的内存。比如上面例子中 如果一个线程在执行 i = i +1，如果在执行这段代码的过程中，在总线上发出了LCOK#锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。 但是上面的方式会有一个问题，由于在锁住总线期间，其他CPU无法访问内存，导致效率低下。 所以就出现了缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。 什么是Java 内存模型在Java虚拟机规范中试图定义一种Java内存模型（Java Memory Model，JMM）来屏蔽各个硬件平台和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。那么Java内存模型规定了哪些东西呢，它定义了程序中变量的访问规则，往大一点说是定义了程序执行的次序。注意，为了获得较好的执行性能，Java内存模型并没有限制执行引擎使用处理器的寄存器或者高速缓存来提升指令执行速度，也没有限制编译器对指令进行重排序。也就是说，在java内存模型中，也会存在缓存一致性问题和指令重排序的问题。 Java内存模型规定所有的变量都是存在主存当中（类似于前面说的物理内存），每个线程都有自己的工作内存（类似于前面的高速缓存）。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存。 多线程中可能发生的问题内存不可见12345678//线程1boolean stop = false;while(!stop)&#123; doSomething();&#125; //线程2stop = true; 线程2通过stop = true 尝试停止线程1，线程1一定会中断吗？不一定 因为上文说过了，每个线程都会有自己的工作内存，线程1运行时，在将stop变量拷贝到自己的工作内存中。线程2这时候修改了stop，然后没来得及同步到主存，这时候线程2去做别的工作了，线程1不知道线程2修改了变量，一直循环下去。 重排序 什么是指令重排序？ 一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。 例如下面代码1234int a = 10; //语句1int r = 2; //语句2a = a + 3; //语句3r = a*a; //语句4 可能的顺序 但是绝不可能出现 语句2 -&gt; 语句1 -&gt; 语句4 -&gt; 语句3 上述顺序改变数据之间的依赖关系，处理器会考虑语句之间的依赖关系 重排序造成的影响 123456789//线程1:context = loadContext(); //语句1inited = true; //语句2 //线程2:while(!inited )&#123; sleep()&#125;doSomethingwithconfig(context); 线程1的代码没有依赖关系，如果顺序变为了 语句2 -&gt; 语句1，线程2可能就会拿到一个没有初始化的 context对象从而导致程序异常，而这种异常是很难排查的 volatile作用volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作 一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 1）保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 2）禁止进行指令重排序。 1. 内存可见性变量使用了volatile之后，所有读取操作全部发生在主存中，也就避免了上述的内存不可见的问题了 2. 防止重排序volatile关键字禁止指令重排序有两层意思： 1）当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行； 2）在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。 volidate 应用场景1.状态标记123456789volatile boolean flag = false; while(!flag)&#123; doSomething();&#125; public void setFlag() &#123; flag = true;&#125; 12345678910volatile boolean inited = false;//线程1:context = loadContext(); inited = true; //线程2:while(!inited )&#123;sleep()&#125;doSomethingwithconfig(context); 2.双重检查锁 （double check）1234567891011121314151617class Singleton &#123; private volatile static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if(instance==null) &#123; synchronized (Singleton.class) &#123; if(instance==null) instance = new Singleton(); &#125; &#125; return instance; &#125;&#125; 关于双重检查锁volatile 的两个作用 可见性 防止new Singleton()时重排序 创建对象可以简单分解为三步 1.分配内存空间 2.初始化对象 3.将对象指向刚分配的内存空间 处理器在优化性能时，可能会将第二步和第三步进行重排序，这样会导致第二个线程获取到未初始化完成的对象，导致程序异常。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>Java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 并发登录人数控制]]></title>
    <url>%2Fpost%2F5f58dc01.html</url>
    <content type="text"><![CDATA[通常系统都会限制同一个账号的登录人数，多人登录要么限制后者登录，要么踢出前者，Spring Security 提供了这样的功能，本文讲解一下在没有使用Security的时候如何手动实现这个功能 本文借鉴了 https://jinnianshilongnian.iteye.com/blog/2039760, 如果你是使用 Shiro + Session 的模式，推荐阅读此文 demo 技术选型 SpringBoot JWT Filter Redis + Redisson JWT（token）存储在Redis中，类似 JSessionId-Session的关系，用户登录后每次请求在Header中携带jwt 如果你是使用session的话，也完全可以借鉴本文的思路，只是代码上需要加些改动 两种实现思路比较时间戳维护一个 username: jwtToken 这样的一个 key-value 在Reids中, Filter逻辑如下 123456789101112131415161718192021222324252627282930313233343536373839404142public class CompareKickOutFilter extends KickOutFilter &#123; @Autowired private UserService userService; @Override public boolean isAccessAllowed(HttpServletRequest request, HttpServletResponse response) &#123; String token = request.getHeader("Authorization"); String username = JWTUtil.getUsername(token); String userKey = PREFIX + username; RBucket&lt;String&gt; bucket = redissonClient.getBucket(userKey); String redisToken = bucket.get(); if (token.equals(redisToken)) &#123; return true; &#125; else if (StringUtils.isBlank(redisToken)) &#123; bucket.set(token); &#125; else &#123; Long redisTokenUnixTime = JWTUtil.getClaim(redisToken, "createTime").asLong(); Long tokenUnixTime = JWTUtil.getClaim(token, "createTime").asLong(); // token &gt; redisToken 则覆盖 if (tokenUnixTime.compareTo(redisTokenUnixTime) &gt; 0) &#123; bucket.set(token); &#125; else &#123; // 注销当前token userService.logout(token); sendJsonResponse(response, 4001, "您的账号已在其他设备登录"); return false; &#125; &#125; return true; &#125;&#125; 队列踢出 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class QueueKickOutFilter extends KickOutFilter &#123; /** * 踢出之前登录的/之后登录的用户 默认踢出之前登录的用户 */ private boolean kickoutAfter = false; /** * 同一个帐号最大会话数 默认1 */ private int maxSession = 1; public void setKickoutAfter(boolean kickoutAfter) &#123; this.kickoutAfter = kickoutAfter; &#125; public void setMaxSession(int maxSession) &#123; this.maxSession = maxSession; &#125; @Override public boolean isAccessAllowed(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; String token = request.getHeader("Authorization"); UserBO currentSession = CurrentUser.get(); Assert.notNull(currentSession, "currentSession cannot null"); String username = currentSession.getUsername(); String userKey = PREFIX + "deque_" + username; String lockKey = PREFIX_LOCK + username; RLock lock = redissonClient.getLock(lockKey); lock.lock(2, TimeUnit.SECONDS); try &#123; RDeque&lt;String&gt; deque = redissonClient.getDeque(userKey); // 如果队列里没有此token，且用户没有被踢出；放入队列 if (!deque.contains(token) &amp;&amp; currentSession.isKickout() == false) &#123; deque.push(token); &#125; // 如果队列里的sessionId数超出最大会话数，开始踢人 while (deque.size() &gt; maxSession) &#123; String kickoutSessionId; if (kickoutAfter) &#123; // 如果踢出后者 kickoutSessionId = deque.removeFirst(); &#125; else &#123; // 否则踢出前者 kickoutSessionId = deque.removeLast(); &#125; try &#123; RBucket&lt;UserBO&gt; bucket = redissonClient.getBucket(kickoutSessionId); UserBO kickoutSession = bucket.get(); if (kickoutSession != null) &#123; // 设置会话的kickout属性表示踢出了 kickoutSession.setKickout(true); bucket.set(kickoutSession); &#125; &#125; catch (Exception e) &#123; &#125; &#125; // 如果被踢出了，直接退出，重定向到踢出后的地址 if (currentSession.isKickout()) &#123; // 会话被踢出了 try &#123; // 注销 userService.logout(token); sendJsonResponse(response, 4001, "您的账号已在其他设备登录"); &#125; catch (Exception e) &#123; &#125; return false; &#125; &#125; finally &#123; if (lock.isHeldByCurrentThread()) &#123; lock.unlock(); LOGGER.info(Thread.currentThread().getName() + " unlock"); &#125; else &#123; LOGGER.info(Thread.currentThread().getName() + " already automatically release lock"); &#125; &#125; return true; &#125;&#125; 比较两种方法 第一种方法逻辑简单粗暴, 只维护一个key-value 不需要使用锁，非要说缺点的话没有第二种方法灵活。 第二种方法我很喜欢，代码很优雅灵活，但是逻辑相对麻烦一些，而且为了保证线程安全地操作队列，要使用分布式锁。目前我们项目中使用的是第一种方法 演示下载地址: https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/login-control 运行项目，访问localhost:8887 demo中没有存储用户信息，随意输入用户名密码，用户名相同则被踢出 访问 localhost:8887/index.html 弹出用户信息, 代表当前用户有效 另一个浏览器登录相同用户名，回到第一个浏览器刷新页面，提示被踢出 application.properties中选择开启哪种过滤器模式，默认是比较时间戳踢出，开启队列踢出 queue-filter.enabled=true]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>Redis</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java理解生产者-消费者设计模式]]></title>
    <url>%2Fpost%2F5b860a98.html</url>
    <content type="text"><![CDATA[在实际的软件开发过程中，经常会碰到如下场景：某个模块负责产生数据，这些数据由另一个模块来负责处理（此处的模块是广义的，可以是类、函数、线程、进程等）。产生数据的模块，就形象地称为生产者；而处理数据的模块，就称为消费者； 生产者和消费者之间通过缓冲区(通常是一个阻塞队列)实现通讯, 生产者将产生的数据放入缓冲区，消费者从缓冲区中获取数据。 举个栗子去食堂吃饭，食堂的叔叔阿姨会先将饭做好，放到食堂窗口，同学们会去食堂打饭。 生产者(食堂的叔叔阿姨) -&gt; 生产数据(做饭) -&gt; 缓冲区(食堂窗口) -&gt; 消费数据(打饭) -&gt; 消费者(同学) 生产者消费者实现思路 生产者和消费者的任务很明确，生产者只管生产数据，然后添加到缓冲队列。而消费者只管从缓冲队列中获取数据 可以说生产者消费者都很无脑，而缓冲队列则要忙一些，他起到了一个平衡生产者和消费者的作用。 如果生产者生产速度过快，消费者消费的很慢，并且缓冲队列达到了最大长度时。缓冲队列会阻塞生产者，让生产者停止生产，等待消费者消费了数据后，再唤醒生产者 同理，当消费者消费速度过快时，队列为空时。缓冲队列则会阻塞消费者，待生产者向队列添加数据后，再唤醒消费者 实现通过上述的分析后，我们来用最基本的Java代码实现一下 我们先来定义一下Consumer 和Producer ，他们的逻辑比较简单，这里我们只循环十次模拟一下生产消费的场景。Buffer 为缓冲区，我们待会再看1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 消费者 */class Consumer extends Thread &#123; private Buffer buffer; private int number; public Consumer(Buffer b, int number) &#123; buffer = b; this.number = number; &#125; public void run() &#123; int value; for (int i = 0; i &lt; 10; i++) &#123; // 从缓冲区中获取数据 value = buffer.get(); try &#123; // 模拟消费数据 sleep(1000); &#125; catch (InterruptedException e) &#123; &#125; System.out.println("消费者 #" + this.number + " got: " + value); &#125; &#125;&#125;/** * 生产者 */class Producer extends Thread &#123; private Buffer buffer; private int number; public Producer(Buffer b, int number) &#123; buffer = b; this.number = number; &#125; public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; // 模拟生产数据 sleep(500); &#125; catch (InterruptedException e) &#123; &#125; // 将数据放入缓冲区 buffer.put(i); System.out.println("生产者 #" + this.number + " put: " + i); &#125; &#125;&#125; 可以看到 Consumer 和Producer 没有什么逻辑，只是对缓冲区的读写操作，下面我们来重点看一下 Buffer的实现 12345678910111213141516171819202122232425262728293031/** * 缓冲区 */class Buffer &#123; private List&lt;Integer&gt; data = new ArrayList&lt;&gt;(); private static final int MAX = 10; private static final int MIN = 0; public synchronized int get() &#123; while (MIN == data.size()) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; &#125; &#125; Integer i = data.remove(0); notifyAll(); return i; &#125; public synchronized void put(int value) &#123; while (MAX == data.size()) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; &#125; &#125; data.add(value); notifyAll(); &#125;&#125; 分析put(): 生产者向缓冲区写入数据的操作，当MAX == data.size()时，就是我们刚刚所说的生产者速度过快，消费者速度过慢的情况，这个时候身为 “缓冲区” 要平衡一下，调用 Object.wait()，让当前生产者线程进入挂起状态，等待消费者消费数据后将其唤醒 当MAX &gt; data.size()时，向ArrayList中添加数据，并尝试唤醒正在等待的消费者（这一步是必须的） get(): 消费者向缓冲区读数据的操作，和上述逻辑相反，当MIN == data.size()时，这时消费者速度太快，生产者太慢，队列中已经没有数据了。”缓冲区” 再一次站了出来，通过wait()，让当前消费者线程进入挂起状态，等待生产者生产数据后将其唤醒 当MIN &lt; data.size()时，取出ArrayList中第一条数据，并尝试唤醒正在等待的生产者 上述案例完整代码 ProducerConsumer.java 上述使用最基本的Java代码实现生产者消费者模式，实际开发中我们可能会使用BlockingQueue、ReentrantLock、ThreadPoolExecutor这些更成熟的轮子，但是一通百通 关于上述案例的思考 为什么缓冲区的判断条件是 while(condition) 而不是 if(condition)？答：防止线程被错误的唤醒举例：当有两个消费者线程wait() 时，此时生产者在队列里放入了一条数据，并调用notifyAll(), 两个消费者线程被唤醒，第一个消费者成功取出队列中数据，而第二个消费者此时就是被错误的唤醒了，程序抛出异常，所以此处使用 while(condition)循环检查 Java中要求wait()方法为什么要放在同步块中？答：防止出现Lost Wake-Up举例：如果队列没有同步限制，消费者和生产者并发执行，很可能出现这种情况，消费者这时候检查了条件正准备wait(),这时候上下文切换到了生产者，生产者咔咔一顿操作向队列中添加了数据，并唤醒了消费者，而此时消费者并没有wait()，这个通知就丢掉了，然后消费者wait() 就这样睡去了… 为什么缓冲区一定要使用阻塞队列实现？同理就是为了防止出现Lost Wake-Up 为什么要使用生产者消费者模式顺序执行不就可以了吗？生产者消费者到底有什么意义？ 并发 （异步）生产者直接调用消费者，两者是同步（阻塞）的，如果消费者吞吐数据很慢，这时候生产者白白浪费大好时光。而使用这种模式之后，生产者将数据丢到缓冲区，继续生产，完全不依赖消费者，程序执行效率会大大提高。 解耦生产者和消费者之间不直接依赖，通过缓冲区通讯，将两个类之间的耦合度降到最低。 参考https://blog.csdn.net/u011109589/article/details/80519863]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot 使用 AOP 防止重复提交]]></title>
    <url>%2Fpost%2F8398a16a.html</url>
    <content type="text"><![CDATA[在传统的web项目中，防止重复提交，通常做法是：后端生成一个唯一的提交令牌（uuid），并存储在服务端。页面提交请求携带这个提交令牌，后端验证并在第一次验证后删除该令牌，保证提交请求的唯一性。 上述的思路其实没有问题的，但是需要前后端都稍加改动，如果在业务开发完在加这个的话，改动量未免有些大了，本节的实现方案无需前端配合，纯后端处理。 思路 自定义注解 @NoRepeatSubmit 标记所有Controller中的提交请求 通过AOP 对所有标记了 @NoRepeatSubmit 的方法拦截 在业务方法执行前，获取当前用户的 token（或者JSessionId）+ 当前请求地址，作为一个唯一 KEY，去获取 Redis 分布式锁（如果此时并发获取，只有一个线程会成功获取锁） 业务方法执行后，释放锁 关于Redis 分布式锁 不了解的同学戳这里 ==&gt; Redis分布式锁的正确实现方式 使用Redis 是为了在负载均衡部署，如果是单机的部署的项目可以使用一个线程安全的本地Cache 替代 Redis Code这里只贴出 AOP 类和测试类，完整代码见 ==&gt; Gitee12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@Aspect@Componentpublic class RepeatSubmitAspect &#123; private final static Logger LOGGER = LoggerFactory.getLogger(RepeatSubmitAspect.class); @Autowired private RedisLock redisLock; @Pointcut("@annotation(noRepeatSubmit)") public void pointCut(NoRepeatSubmit noRepeatSubmit) &#123; &#125; @Around("pointCut(noRepeatSubmit)") public Object around(ProceedingJoinPoint pjp, NoRepeatSubmit noRepeatSubmit) throws Throwable &#123; int lockSeconds = noRepeatSubmit.lockTime(); HttpServletRequest request = RequestUtils.getRequest(); Assert.notNull(request, "request can not null"); // 此处可以用token或者JSessionId String token = request.getHeader("Authorization"); String path = request.getServletPath(); String key = getKey(token, path); String clientId = getClientId(); boolean isSuccess = redisLock.tryLock(key, clientId, lockSeconds); if (isSuccess) &#123; LOGGER.info("tryLock success, key = [&#123;&#125;], clientId = [&#123;&#125;]", key, clientId); // 获取锁成功, 执行进程 Object result; try &#123; result = pjp.proceed(); &#125; finally &#123; // 解锁 redisLock.releaseLock(key, clientId); LOGGER.info("releaseLock success, key = [&#123;&#125;], clientId = [&#123;&#125;]", key, clientId); &#125; return result; &#125; else &#123; // 获取锁失败，认为是重复提交的请求 LOGGER.info("tryLock fail, key = [&#123;&#125;]", key); return new ResultBean(ResultBean.FAIL, "重复请求，请稍后再试", null); &#125; &#125; private String getKey(String token, String path) &#123; return token + path; &#125; private String getClientId() &#123; return UUID.randomUUID().toString(); &#125;&#125; 多线程测试测试代码如下，模拟十个请求并发同时提交1234567891011121314151617181920212223242526272829303132333435363738394041424344@Componentpublic class RunTest implements ApplicationRunner &#123; private static final Logger LOGGER = LoggerFactory.getLogger(RunTest.class); @Autowired private RestTemplate restTemplate; @Override public void run(ApplicationArguments args) throws Exception &#123; System.out.println("执行多线程测试"); String url="http://localhost:8000/submit"; CountDownLatch countDownLatch = new CountDownLatch(1); ExecutorService executorService = Executors.newFixedThreadPool(10); for(int i=0; i&lt;10; i++)&#123; String userId = "userId" + i; HttpEntity request = buildRequest(userId); executorService.submit(() -&gt; &#123; try &#123; countDownLatch.await(); System.out.println("Thread:"+Thread.currentThread().getName()+", time:"+System.currentTimeMillis()); ResponseEntity&lt;String&gt; response = restTemplate.postForEntity(url, request, String.class); System.out.println("Thread:"+Thread.currentThread().getName() + "," + response.getBody()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; countDownLatch.countDown(); &#125; private HttpEntity buildRequest(String userId) &#123; HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_JSON); headers.set("Authorization", "yourToken"); Map&lt;String, Object&gt; body = new HashMap&lt;&gt;(); body.put("userId", userId); return new HttpEntity&lt;&gt;(body, headers); &#125;&#125; 成功防止重复提交，控制台日志如下，可以看到十个线程的启动时间几乎同时发起，只有一个请求提交成功了 本节demo戳这里 ==&gt; Giteebuild项目之后，启动本地redis，运行项目自动执行测试方法 参考https://www.jianshu.com/p/09c6b05b670a]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>分布式锁</tag>
        <tag>SpringBoot</tag>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-并发控制----读锁、写锁、乐观锁]]></title>
    <url>%2Fpost%2Fe708def4.html</url>
    <content type="text"><![CDATA[并发是一个让人很头疼的问题，通常我们会在服务端或者数据库端做处理，保证在并发下数据的准确性，今天我们简要的讨论一下MySQL中如何通过锁解决并发问题 读锁 也叫共享锁 （shared lock） 如何使用SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE 详解即事务A 使用共享锁 获取了某条（或者某些）记录时，事务B 可以读取这些记录，可以继续添加共享锁，但是不能修改或删除这些记录（当事务B 对这些数据修改或删除时会进入阻塞状态，直至锁等待超时或者事务A提交） 使用场景读取结果集的最新版本，同时防止其他事务产生更新该结果集主要用在需要数据依存关系时确认某行记录是否存在，并确保没有人对这个记录进行UPDATE或者DELETE操作 注意事项当使用读锁时，避免产生如下操作 123456事务1BEGIN;select * from sys_user where id = 1 LOCK IN SHARE MODE; (步骤1)update sys_user set username = &quot;taven&quot; where id = 1; (步骤3，发生阻塞)COMMIT; 123456事务2BEGIN;select * from sys_user where id = 1 LOCK IN SHARE MODE; (步骤2)update sys_user set username = &quot;taven&quot; where id = 1; (步骤4，死锁)COMMIT; 分析根据我们之前对读锁定义可知，当有事务拿到一个结果集的读锁时，其他事务想要更新该结果集，需要拿到读锁的事务提交（释放锁）。而上述情况两个事务分别拿到了读锁，而且都有update 操作，两个事务互相等待造成死锁（都在等待对方释放读锁） 写锁 也叫排它锁（exclusive lock） 如何使用SELECT * FROM table_name WHERE ... FOR UPDATE 详解一个写锁会阻塞其他的读锁和写锁即事务A 对某些记录添加写锁时，事务B 无法向这些记录添加写锁或者读锁（不添加锁的读取是可以的），事务B 也无法执行对 锁住的数据 update delete 使用场景读取结果集的最新版本，同时防止其他事务产生读取或者更新该结果集。例如：并发下对商品库存的操作 注意事项在使用读锁、写锁时都需要注意，读锁、写锁属于行级锁。即事务1 对商品A 获取写锁，和事务2 对商品B 获取写锁互相不会阻塞的。需要我们注意的是我们的SQL要合理使用索引，当我们的SQL 全表扫描的时候，行级锁会变成表锁。使用EXPLAIN查看 SQL是否使用了索引，扫描了多少行 乐观锁 上述介绍的是行级锁，可以最大程度地支持并发处理（同时也带来了最大的锁开销）乐观锁是一种逻辑锁，通过数据的版本号（vesion）的机制来实现，极大降低了数据库的性能开销。 我们为表添加一个字段 version，读取数据时将此版本号一同读出，之后更新时，对此版本号+1，同时将提交数据的version 与数据库中对应记录的当前version 进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据 123update t_goods set status=2,version=version+1where id=#&#123;id&#125; and version &lt; #&#123;version&#125;; // 更新前将version自增 或者123update t_goods set status=2,version=version+1where id=#&#123;id&#125; and version = #&#123;version&#125;; // 更新前version 不自增]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>并发</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-理解事务隔离级别]]></title>
    <url>%2Fpost%2Feca98c0.html</url>
    <content type="text"><![CDATA[在SQL标准中定义了四种隔离级别，每一种级别都规定了一个事务中所做的修改，哪些在事务内和事务间是可见的，哪些是不可见，较低的隔离级别通常可以执行更高的并发，系统的开销也更低 事务的ACID 原子性（atomicity）一个事务必须视为一个不可分割的最小工作单元，整个事务的所有操作要么全部提交成功，要么全部回滚 一致性（consistency）数据库总是从一个一致性状态转换到另一个一致性状态。 隔离性（isolation）通常来说，一个事务所作的修改在最终提交以前，对其他事务是不可见的，在讨论隔离级别的时候，会理解为什么说是 “通常来说” 是不可见的 持久性（durability）一旦事务提交，则所做的修改就会永久的保存到数据库中 隔离级别1.READ UNCOMMITTED （未提交读） 即在该级别，事务中的对数据的修改，即使没有提交，对其他事务也是可见的。这种情况被称为脏读（Dirty Read），这个级别会导致很多问题，而且性能相比其他级别不会好太多，实际很少使用。 2.READ COMMITTED （提交读） 大多数据库的默认隔离级别（但MySQL不是），该级别解决了脏读的问题。但是事务中会读到其他事务已提交的数据，无法保证读一致性，会造成不可重复读的问题。如下事务的两次读取是不一致的 3.REPEATABLE READ （可重复读） 该隔离级别，解决了脏读，不可重复读。InnoDB 使用 MVCC（多版本并发控制下文会讲到） 保证了事务中的读一致性。但还是会出现一个幻读的情况 如下图（左为事务A，右为事务B）事务执行这样的逻辑：查找是否有某条记录，如果不存在则新增。 事务A 查询是否存在 id=&quot;1&quot;这条数据，发现不存在，准备执行insert 此时事务B insert了这条数据，并提交了事务。 事务A 执行insert 发生主键冲突，再次执行了 select * from where id = &quot;1&quot;，发现依旧不存在当前结果集 RR 级别下如何防止幻读12# 用 X锁SELECT i` FROM `users` WHERE `id` = &quot;1&quot; FOR UPDATE; 如果 id = 1 的记录存在则会被加行（X）锁，如果不存在，则会加 next-lock key / gap 锁（范围行锁），即记录存在与否，mysql 都会对记录应该对应的索引加锁，其他事务是无法再获得做操作的。 4.SERIALIZABLE （串行化）在该隔离级别下，读取的每一行都会被添加锁（个人理解是读锁和gap锁），导致大量的超时和锁争用问题，实际应用中很少使用，只有在非常需要确保数据一致性且可以接受没有并发的情况下，才考虑该级别。 在 SERIALIZABLE 级别下再运行一下 上述的demo 可以看到步骤2的 insert 被阻塞了，上述“幻读”的情况 MVCCMVCC 是什么 MVVC (Multi-Version Concurrency Control, 多版本并发控制)，在InnoDB引擎下，MVCC是为了实现事务的隔离性，通过版本号，避免同一数据在不同事务间的竞争，你可以把它当成基于多版本号的一种乐观锁，读不加锁，读写不冲突MVCC 实现机制 InnoDB在每行数据都增加两个隐藏字段，一个记录创建的版本号，一个记录删除的版本号。 在MVVC 中，为了保证数据操作在多线程过程中，保证事务隔离的机制，降低锁竞争的压力，保证较高的并发量。在每开启一个事务时，会生成一个事务的版本号，被操作的数据会生成一条新的数据行（临时），但是在提交前对其他事务是不可见的，对于数据的更新（包括增删改）操作成功，会将这个版本号更新到数据的行中，事务提交成功，将新的版本号更新到此数据行中，这样保证了每个事务操作的数据，都是互不影响的，也不存在锁的问题。 MVVC下的CRUD （REPEATABLE READ下） SELECT： InnoDB 查询的每行数据必须满足以下两点 1、InnoDB必须找到一个行的版本，它至少要和事务的版本一样老(即它的版本号不大于事务的版本号)。这保证了不管是事务开始之前，或者事务创建时，或者修改了这行数据的时候，这行数据是存在的。 2、这行数据的删除版本必须是未定义的或者比事务版本要大。这可以保证在事务开始之前这行数据没有被删除。符合这两个条件的行可能会被当作查询结果而返回。 INSERT： InnoDB为这个新行记录当前的系统版本号。 DELETE： InnoDB将当前的系统版本号设置为这一行的删除ID。 UPDATE： InnoDB会写一个这行数据的新拷贝，这个拷贝的版本为当前的系统版本号。它同时也会将这个版本号写到旧行的删除版本里。 参考 MySQL之MVVC简介 mysql 幻读的详解、实例及解决办法 《高性能MySQL 第三版》]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-10w+数据-insert-优化]]></title>
    <url>%2Fpost%2F5c987bd6.html</url>
    <content type="text"><![CDATA[由于业务原因，遇到了如题所述的业务问题，事务执行时间在30s~50s 不等，效果非常不理想 方案1. jdbc批处理5w+ 数据测试，分别使用了mybatis insert()()（拼接xml）, mybatis的批处理和 jdbc的批处理。可以看到在jdbc执行时间方面是差不多的，但是在方法执行时间上，批处理要稍微快了一些，但是还是不理想 方案2. 优化MySQL 参数修改 my.ini innodb_buffer_pool_size : InnoDB, unlike MyISAM, uses a buffer pool to cache both indexes androw data. The bigger you set this the less disk I/O is needed toaccess data in tables. On a dedicated database server you may set thisparameter up to 80% of the machine physical memory size. Do not set ittoo large, though, because competition of the physical memory maycause paging in the operating system. Note that on 32bit systems youmight be limited to 2-3.5G of user level memory per process, so do notset it too high. Innodb的缓冲池会缓存数据和索引，设置的越大访问表中的数据所需的磁盘I/O就越少。 修改innodb_buffer_pool_size = 512M测试一下效率，这速度简直感人！ innodb_log_buffer_size : The size of the buffer InnoDB uses for buffering log data. As soon asit is full, InnoDB will have to flush it to disk. As it is flushed once per second anyway, it does not make sense to have it very large(even with long transactions). 表示InnoDB写入到磁盘上的日志文件时使用的缓冲区的字节数，默认值为8M。当缓冲区充满时，InnoDB将刷新数据到磁盘。由于它每秒刷新一次，所以将它设置得非常大是没有意义的 (即使是长事务)。 innodb_log_file_size : Size of each log file in a log group. You should set the combined sizeof log files to about 25%-100% of your buffer pool size to avoidunneeded buffer pool flush activity on log file overwrite. However,note that a larger logfile size will increase the time needed for therecovery process. 该值越大，缓冲池中必要的检查点刷新活动就会越少，节省磁盘I/ O。但是越大的日志文件，mysql的崩溃恢复就越慢 设置上述两个参数innodb_log_file_size=64M innodb_log_buffer_size=16M，效率提升的并不明显。 总结 数据量大时，批处理在方法执行时间上要比 mybatis xml拼接快一点 （批处理只编译一条SQL，而拼接的方式SQL会很长） 性能瓶颈优化还是要从数据库下手，目前来看MySQL 大数据量时很依赖 innodb_buffer_pool_size （缓冲池）参考 https://my.oschina.net/realfighter/blog/368225]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java打包成exe 在没有JRE环境的电脑上运行]]></title>
    <url>%2Fpost%2F33eaa3e0.html</url>
    <content type="text"><![CDATA[公司业务需求原因，需要给用户提供一个桌面应用程序。由于时间关系，没有考虑.net，使用了江湖上失传已久的Java Swing 准备工作 将你的Java项目打包为 可执行Jar 使用 exe4j 生成.exe 新建一个文件夹，将你的 JRE（与JDK同级别的那个JRE），可执行jar 都复制过来，下图除红圈以外的Jar 为项目的依赖Jar 包 打开 exe4j ，Next 选择 JAR IN EXE 选择刚刚我们新建的文件夹 （包含JRE，和项目可执行JAR） 这里是设置文件名，图标，是否仅允许一个应用实例等等，需要注意的是，64位的 jdk是需要设置一下的，如图 这里是将项目的，可执行JAR，还有依赖JAR 全部添加进来，注意一定要使用相对路径，Main class from 选择一个项目的启动类。 设置 Search sequence 清除默认配置，设置JRE，使用相对路径 后几步默认配置，下一步即可。 备注 需要注意的是，JAR和JRE 一定要使用相对路径（绝对路径无法保证你的路径和用户电脑一致） exe4j，没有激活之前生成的exe，启动之前会跳个对话框，激活即可 生成的exe 不依赖之前的JAR，只依赖JRE 参考：https://www.cnblogs.com/lsy-blogs/p/7668425.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>swing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Spring Cache + Redis 作为缓存]]></title>
    <url>%2Fpost%2Fef257646.html</url>
    <content type="text"><![CDATA[本文介绍如何使用 spring-cache，以及集成 Redis 作为缓存实现。表格过长，推荐读者使用电脑阅读 准备工作Redis windows 安装 如何配置maven完整依赖详见 ==&gt; Gitee123456789101112131415161718&lt;!-- 使用spring cache --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- redis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 为了解决 ClassNotFoundException: org.apache.commons.poolimpl.GenericObjectPoolConfig --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;0&lt;/version&gt;&lt;/dependency&gt; application.properties1234567891011121314151617181920212223# Redis数据库索引（默认为0）spring.redis.database=0 # Redis服务器地址spring.redis.host=localhost# Redis服务器连接端口spring.redis.port=6379 # Redis服务器连接密码（默认为空）#spring.redis.password=yourpwd# 连接池最大连接数（使用负值表示没有限制）spring.redis.lettuce.pool.max-active=8 # 连接池最大阻塞等待时间 spring.redis.lettuce.pool.max-wait=-1ms# 连接池中的最大空闲连接spring.redis.lettuce.pool.max-idle=8 # 连接池中的最小空闲连接spring.redis.lettuce.pool.min-idle=0 # 连接超时时间（毫秒）spring.redis.timeout=5000ms#配置缓存相关cache.default.expire-time=200cache.user.expire-time=180cache.user.name=test @EnableCaching标记注解 @EnableCaching，开启缓存，并配置Redis缓存管理器，需要初始化一个缓存空间。在缓存的时候，也需要标记使用哪一个缓存空间123456789101112131415161718192021222324252627282930313233343536373839404142434445@Configuration@EnableCachingpublic class RedisConfig &#123; @Value("$&#123;cache.default.expire-time&#125;") private int defaultExpireTime; @Value("$&#123;cache.user.expire-time&#125;") private int userCacheExpireTime; @Value("$&#123;cache.user.name&#125;") private String userCacheName; /** * 缓存管理器 * * @param lettuceConnectionFactory * @return */ @Bean public CacheManager cacheManager(RedisConnectionFactory lettuceConnectionFactory) &#123; RedisCacheConfiguration defaultCacheConfig = RedisCacheConfiguration.defaultCacheConfig(); // 设置缓存管理器管理的缓存的默认过期时间 defaultCacheConfig = defaultCacheConfig.entryTtl(Duration.ofSeconds(defaultExpireTime)) // 设置 key为string序列化 .serializeKeysWith(RedisSerializationContext.SerializationPair.fromSerializer(new StringRedisSerializer())) // 设置value为json序列化 .serializeValuesWith(RedisSerializationContext.SerializationPair.fromSerializer(new GenericJackson2JsonRedisSerializer())) // 不缓存空值 .disableCachingNullValues(); Set&lt;String&gt; cacheNames = new HashSet&lt;&gt;(); cacheNames.add(userCacheName); // 对每个缓存空间应用不同的配置 Map&lt;String, RedisCacheConfiguration&gt; configMap = new HashMap&lt;&gt;(); configMap.put(userCacheName, defaultCacheConfig.entryTtl(Duration.ofSeconds(userCacheExpireTime))); RedisCacheManager cacheManager = RedisCacheManager.builder(lettuceConnectionFactory) .cacheDefaults(defaultCacheConfig) .initialCacheNames(cacheNames) .withInitialCacheConfigurations(configMap) .build(); return cacheManager; &#125;&#125; 到此配置工作已经结束了 Spring Cache 使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Service@CacheConfig(cacheNames="user")// cacheName 是一定要指定的属性，可以通过 @CacheConfig 声明该类的通用配置public class UserService &#123; /** * 将结果缓存，当参数相同时，不会执行方法，从缓存中取 * * @param id * @return */ @Cacheable(key = "#id") public User findUserById(Integer id) &#123; System.out.println("===&gt; findUserById(id), id = " + id); return new User(id, "taven"); &#125; /** * 将结果缓存，并且该方法不管缓存是否存在，每次都会执行 * * @param user * @return */ @CachePut(key = "#user.id") public User update(User user) &#123; System.out.println("===&gt; update(user), user = " + user); return user; &#125; /** * 移除缓存，根据指定key * * @param user */ @CacheEvict(key = "#user.id") public void deleteById(User user) &#123; System.out.println("===&gt; deleteById(), user = " + user); &#125; /** * 移除当前 cacheName下所有缓存 * */ @CacheEvict(allEntries = true) public void deleteAll() &#123; System.out.println("===&gt; deleteAll()"); &#125;&#125; 注解 作用 @Cacheable | 将方法的结果缓存起来，下一次方法执行参数相同时，将不执行方法，返回缓存中的结果@CacheEvict | 移除指定缓存@CachePut | 标记该注解的方法总会执行，根据注解的配置将结果缓存@Caching | 可以指定相同类型的多个缓存注解，例如根据不同的条件@CacheConfig | 类级别注解，可以设置一些共通的配置，@CacheConfig(cacheNames=&quot;user&quot;), 代表该类下的方法均使用这个cacheNames 下面详细讲一下每个注解的作用和可选项。 Spring Cache 注解@EnableCaching 做了什么 @EnableCaching 注释触发后置处理器, 检查每一个Spring bean 的 public 方法是否存在缓存注解。如果找到这样的一个注释, 自动创建一个代理拦截方法调用和处理相应的缓存行为。 常用缓存注解简述@Cacheable将方法的结果缓存，必须要指定一个 cacheName（缓存空间）12@Cacheable(&quot;books&quot;)public Book findBook(ISBN isbn) &#123;...&#125; 默认 cache key缓存的本质还是以 key-value 的形式存储的，默认情况下我们不指定key的时候 ，使用 SimpleKeyGenerator 作为key的生成策略 如果没有给出参数，则返回SimpleKey.EMPTY。 如果只给出一个Param，则返回该实例。 如果给出了更多的Param，则返回包含所有参数的SimpleKey。 注意：当使用默认策略时，我们的参数需要有 有效的hashCode()和equals()方法 自定义 cache key12345678@Cacheable(cacheNames=&quot;books&quot;, key=&quot;#isbn&quot;)public Book findBook(ISBN isbn, boolean checkWarehouse, boolean includeUsed)@Cacheable(cacheNames=&quot;books&quot;, key=&quot;#isbn.rawNumber&quot;)public Book findBook(ISBN isbn, boolean checkWarehouse, boolean includeUsed)@Cacheable(cacheNames=&quot;books&quot;, key=&quot;T(someType).hash(#isbn)&quot;)public Book findBook(ISBN isbn, boolean checkWarehouse, boolean includeUsed) 如上，配合Spring EL 使用，下文会详细介绍 Spring EL 对 Cache 的支持 指定对象 指定对象中的属性 某个类的某个静态方法 自定义 keyGenerator12@Cacheable(cacheNames=&quot;books&quot;, keyGenerator=&quot;myKeyGenerator&quot;)public Book findBook(ISBN isbn, boolean checkWarehouse, boolean includeUsed) 实现 KeyGenerator接口可以自定义 cache key 的生成策略 自定义 cacheManager12@Cacheable(cacheNames=&quot;books&quot;, cacheManager=&quot;anotherCacheManager&quot;) public Book findBook(ISBN isbn) &#123;...&#125; 当我们的项目包含多个缓存管理器时，可以指定具体的缓存管理器，作为缓存解析 同步缓存在多线程环境中，可能会出现相同的参数的请求并发调用方法的操作，默认情况下，spring cache 不会锁定任何东西，相同的值可能会被计算几次，这就违背了缓存的目的 对于这些特殊情况，可以使用sync属性。此时只有一个线程在处于计算，而其他线程则被阻塞，直到在缓存中更新条目为止。12@Cacheable(cacheNames=&quot;foos&quot;, sync=true) public Foo executeExpensiveOperation(String id) &#123;...&#125; 条件缓存 condition: 什么情况缓存，condition = true 时缓存，反之不缓存 unless: 什么情况不缓存，unless = true 时不缓存，反之缓存12345@Cacheable(cacheNames=&quot;book&quot;, condition=&quot;#name.length() &lt; 32&quot;) public Book findBook(String name)@Cacheable(cacheNames=&quot;book&quot;, condition=&quot;#name.length() &lt; 32&quot;, unless=&quot;#result?.hardback&quot;)public Optional&lt;Book&gt; findBook(String name) Spring EL 对 Cache 的支持 Name Location Description Example methodName Root object 被调用的方法的名称 #root.methodName method Root object 被调用的方法 #root.method.name target Root object 当前调用方法的对象 #root.target targetClass Root object 当前调用方法的类 #root.targetClass args Root object 当前方法的参数 #root.args[0] caches Root object 当前方法的缓存集合 #root.caches[0].name Argument name Evaluation context 当前方法的参数名称 #iban or #a0 (you can also use #p0 or #p&lt;#arg&gt; notation as an alias). result Evaluation context 方法返回的结果(要缓存的值)。只有在 unless 、@CachePut(用于计算键)或@CacheEvict(beforeInvocation=false)中才可用.对于支持的包装器(例如Optional)，#result引用的是实际对象，而不是包装器 #result @CachePut这个注解和 @Cacheable 有点类似，都会将结果缓存，但是标记 @CachePut 的方法每次都会执行，目的在于更新缓存，所以两个注解的使用场景完全不同。@Cacheable 支持的所有配置选项，同样适用于@CachePut 12@CachePut(cacheNames=&quot;book&quot;, key=&quot;#isbn&quot;)public Book updateBook(ISBN isbn, BookDescriptor descriptor) 需要注意的是，不要在一个方法上同时使用@Cacheable 和 @CachePut @CacheEvict用于移除缓存 可以移除指定key 声明 allEntries=true移除该CacheName下所有缓存 声明beforeInvocation=true 在方法执行之前清除缓存，无论方法执行是否成功12345@CacheEvict(cacheNames=&quot;book&quot;, key=&quot;#isbn&quot;)public Book updateBook(ISBN isbn, BookDescriptor descriptor)@CacheEvict(cacheNames=&quot;books&quot;, allEntries=true) public void loadBooks(InputStream batch) @Caching可以让你在一个方法上嵌套多个相同的Cache 注解（@Cacheable, @CachePut, @CacheEvict），分别指定不同的条件12@Caching(evict = &#123; @CacheEvict(&quot;primary&quot;), @CacheEvict(cacheNames=&quot;secondary&quot;, key=&quot;#p0&quot;) &#125;)public Book importBooks(String deposit, Date date) @CacheConfig类级别注解，用于配置一些共同的选项（当方法注解声明的时候会被覆盖），例如 CacheName。 支持的选项如下：123456789101112@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface CacheConfig &#123; String[] cacheNames() default &#123;&#125;; String keyGenerator() default &quot;&quot;; String cacheManager() default &quot;&quot;; String cacheResolver() default &quot;&quot;;&#125; 参考： https://docs.spring.io/spring/docs/current/spring-framework-reference/integration.html#cache-annotations-cacheable https://spring.io/guides/gs/caching/ 本文demo： https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/springboot-redis]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>SpringBoot</tag>
        <tag>Cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Shiro 一》SpringBoot+Shiro 构建Web工程]]></title>
    <url>%2Fpost%2F9d0d6a3a.html</url>
    <content type="text"><![CDATA[Shiro 一款简单易用，功能强大的安全框架，帮助我们安全高效的构建企业级应用。之前几个项目都用到过 Shiro，最近抽空梳理了一下，分享一些经验。 本文demo：戳这里本文demo选型：thymeleaf, springboot 2, shiro, ehcachePS:如果我不拖延的话，估计还是会有后续的 :） Shiro 能做什么 认证：登录用户的认证 权限：基于角色和权限的访问权限（url权限），以及颗粒化权限控制（按钮权限） 加密技术：Shiro的crypto包中包含了一系列的易于理解和使用的加密、哈希（aka摘要）辅助类 session管理：可在web容器以及 EJB容器中使用 session，可扩展 （例如我们可以通过重写 sessionDao 将 session 存储到数据库中） RememberMe：基于cookie的记住我服务 Shiro 常用组件介绍 Subject：Subject其实代表的就是当前正在执行操作的用户，只不过因为“User”一般指代人，但是一个“Subject”可以是人，也可以是任何的第三方系统，服务账号等任何其他正在和当前系统交互的第三方软件系统。 所有的Subject实例都被绑定到一个SecurityManager，如果你和一个Subject交互，所有的交互动作都会被转换成Subject与SecurityManager的交互 SecurityManager：Shiro的核心，他主要用于协调Shiro内部各种安全组件，不过我们一般不用太关心SecurityManager，对于应用程序开发者来说，主要还是使用Subject的API来处理各种安全验证逻辑 Realm：这是用于连接Shiro和客户系统的用户数据的桥梁。一旦Shiro真正需要访问各种安全相关的数据（比如使用用户账户来做用户身份验证以及权限验证）时，他总是通过调用系统配置的各种Realm来读取数据 关于Shiro 的其余核心组件参考 Shiro 官网 或者 Shiro的架构 本文不做过多的阐述 Shiro 是如何工作的简单来讲的话，在Spring项目中 Shiro 会将他的所有组件注册到 SecurityManager中 再通过将 SecurityManager 注册到 ShiroFilterFactoryBean（这个类实现了Spring 的BeanPostProcessor会预先加载） 中， 最后以 filter 的形式注册到Spring容器（实现了Spring的FactoryBean，构造一个 filter 注册到 Spring 容器中），实现用户权限的管理。 Shiro 如何集成 shiro 所需依赖，完整见demo源码 12345678910111213141516171819202122&lt;!--shiro--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-ehcache&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 基于thymeleaf的shiro扩展 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.theborakompanioni&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-extras-shiro&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt; ShiroConfig 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117@Configurationpublic class ShiroConfig &#123; private static final Logger log = LoggerFactory.getLogger(ShiroConfig.class); @Bean public ShiroFilterFactoryBean shiroFilterFactoryBean(DefaultWebSecurityManager securityManager) &#123; ShiroFilterFactoryBean shiroFilter = new ShiroFilterFactoryBean(); shiroFilter.setSecurityManager(securityManager); Map&lt;String, String&gt; chainDefinition = new LinkedHashMap&lt;&gt;(); // 静态资源与登录请求不拦截 chainDefinition.put("/js/**", "anon"); chainDefinition.put("/css/**", "anon"); chainDefinition.put("/img/**", "anon"); chainDefinition.put("/layui/**", "anon"); chainDefinition.put("/login", "anon"); chainDefinition.put("/login.html", "anon"); // 用户为授权通过认证 &amp;&amp; 包含'admin'角色 chainDefinition.put("/admin/**", "authc, roles[super_admin]"); // 用户为授权通过认证或者RememberMe &amp;&amp; 包含'document:read'权限 chainDefinition.put("/docs/**", "user, perms[document:read]"); // 用户访问所有请求 授权通过 || RememberMe chainDefinition.put("/**", "user"); shiroFilter.setFilterChainDefinitionMap(chainDefinition); // 当 用户身份失效时重定向到 loginUrl shiroFilter.setLoginUrl("/login.html"); // 用户登录后默认重定向请求 shiroFilter.setSuccessUrl("/index.html"); return shiroFilter; &#125; @Bean public Realm realm() &#123; ShiroRealm realm = new ShiroRealm(); realm.setCredentialsMatcher(credentialsMatcher()); realm.setCacheManager(ehCacheManager()); return realm; &#125; @Bean public CacheManager ehCacheManager() &#123; EhCacheManager cacheManager = new EhCacheManager(); cacheManager.setCacheManagerConfigFile("classpath:ehcache.xml"); return cacheManager; &#125; @Bean public CredentialsMatcher credentialsMatcher() &#123; AuthCredentialsMatcher credentialsMatcher = new AuthCredentialsMatcher(ehCacheManager()); credentialsMatcher.setHashAlgorithmName(AuthCredentialsMatcher.HASH_ALGORITHM_NAME); credentialsMatcher.setHashIterations(AuthCredentialsMatcher.HASH_ITERATIONS); credentialsMatcher.setStoredCredentialsHexEncoded(true); return credentialsMatcher; &#125; @Bean public DefaultWebSecurityManager securityManager() &#123; log.debug("--------------shiro已经加载----------------"); DefaultWebSecurityManager manager = new DefaultWebSecurityManager(); manager.setCacheManager(ehCacheManager()); manager.setRealm(realm()); manager.setRememberMeManager(rememberMeManager()); return manager; &#125; @Bean public RememberMeManager rememberMeManager() &#123; CookieRememberMeManager cookieRememberMeManager = new CookieRememberMeManager(); //rememberMe cookie加密的密钥 建议每个项目都不一样 默认AES算法 密钥长度(128 256 512 位) cookieRememberMeManager.setCipherKey(Base64.decode("2AvVhdsgUs0FSA3SDFAdag==")); cookieRememberMeManager.setCookie(rememberMeCookie()); return cookieRememberMeManager; &#125; @Bean public SimpleCookie rememberMeCookie()&#123; //这个参数是cookie的名称，对应前端的checkbox的name = rememberMe SimpleCookie simpleCookie = new SimpleCookie("rememberMe"); //&lt;!-- 记住我cookie生效时间30天 ,单位秒;--&gt; simpleCookie.setMaxAge(259200); return simpleCookie; &#125; /** * Shiro生命周期处理器: * 用于在实现了Initializable接口的Shiro bean初始化时调用Initializable接口回调(例如:UserRealm) * 在实现了Destroyable接口的Shiro bean销毁时调用 Destroyable接口回调(例如:DefaultSecurityManager) */ @Bean public LifecycleBeanPostProcessor lifecycleBeanPostProcessor() &#123; return new LifecycleBeanPostProcessor(); &#125; /** * 启用shrio授权注解拦截方式，AOP式方法级权限检查 */ @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor(DefaultWebSecurityManager securityManager) &#123; AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor = new AuthorizationAttributeSourceAdvisor(); authorizationAttributeSourceAdvisor.setSecurityManager(securityManager); return authorizationAttributeSourceAdvisor; &#125; /** * thymeleaf的shiro扩展 * * @return */ @Bean public ShiroDialect shiroDialect() &#123; return new ShiroDialect(); &#125;&#125; 以上基本是Spring项目集成 Shiro 的通用配置，下面针对上述的几个Bean 聊一聊1. ShiroFilterFactoryBean：用于定义 请求的拦截规则, Shiro为我们默认提供了一些选项，常用如下 anon: 请求不拦截 authc: 要求用户必须认证通过 user: 要求用户为记住我状态 roles[xxx]: 要求用户必须满足 xxx 角色 perms[xxx]: 要求用户必须满足 xxx 权限其实上述每一个都对应了一个 Shiro 过滤器 Filter Name Class anon org.apache.shiro.web.filter.authc.AnonymousFilter authc org.apache.shiro.web.filter.authc.FormAuthenticationFilter authcBasic org.apache.shiro.web.filter.authc.BasicHttpAuthenticationFilter logout org.apache.shiro.web.filter.authc.LogoutFilter noSessionCreation org.apache.shiro.web.filter.session.NoSessionCreationFilter perms | org.apache.shiro.web.filter.authz.PermissionsAuthorizationFilterport| org.apache.shiro.web.filter.authz.PortFilterrest| org.apache.shiro.web.filter.authz.HttpMethodPermissionFilterroles| org.apache.shiro.web.filter.authz.RolesAuthorizationFilterssl| org.apache.shiro.web.filter.authz.SslFilteruser| org.apache.shiro.web.filter.authc.UserFilter 我们也可以自定义 过滤器来实现拦截 2. Realm：上面提到过Realm是用于连接Shiro和客户系统的用户数据的桥梁, 我们通过实现AuthorizingRealm 来提供用户认证和授权两个API 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class ShiroRealm extends AuthorizingRealm &#123; private static final Logger log = LoggerFactory.getLogger(AuthorizingRealm.class); @Autowired @Lazy // 这里lazy 是有必要的, shiro组件会预先加载，导致依赖的bean 没有生成代理对象（AOP失效） private UserService userService; /** * 认证 * * @param authenticationToken * @return * @throws AuthenticationException */ @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException &#123; String username = (String) authenticationToken.getPrincipal(); if (log.isDebugEnabled()) &#123; log.debug(String.format("user:%s executing doGetAuthenticationInfo", username)); &#125; User user = userService.getUserByUsername(username); if (user == null) &#123; throw new UnknownAccountException(); &#125; if (Constant.IS_LOCK.equals(user.getIsLock())) &#123; throw new LockedAccountException(); &#125; // ShiroUser 作为实际的 principal ShiroUser shiroUser = new ShiroUser(); BeanUtils.copyProperties(user, shiroUser); // SimpleAuthenticationInfo(Object principal, Object credentials, String realmName) // principal 会被封装到 subject 中 // shiro 默认会把我们的 credentials (也就是password) 和 token 中的作对比，所以我们可以不用做密码校验 ByteSource salt = ByteSource.Util.bytes(user.getUsername()); SimpleAuthenticationInfo info = new SimpleAuthenticationInfo(shiroUser, user.getPassword(), salt, getName()); if (log.isDebugEnabled()) &#123; log.debug(String.format("user:%s executed doGetAuthenticationInfo", username)); &#125; return info; &#125; /** * 授权 * * @param principalCollection * @return */ @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) &#123; ShiroUser shiroUser = (ShiroUser) principalCollection.getPrimaryPrincipal(); if (log.isDebugEnabled()) &#123; log.debug(String.format("user:%s executing doGetAuthorizationInfo", shiroUser.getUsername())); &#125; AuthorizationDTO authorizationDTO = userService.getRolesAndPermissions(shiroUser.getId()); SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); info.addRoles(authorizationDTO.getRoleCodeSet()); info.addStringPermissions(authorizationDTO.getPermissionCodeSet()); if (log.isDebugEnabled()) &#123; log.debug(String.format("user:%s executed doGetAuthorizationInfo", shiroUser.getUsername())); &#125; return info; &#125;&#125; doGetAuthenticationInfo : 认证方法，在执行 subject.login(token);后，Shiro认证器会读取 Realm 中的该方法获取 AuthenticationInfo对象（认证信息），包含principal（我们存储在shiro subject中的对象），credentials （密码）。 doGetAuthorizationInfo: 授权方法，在需要校验用户访问权限的时候，Shiro授权器会读取 Realm 中的该方法获取 AuthorizationInfo对象（授权信息）读取DB后，可以通过 addRoles(roleCollection) 和 addStringPermissions(permCollection) 设置当前用户的角色和权限。Shiro 在拿到这个权限信息后，会去找缓存管理器，以当前 subject 的 principal 作为key 缓存起来。 3. CredentialsMatcher: 密码匹配器，用于匹配 doGetAuthenticationInfo 方法返回的 credentials 和 subject.login(token);时的 token 中的 password是否一致。常用的实现有 SimpleCredentialsMatcher（默认是该实现）、HashedCredentialsMatcher （该实现可以进行加密匹配） 4. DefaultWebSecurityManager：如上述，用于协调Shiro内部各种安全组件，我们需要将我们扩展的bean 注册到 SecurityManager 中 5. RememberMeManager：开启该组件后使用记住我服务， token 中 rememberMe 为 true 时，登录成功之后会创建RememberMe cookie。 其余参考上文代码注释 关于 thymeleaf-extras-shiroShiro 默认支持在 jsp 中使用 shiro标签。但是想在 thymeleaf 中使用 Shiro 标签呢？ 使用 thymeleaf-extras-shiro 完美解决 thymeleaf 颗粒化权限控制1234567891011你好, &lt;span th:text=&quot;$&#123;principal&#125;&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;p shiro:hasRole=&quot;super_admin&quot;&gt;当前角色超级管理员&lt;/p&gt;&lt;button shiro:hasPermission=&quot;&apos;sys:user:add&apos;&quot;&gt;添加&lt;/button&gt;&lt;button shiro:hasPermission=&quot;&apos;sys:user:update&apos;&quot;&gt;编辑&lt;/button&gt;&lt;button shiro:hasPermission=&quot;&apos;sys:user:lock&apos;&quot;&gt;冻结&lt;/button&gt;&lt;div shiro:hasAllPermissions=&quot;&apos;sys:user:add, sys:user:update, sys:user:lock&apos;&quot;&gt; &lt;span&gt;满足所有权限时显示&lt;/span&gt;&lt;/div&gt;&lt;div shiro:hasAnyPermissions=&quot;&apos;sys:user:add, sys:user:update, sys:user:lock&apos;&quot;&gt; &lt;span&gt;满足一个权限即可显示&lt;/span&gt;&lt;/div&gt; 更多用法参考Github 文档：https://github.com/theborakompanioni/thymeleaf-extras-shiro 本文demohttps://gitee.com/yintianwen7/taven-springboot-learning/tree/master/springboot-shiro如果你发现我的文章或者demo中存在问题，请联系我]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>Shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-又一次事务不生效的排查]]></title>
    <url>%2Fpost%2F836297d7.html</url>
    <content type="text"><![CDATA[上一篇关于事务的 解决 spring service 调用当前类方法事务不生效 正文今天在工作中突然发现标记了 @Transactional 的方法，事务并没有生效。在检查了业务层是否 try catch异常，MySQL存储引擎之后。心态已近崩溃的边缘：)这个时候突然发现了一个神奇的现象！两个@Service 都标记了 @Transactional，userService 并没有生成代理对象，也就导致了事务不生效。 继续排查，最后锁定了凶手 — BeanPostProcessor 先看一下官方描述1234567891011121314/** * Factory hook that allows for custom modification of new bean instances, * e.g. checking for marker interfaces or wrapping them with proxies. * * &lt;p&gt;ApplicationContexts can autodetect BeanPostProcessor beans in their * bean definitions and apply them to any beans subsequently created. * Plain bean factories allow for programmatic registration of post-processors, * applying to all beans created through this factory. * * &lt;p&gt;Typically, post-processors that populate beans via marker interfaces * or the like will implement &#123;@link #postProcessBeforeInitialization&#125;, * while post-processors that wrap beans with proxies will normally * implement &#123;@link #postProcessAfterInitialization&#125;. * 简单的来说，BeanPostProcessor是Spring 给我们提供的一个扩展接口1234567public interface BeanPostProcessor &#123; // bean实例化方法调用前被调用 Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; // bean实例化方法调用后被调用 Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException;&#125; 分析 ApplicationContexts 检测到 BeanPostProcessor 之后会将他应用于随后创建的所有 bean，所以 BeanPostProcessor 会在其他Bean的之前加载，但是随之引发的问题的就是 BeanPostProcessor 实现类所引用的Bean 没有被代理，只是被托管到 IOC 容器中。 我的项目里引用了 Shiro，而 Shiro 的所有组件最终会被封装到 ShiroFilterFactoryBean （该类实现了 BeanPostProcessor）中，而 Shiro 的 Realm 中又依赖了我们的 Service，该 Service 预先加载，导致没有被代理 解决方案1234567public class AuthRealm extends AuthorizingRealm &#123; @Autowired @Lazy // 延迟加载 private UserService userService;&#125; 总结这里给大家简单介绍引起事务不生效的几个原因 try catch 捕获 Service 运行时异常，因为 Spring 默认在捕获到 RuntimeException 时回滚 MySQL存储引擎 InnoDB 是支持事务的，而 MyISAM 不支持 由于各种原因没有使用或者 Spring 没有生成代理对象]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>AOP</tag>
        <tag>Shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-insert-or-update]]></title>
    <url>%2Fpost%2F73b0b9b2.html</url>
    <content type="text"><![CDATA[我们经常会遇到类似的业务场景，插入一条数据如果他不存在则执行 insert ，当这条记录存在的时候，我们去 update 他的一些属性（或者什么都不做）。 解决方案： 使用 ON DUPLICATE KEY UPDATE在 主键 或者 唯一约束 重复时，执行更新操作。 使用 REPLACE INTO在 主键 或者 唯一约束 重复时，先 delete 再 insert。 ON DUPLICATE KEY UPDATE 创建表，建立唯一约束，准备一条数据123456789101112CREATE TABLE `stu_class_ref` ( `id` varchar(30) NOT NULL, `stu_id` varchar(30) DEFAULT NULL, `class_id` varchar(30) DEFAULT NULL, `note` varchar(100) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `stu_id` (`stu_id`,`class_id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;INSERT INTO `test`.`stu_class_ref`(`id`, `stu_id`, `class_id`, `note`) VALUES (&apos;001&apos;, &apos;zhangsan&apos;, &apos;yuwen&apos;, NULL); 使用 ON DUPLICATE KEY UPDATE 123456INSERT INTO `test`.`stu_class_ref`(`id`, `stu_id`, `class_id`, `note`) VALUES (UUID_SHORT(), &apos;zhangsan&apos;, &apos;yuwen&apos;, &apos;我喜欢语文:)&apos;)ON DUPLICATE KEY UPDATE note = &apos;我喜欢语文:)&apos;&gt; Affected rows: 2&gt; 时间: 0.042s Affected rows: 2，MySQL 检查插入的行是否会产生重复键错误，如果会则执行update 如果想要引用 VALUES 中的值，参考如下123456INSERT INTO `test`.`stu_class_ref`(`id`, `stu_id`, `class_id`, `note`) VALUES (UUID_SHORT(), &apos;zhangsan&apos;, &apos;yuwen&apos;, NULL)ON DUPLICATE KEY UPDATE note = VALUES(class_id)&gt; Affected rows: 2&gt; 时间: 0.006s REPLACE INTO MySQL 中 还有一个黑科技语法 REPLACE INTO1234REPLACE INTO `stu_class_ref`(`id`, `stu_id`, `class_id`, `note`) VALUES (UUID_SHORT(), &apos;zhangsan&apos;, &apos;yuwen&apos;, NULL)&gt; Affected rows: 2&gt; 时间: 0.004s REPLACE INTO 就比较简单粗暴了，他会先执行delete 操作，然后insert ON DUPLICATE KEY UPDATE 与 REPLACE INTO 再来创建一张表, 创建三个唯一约束, 插入三条数据123456789101112131415161718CREATE TABLE `interesting` ( `id` varchar(30) NOT NULL, `uni_a` varchar(30) DEFAULT NULL, `uni_b` varchar(30) DEFAULT NULL, `uni_c` varchar(30) DEFAULT NULL, `version` int(11) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `uni_a` (`uni_a`) USING BTREE, UNIQUE KEY `uni_b` (`uni_b`) USING BTREE, UNIQUE KEY `uni_c` (`uni_c`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;INSERT INTO `test`.`interesting`(`id`, `uni_a`, `uni_b`, `uni_c`, `version`) VALUES (&apos;1&apos;, &apos;a&apos;, &apos;a&apos;, &apos;a&apos;, NULL);INSERT INTO `test`.`interesting`(`id`, `uni_a`, `uni_b`, `uni_c`, `version`) VALUES (&apos;2&apos;, &apos;b&apos;, &apos;b&apos;, &apos;b&apos;, NULL);INSERT INTO `test`.`interesting`(`id`, `uni_a`, `uni_b`, `uni_c`, `version`) VALUES (&apos;3&apos;, &apos;c&apos;, &apos;c&apos;, &apos;c&apos;, NULL); 执行 ON DUPLICATE KEY UPDATE12345INSERT INTO `interesting`(`id`, `uni_a`, `uni_b`, `uni_c`, `version`) VALUES (UUID_SHORT(), &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, NULL)ON DUPLICATE KEY UPDATE version = 666&gt; Affected rows: 2&gt; 时间: 0.049s Affected rows: 2 但是其实三条主键都有冲突了 再看一下 REPLACE INTO1234REPLACE INTO `interesting`(`id`, `uni_a`, `uni_b`, `uni_c`, `version`) VALUES (UUID_SHORT(), &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, NULL)&gt; Affected rows: 4&gt; 时间: 0.026s Affected rows: 4 REPLACE INTO 将三条有冲突的全部delete 然后 insert ####总结： ON DUPLICATE KEY UPDATE 只会对所匹配的第一行进行update, REPLACE INTO 会对所有匹配行进行delete, insert 所以应避免对有多个唯一索引的表使用]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 使用 hibernate validator]]></title>
    <url>%2Fpost%2F81ef5d67.html</url>
    <content type="text"><![CDATA[本文将全面的介绍如何使用 validator 进行数据校验 本文源码：https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/springboot-validate 准备工作我们只需要引入 spring-boot-starter-web包即可使用 常用注解 注解 释义 @Valid 被注释的元素是一个对象，需要检查此对象的所有字段值 @Null 被注释的元素必须为 null @NotNull 被注释的元素必须不为 null @NotEmpty 被注释的字符串的必须非空, 即不为null, “” @NotBlank 被注释的字符串的必须非空, 即不为null, “”, “ “ @AssertTrue 被注释的元素必须为 true @AssertFalse 被注释的元素必须为 false @Min(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @Size(max, min) 被注释的元素的大小必须在指定的范围内 @Digits (integer, fraction) 被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past 被注释的元素必须是一个过去的日期 @Future 被注释的元素必须是一个将来的日期 @Pattern(value) 被注释的元素必须符合指定的正则表达式 @Email 被注释的元素必须是电子邮箱地址 @Length(min=, max=) 被注释的字符串的大小必须在指定的范围内 @Range(min=, max=) 被注释的元素必须在合适的范围 简单的实体校验123456789101112131415161718public class CardDTO &#123; @NotBlank private String cardId; @Size(min = 10, max = 10) @NotNull private String cardNum; // 卡号 @Past @NotNull private Date createDate; @Range(max = 3) private String cardType; // 省略get set&#125; 123456789@RestControllerpublic class UserController &#123; @PostMapping(&quot;simple&quot;) public Object simple(@RequestBody @Valid CardDTO cardDTO) &#123; return cardDTO; &#125;&#125; 实体属性上添加校验注解 controller 方法 参数前 使用@Valid 即可 复杂的实体校验嵌套实体校验123456789101112131415public class UserDTO &#123; @NotBlank private String userId; @NotBlank private String username; private String password; @Valid private List&lt;CardDTO&gt; cardList; //省略 get set&#125; controller 写法 同上，只是在 UserDTO cardList 属性上标记@Valid 注解 即可。 List 校验 我们需要像 嵌套校验 时一样，对List&lt;CardDTO&gt; 做一层封装 123456789101112131415public class ValidList&lt;E&gt; implements List&lt;E&gt; &#123; @Valid private List&lt;E&gt; list = new ArrayList&lt;&gt;(); public List&lt;E&gt; getList() &#123; return list; &#125; public void setList(List&lt;E&gt; list) &#123; this.list = list; &#125; // 省略了 实现方法&#125; 重写实现方法完全使用 this.list.xxx()Gitee:spring 会将数据封装到我们定义的 list 属性中，又将属性声明了 @Valid 使得 hibernate validator 可以为我们做校验！ 使用 @Validated 分组校验12345public interface Insert &#123;&#125;public interface Update &#123;&#125; 定义两个接口 12345678910111213public class GroupCardDTO &#123; @NotBlank(groups = &#123;Update.class&#125;) private String id; @NotBlank(groups = &#123;Insert.class&#125;) private String cardNum; @NotNull(groups = &#123;Insert.class, Update.class&#125;) private Integer cardType; //省略 get set&#125; 实体标记的注解中添加 group 属性 1234@PostMapping(&quot;insert_card&quot;)public Object insert_card(@RequestBody @Validated(Insert.class) GroupCardDTO card)&#123; return card;&#125; 使用 @Validated(xxx.class) 标记参数，完成分组校验！ 自定义注解校验当 validator 提供的注解无法满足我们的业务需求，可以通过自定义的方式来实现校验。 需求：校验某字符串必须为大写或者小写1234public enum CaseMode &#123; UPPER, LOWER&#125; 定义一个枚举类 12345678910111213141516171819import javax.validation.Constraint;import javax.validation.Payload;import java.lang.annotation.*;@Target( &#123; ElementType.FIELD &#125;)@Retention(RetentionPolicy.RUNTIME)@Constraint(validatedBy = CheckCaseValidator.class)@Documentedpublic @interface CheckCase &#123; String message() default ""; Class&lt;?&gt;[] groups() default &#123;&#125;; Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;; CaseMode value() default CaseMode.LOWER;&#125; 定义注解 @Constraint 指定我们的校验逻辑实现类 12345678910111213141516171819202122232425262728293031import javax.validation.ConstraintValidator;import javax.validation.ConstraintValidatorContext;public class CheckCaseValidator implements ConstraintValidator&lt;CheckCase, String&gt; &#123; private CaseMode caseMode; @Override public void initialize(CheckCase constraintAnnotation) &#123; this.caseMode = constraintAnnotation.value(); &#125; @Override public boolean isValid(String value, ConstraintValidatorContext context) &#123; if (value == null || "".equals(value.trim())) &#123; return false; &#125; switch (this.caseMode) &#123; case LOWER: return value.equals(value.toLowerCase()); case UPPER: return value.equals(value.toUpperCase()); default: return false; &#125; &#125;&#125; initialize() 初始化时执行，可以用来获取注解中的属性 isValid() 实现我们的校验逻辑 备注 我们自定义的注解依然支持 @Validated group 分组 手动使用 validator 校验123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import org.hibernate.validator.internal.engine.path.PathImpl;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Set;import javax.validation.ConstraintViolation;import javax.validation.Validation;import javax.validation.Validator;import javax.validation.ValidatorFactory;public class ValidateUtil &#123; /** * 校验实体类 * * @param t * @return */ public static &lt;T&gt; List&lt;Map&gt; validate(T t) &#123; //定义返回错误List List&lt;Map&gt; errList = new ArrayList&lt;&gt;(); Map&lt;String, String&gt; errorMap; ValidatorFactory factory = Validation.buildDefaultValidatorFactory(); Validator validator = factory.getValidator(); Set&lt;ConstraintViolation&lt;T&gt;&gt; errorSet = validator.validate(t); for (ConstraintViolation&lt;T&gt; c : errorSet) &#123; errorMap = new HashMap&lt;&gt;(); errorMap.put("field", c.getPropertyPath().toString()); //获取发生错误的字段 errorMap.put("msg", c.getMessage()); //获取校验信息 errList.add(errorMap); &#125; return errList; &#125; /** * 使用 ValidList 校验List, 返回对应索引和错误消息 * * @param t * @param &lt;T&gt; * @return */ public static &lt;T&gt; List&lt;Map&gt; validateList(T t) &#123; //定义返回错误List List&lt;Map&gt; errList = new ArrayList&lt;&gt;(); Map&lt;String, Object&gt; errorMap; ValidatorFactory factory = Validation.buildDefaultValidatorFactory(); Validator validator = factory.getValidator(); Set&lt;ConstraintViolation&lt;T&gt;&gt; errorSet = validator.validate(t); for (ConstraintViolation&lt;T&gt; c : errorSet) &#123; errorMap = new HashMap&lt;&gt;(); int index = ((PathImpl) c.getPropertyPath()).getLeafNode().getIndex(); errorMap.put("index", index); // 当前索引 errorMap.put("field", c.getPropertyPath().toString()); //获取发生错误的字段 errorMap.put("msg", c.getMessage()); //获取校验信息 errList.add(errorMap); &#125; return errList; &#125;&#125; 本节源码https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/springboot-validate]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JWT 鉴权]]></title>
    <url>%2Fpost%2F606cdcbb.html</url>
    <content type="text"><![CDATA[JWT 是什么 JSON Web Token（JWT）是一个开放式标准（RFC 7519），它定义了一种紧凑且自包含的方式，用于在各方之间以JSON对象安全传输信息。这些信息可以通过数字签名进行验证和信任。 JWT 的组成JWT 的格式为 xxx.yyy.zzz。包含 Header（头部），Payload（负载），Signature（签名）三部分。 Header通常会声明使用的加密算法和token类型 1234&#123; &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot;&#125; PayloadPayload 包含 Claims，Claim是一些实体（一般都是用户）的状态和额外的数据组成。JWT 为我们预定义了一些 Claims，例如：iss (issuer), exp (expiration time), sub (subject), aud (audience), and others.但是并不要求我们强制使用，我们也可以根据需求自定义 Claim 12345&#123; &quot;sub&quot;: &quot;1234567890&quot;, &quot;name&quot;: &quot;Taven&quot;, &quot;admin&quot;: true&#125; Signature 1234HMACSHA256( base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload), secret) 这段伪代码已经很好的讲解了，签名是如何计算的。服务端提供一个secret，将header、payload 进行base64编码，然后使用header中声明的算法计算签名。 JWT 与 session 对比 性能：JWT 轻量级，而session 会占用大量服务端内存； 部署：使用session的系统 负载均衡需要考虑 session 共享，而JWT 不需要； 安全：安全性的话，小弟不敢多说，我觉得五五开吧，有人说 JWT 泄露的话，直接就可以登录了。但是使用 session 如果请求被拦截了都是一样的； 场景：APP 中由于没有cookie吧，多是使用token。而 web 应用的话，使用 token 和 session 都是可以的。 实现思路简单一句话就是，登录之后服务端给客户端颁发JWT，客户端将 JWT 放在请求头中，服务端 filter 校验http header中的JWT。 查阅了一些资料后发现一些分歧 方案1： 有的朋友认为服务端不需要存储 JWT，只在 filter 校验的时候解析无误，即认为 JWT 是可用的。在需要重置 JWT 的时候（例如注销），客户端主动删除JWT。这种做法引发的问题，例如在注销了之后，token 依然可用，由于服务端没有存储 token，无法去校验。于是乎有了方案2 方案2：服务端数据库中存储 JWT，filter 每次校验 token 与数据库中是否一致，这种做法虽然稳妥，但是在性能上一定是不如方案1的。 demohttps://gitee.com/yintianwen7/taven-springboot-learning/tree/master/spring-boot-jwt]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>JWT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot Atomikos 多数据源分布式事务]]></title>
    <url>%2Fpost%2F2727b3ac.html</url>
    <content type="text"><![CDATA[之前的 《spring 动态切换、添加数据源实现以及源码浅析》 中介绍了如何使用 spring 提供的 AbstractRoutingDataSource 配置多数据源，有了多数据源自然要管理事务的一致性。上篇文章中提到过配置多数据源的两种方式 使用AbstractRoutingDataSource 配置多个 SqlSessionFactory 前言粗略阅读了一下spring的源码，由于 spring 事务的机制，在开启事务之前spring 会去创建当前数据源的 事务object，直到事务提交，spring 都不会在乎你是否切换了数据源。这就导致了，使用 AbstractRouting DataSource 方式开启事务时，切换数据源不生效。关于如何解决这个问题，感兴趣的朋友可以去阅读一下：https://www.jianshu.com/p/61e8961c6154 本文只讨论上述第二种方式结合 atomikos 管理多数据源事务。 Atomikos来自：http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-jta.html Atomikos is a popular open source transaction manager which can be embedded into your Spring Boot application. You can use thespring-boot-starter-jta-atomikos Starter to pull in the appropriate Atomikos libraries. Spring Boot auto-configures Atomikos and ensures that appropriate depends-on settings are applied to your Spring beans for correct startup and shutdown ordering. By default, Atomikos transaction logs are written to a transaction-logs directory in your application’s home directory (the directory in which your application jar file resides). You can customize the location of this directory by setting a spring.jta.log-dir property in your application.properties file. Properties starting with spring.jta.atomikos.properties can also be used to customize the Atomikos UserTransactionServiceImp. See the AtomikosProperties Javadoc for complete details. 引入spring-boot-starter-jta-atomikos，spring boot 为我们自动配置Atomikos，我们可以通过 spring.jta.xxx 修改默认配置。 Talk is cheap. Show me the code demo源码：https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/spring-atomikos 添加 maven 依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jta-atomikos&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt; application.properties 1234567891011121314151617181920212223242526272829303132333435363738394041424344#spring.jta.log-dir=classpath:tx-logsspring.jta.transaction-manager-id=txManagerspring.datasource.druid.system-db.name=system-dbspring.datasource.druid.system-db.url=jdbc:mysql://localhost:3306/test1?useSSL=falsespring.datasource.druid.system-db.username=rootspring.datasource.druid.system-db.password=taven753spring.datasource.druid.system-db.initialSize=5spring.datasource.druid.system-db.minIdle=5spring.datasource.druid.system-db.maxActive=20spring.datasource.druid.system-db.maxWait=60000spring.datasource.druid.system-db.timeBetweenEvictionRunsMillis=60000spring.datasource.druid.system-db.minEvictableIdleTimeMillis=30000spring.datasource.druid.system-db.validationQuery=SELECT 1spring.datasource.druid.system-db.validationQueryTimeout=10000spring.datasource.druid.system-db.testWhileIdle=truespring.datasource.druid.system-db.testOnBorrow=falsespring.datasource.druid.system-db.testOnReturn=falsespring.datasource.druid.system-db.poolPreparedStatements=truespring.datasource.druid.system-db.maxPoolPreparedStatementPerConnectionSize=20spring.datasource.druid.system-db.filters=stat,wallspring.datasource.druid.system-db.connectionProperties=druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000spring.datasource.druid.system-db.useGlobalDataSourceStat=truespring.datasource.druid.business-db.name=business-dbspring.datasource.druid.business-db.url=jdbc:mysql://localhost:3306/test2?useSSL=falsespring.datasource.druid.business-db.username=rootspring.datasource.druid.business-db.password=taven753spring.datasource.druid.business-db.initialSize=5spring.datasource.druid.business-db.minIdle=5spring.datasource.druid.business-db.maxActive=20spring.datasource.druid.business-db.maxWait=60000spring.datasource.druid.business-db.timeBetweenEvictionRunsMillis=60000spring.datasource.druid.business-db.minEvictableIdleTimeMillis=30000spring.datasource.druid.business-db.validationQuery=SELECT 1spring.datasource.druid.business-db.validationQueryTimeout=10000spring.datasource.druid.business-db.testWhileIdle=truespring.datasource.druid.business-db.testOnBorrow=falsespring.datasource.druid.business-db.testOnReturn=falsespring.datasource.druid.business-db.poolPreparedStatements=truespring.datasource.druid.business-db.maxPoolPreparedStatementPerConnectionSize=20spring.datasource.druid.business-db.filters=stat,wallspring.datasource.druid.business-db.connectionProperties=druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000spring.datasource.druid.business-db.useGlobalDataSourceStat=true system 数据源的配置类 12345678910111213141516171819202122232425262728293031323334353637383940414243import javax.sql.DataSource;import org.apache.ibatis.session.SqlSessionFactory;import org.mybatis.spring.SqlSessionFactoryBean;import org.mybatis.spring.annotation.MapperScan;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.jta.atomikos.AtomikosDataSourceBean;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Primary;import com.gitee.taven.config.prop.SystemProperties;import com.gitee.taven.utils.PojoUtil;@Configuration@MapperScan(basePackages = SystemDataSourceConfig.PACKAGE, sqlSessionFactoryRef = "systemSqlSessionFactory")public class SystemDataSourceConfig &#123; static final String PACKAGE = "com.gitee.taven.mapper.system"; @Autowired private SystemProperties systemProperties; @Bean(name = "systemDataSource") @Primary public DataSource systemDataSource() &#123; AtomikosDataSourceBean ds = new AtomikosDataSourceBean(); ds.setXaProperties(PojoUtil.obj2Properties(systemProperties)); ds.setXaDataSourceClassName("com.alibaba.druid.pool.xa.DruidXADataSource"); ds.setUniqueResourceName("systemDataSource"); ds.setPoolSize(5); return ds; &#125; @Bean @Primary public SqlSessionFactory systemSqlSessionFactory() throws Exception &#123; SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); sqlSessionFactoryBean.setDataSource(systemDataSource()); return sqlSessionFactoryBean.getObject(); &#125; &#125; business 数据源的配置类同上 省略了 mybatis 代码，通过service 测试事务，抛出异常后，事务会回滚 1234567891011121314151617181920212223@Servicepublic class UserService &#123; @Autowired private UsersMapper usersMapper; @Autowired private UserInformationsMapper userInformationsMapper; @Transactional public void testJTA() &#123; Users u = new Users(); u.setUsername("hmj"); u.setPassword("hmjbest"); usersMapper.insertSelective(u); UserInformations ui = new UserInformations(); ui.setUserid(666l); ui.setEmail("dsb"); userInformationsMapper.insertSelective(ui); // int i = 10/0; &#125; &#125; demo源码：https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/spring-atomikos]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>多数据源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 多线程异步调用---提高程序执行效率]]></title>
    <url>%2Fpost%2F574b9274.html</url>
    <content type="text"><![CDATA[原文：https://spring.io/guides/gs/async-method/ 为什么要用异步？当需要调用多个服务时，使用传统的同步调用来执行时，是这样的 调用服务A 等待服务A的响应 调用服务B 等待服务B的响应 调用服务C 等待服务C的响应 根据从服务A、服务B和服务C返回的数据完成业务逻辑，然后结束 如果每个服务需要3秒的响应时间，这样顺序执行下来，可能需要9秒以上才能完成业务逻辑，但是如果我们使用异步调用 调用服务A 调用服务B 调用服务C 然后等待从服务A、B和C的响应 根据从服务A、服务B和服务C返回的数据完成业务逻辑，然后结束 理论上 3秒左右即可完成同样的业务逻辑 Talk is cheap. Show me the code123456789101112131415161718192021222324252627public class User &#123; private String name; private String blog; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getBlog() &#123; return blog; &#125; public void setBlog(String blog) &#123; this.blog = blog; &#125; @Override public String toString() &#123; return "User [name=" + name + ", blog=" + blog + "]"; &#125;&#125; 12345678910111213141516171819202122232425262728293031import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.boot.web.client.RestTemplateBuilder;import org.springframework.scheduling.annotation.Async;import org.springframework.stereotype.Service;import org.springframework.web.client.RestTemplate;import java.util.concurrent.CompletableFuture;@Servicepublic class GitHubLookupService &#123; private static final Logger logger = LoggerFactory.getLogger(GitHubLookupService.class); private final RestTemplate restTemplate; public GitHubLookupService(RestTemplateBuilder restTemplateBuilder) &#123; this.restTemplate = restTemplateBuilder.build(); &#125; @Async public CompletableFuture&lt;User&gt; findUser(String user) throws InterruptedException &#123; logger.info("Looking up " + user); String url = String.format("https://api.github.com/users/%s", user); User results = restTemplate.getForObject(url, User.class); // Artificial delay of 3s for demonstration purposes Thread.sleep(3000L); return CompletableFuture.completedFuture(results); &#125;&#125; The findUser method is flagged with Spring’s @Async annotation, indicating it will run on a separate thread. The method’s return type is CompletableFuture&lt;User&gt; instead of User, a requirement for any asynchronous service. findUser 方法被标记为Spring的 @Async 注解，表示它将在一个单独的线程上运行。该方法的返回类型是 CompleetableFuture&lt;user&gt; 而不是 User，这是任何异步服务的要求。 1234567891011121314151617181920212223242526272829import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.Bean;import org.springframework.scheduling.annotation.EnableAsync;import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;import java.util.concurrent.Executor;@SpringBootApplication@EnableAsyncpublic class App &#123; public static void main(String[] args) &#123; // close the application context to shut down the custom ExecutorService SpringApplication.run(App.class, args).close(); &#125; @Bean public Executor asyncExecutor() &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(2); executor.setMaxPoolSize(2); executor.setQueueCapacity(500); executor.setThreadNamePrefix("GithubLookup-"); executor.initialize(); return executor; &#125;&#125; The @EnableAsync annotation switches on Spring’s ability to run @Async methods in a background thread pool. This class also customizes the used Executor. In our case, we want to limit the number of concurrent threads to 2 and limit the size of the queue to 500. There are many more things you can tune. By default, a SimpleAsyncTaskExecutor is used. @EnableAsync 注解开启Spring在后台线程池中运行 @Async 方法的能力。该类也可以自定义使用的 Executor。在我们的示例中，我们希望将并发线程的数量限制为2，并将队列的大小限制为500。有很多你可以配置的东西)。默认情况下，使用SimpleAsyncTaskExecutor。 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.util.concurrent.CompletableFuture;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.boot.CommandLineRunner;import org.springframework.stereotype.Component;import com.gitee.taven.entity.User;import com.gitee.taven.service.GitHubLookupService;@Componentpublic class AppRunner implements CommandLineRunner &#123; private static final Logger logger = LoggerFactory.getLogger(AppRunner.class); private final GitHubLookupService gitHubLookupService; public AppRunner(GitHubLookupService gitHubLookupService) &#123; this.gitHubLookupService = gitHubLookupService; &#125; @Override public void run(String... args) throws Exception &#123; // Start the clock long start = System.currentTimeMillis(); // Kick of multiple, asynchronous lookups CompletableFuture&lt;User&gt; page1 = gitHubLookupService.findUser("PivotalSoftware"); CompletableFuture&lt;User&gt; page2 = gitHubLookupService.findUser("CloudFoundry"); CompletableFuture&lt;User&gt; page3 = gitHubLookupService.findUser("Spring-Projects"); // Wait until they are all done CompletableFuture.allOf(page1,page2,page3).join(); // Print results, including elapsed time float exc = (float)(System.currentTimeMillis() - start)/1000; logger.info("Elapsed time: " + exc + " seconds"); logger.info("--&gt; " + page1.get()); logger.info("--&gt; " + page2.get()); logger.info("--&gt; " + page3.get()); &#125; &#125; 通过实现 CommandLineRunner 调用 service 服务，我们设置了 Thread.sleep(3000L); 运行demo，4.73s 结束战斗！ 本文demohttps://gitee.com/yintianwen7/taven-springboot-learning/tree/master/spring-async-demo]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>多线程</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 动态切换、添加数据源实现以及源码浅析]]></title>
    <url>%2Fpost%2F9ffabefe.html</url>
    <content type="text"><![CDATA[公司项目需求，由于要兼容老系统的数据库结构，需要搭建一个 可以动态切换、添加数据源的后端服务。 分析参考了过去的项目，通过配置多个SqlSessionFactory 来实现多数据源，这么做的话，未免过于笨重，而且无法实现动态添加数据源这个需求通过 spring AbstractRoutingDataSource 为我们抽象了一个 DynamicDataSource 解决这一问题 源码简单分析下 AbstractRoutingDataSource 的源码 targetDataSources 就是我们的多个数据源，在初始化的时候会调用afterPropertiesSet()，去解析我们的数据源 然后 put 到 resolvedDataSources 实现了 DataSource 的 getConnection(); 我们看看 determineTargetDataSource(); 做了什么 通过下面的 determineCurrentLookupKey();（这个方法需要我们实现） 返回一个key，然后从 resolvedDataSources （其实也就是 targetDataSources） 中 get 一个数据源，实现了每次调用 getConnection(); 打开连接 切换数据源，如果想动态添加的话 只需要重新 set targetDataSources 再调用 afterPropertiesSet() 即可 Talk is cheap. Show me the code我使用的springboot版本为 1.5.x，下面是核心代码完整代码：https://gitee.com/yintianwen7/spring-dynamic-datasource123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/** * 多数据源配置 * * @author Taven * */@Configuration@MapperScan("com.gitee.taven.mapper")public class DataSourceConfigurer &#123; /** * DataSource 自动配置并注册 * * @return data source */ @Bean("db0") @Primary @ConfigurationProperties(prefix = "datasource.db0") public DataSource dataSource0() &#123; return DruidDataSourceBuilder.create().build(); &#125; /** * DataSource 自动配置并注册 * * @return data source */ @Bean("db1") @ConfigurationProperties(prefix = "datasource.db1") public DataSource dataSource1() &#123; return DruidDataSourceBuilder.create().build(); &#125; /** * 注册动态数据源 * * @return */ @Bean("dynamicDataSource") public DataSource dynamicDataSource() &#123; DynamicRoutingDataSource dynamicRoutingDataSource = new DynamicRoutingDataSource(); Map&lt;Object, Object&gt; dataSourceMap = new HashMap&lt;&gt;(); dataSourceMap.put("dynamic_db0", dataSource0()); dataSourceMap.put("dynamic_db1", dataSource1()); dynamicRoutingDataSource.setDefaultTargetDataSource(dataSource0());// 设置默认数据源 dynamicRoutingDataSource.setTargetDataSources(dataSourceMap); return dynamicRoutingDataSource; &#125; /** * Sql session factory bean. * Here to config datasource for SqlSessionFactory * &lt;p&gt; * You need to add @&#123;@code @ConfigurationProperties(prefix = "mybatis")&#125;, if you are using *.xml file, * the &#123;@code 'mybatis.type-aliases-package'&#125; and &#123;@code 'mybatis.mapper-locations'&#125; should be set in * &#123;@code 'application.properties'&#125; file, or there will appear invalid bond statement exception * * @return the sql session factory bean */ @Bean @ConfigurationProperties(prefix = "mybatis") public SqlSessionFactoryBean sqlSessionFactoryBean() &#123; SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); // 必须将动态数据源添加到 sqlSessionFactoryBean sqlSessionFactoryBean.setDataSource(dynamicDataSource()); return sqlSessionFactoryBean; &#125; /** * 事务管理器 * * @return the platform transaction manager */ @Bean public PlatformTransactionManager transactionManager() &#123; return new DataSourceTransactionManager(dynamicDataSource()); &#125;&#125; 通过 ThreadLocal 获取线程安全的数据源 key12345678910111213141516171819202122232425262728293031323334353637package com.gitee.taven.config;public class DynamicDataSourceContextHolder &#123; private static final ThreadLocal&lt;String&gt; contextHolder = new ThreadLocal&lt;String&gt;() &#123; @Override protected String initialValue() &#123; return "dynamic_db0"; &#125; &#125;; /** * To switch DataSource * * @param key the key */ public static void setDataSourceKey(String key) &#123; contextHolder.set(key); &#125; /** * Get current DataSource * * @return data source key */ public static String getDataSourceKey() &#123; return contextHolder.get(); &#125; /** * To set DataSource as default */ public static void clearDataSourceKey() &#123; contextHolder.remove(); &#125;&#125; 动态 添加、切换数据源123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * 动态数据源 * * @author Taven * */public class DynamicRoutingDataSource extends AbstractRoutingDataSource &#123; private final Logger logger = LoggerFactory.getLogger(getClass()); private static Map&lt;Object, Object&gt; targetDataSources = new HashMap&lt;&gt;(); /** * 设置当前数据源 * * @return */ @Override protected Object determineCurrentLookupKey() &#123; logger.info("Current DataSource is [&#123;&#125;]", DynamicDataSourceContextHolder.getDataSourceKey()); return DynamicDataSourceContextHolder.getDataSourceKey(); &#125; @Override public void setTargetDataSources(Map&lt;Object, Object&gt; targetDataSources) &#123; super.setTargetDataSources(targetDataSources); DynamicRoutingDataSource.targetDataSources = targetDataSources; &#125; /** * 是否存在当前key的 DataSource * * @param key * @return 存在返回 true, 不存在返回 false */ public static boolean isExistDataSource(String key) &#123; return targetDataSources.containsKey(key); &#125; /** * 动态增加数据源 * * @param map 数据源属性 * @return */ public synchronized boolean addDataSource(Map&lt;String, String&gt; map) &#123; try &#123; Connection connection = null; // 排除连接不上的错误 try &#123; Class.forName(map.get(DruidDataSourceFactory.PROP_DRIVERCLASSNAME)); connection = DriverManager.getConnection( map.get(DruidDataSourceFactory.PROP_URL), map.get(DruidDataSourceFactory.PROP_USERNAME), map.get(DruidDataSourceFactory.PROP_PASSWORD)); System.out.println(connection.isClosed()); &#125; catch (Exception e) &#123; return false; &#125; finally &#123; if (connection != null &amp;&amp; !connection.isClosed()) connection.close(); &#125; String database = map.get("database");//获取要添加的数据库名 if (StringUtils.isBlank(database)) return false; if (DynamicRoutingDataSource.isExistDataSource(database)) return true; DruidDataSource druidDataSource = (DruidDataSource) DruidDataSourceFactory.createDataSource(map); druidDataSource.init(); Map&lt;Object, Object&gt; targetMap = DynamicRoutingDataSource.targetDataSources; targetMap.put(database, druidDataSource); // 当前 targetDataSources 与 父类 targetDataSources 为同一对象 所以不需要set// this.setTargetDataSources(targetMap); this.afterPropertiesSet(); logger.info("dataSource &#123;&#125; has been added", database); &#125; catch (Exception e) &#123; logger.error(e.getMessage()); return false; &#125; return true; &#125; &#125; 可以通过 AOP 或者 手动 DynamicDataSourceContextHolder.setDataSourceKey(String key) 切换数据源 需要注意的：当我们开启了事务之后，是无法在去切换数据源的 本文项目源码：https://gitee.com/yintianwen7/spring-dynamic-datasource参考文献：https://github.com/helloworlde/SpringBoot-DynamicDataSource]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>多数据源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Service 调用当前类方法事务不生效]]></title>
    <url>%2Fpost%2Ff2e584e4.html</url>
    <content type="text"><![CDATA[今天在测试框架的时候，我想在一个service类的方法中调用 当前类的另一个方法（该方法通过@Transactional开启事务），这时候发现被调用类的事务并没有生效。 1234567891011public boolean test1() &#123; // xxx 业务逻辑 return test2();&#125;@Transactionalpublic boolean test2() &#123; testMapper.insertSalary(&quot;test&quot;, UUID.randomUUID().toString()); int a = 10/0; return true;&#125; WHY? 搜索引擎一番查询之后，了解到问题的关键：@Transactional 是基于aop生的代理对象开启事务的PS:不了解代理模式的小伙伴，结尾有传送门 思路 spring 的事务是通过 aop 管理的 aop 会通过动态代理 为我们生成代理对象，aop 的功能（例如事务）都是在代理对象中实现的 aop 生成的代理类又在 spring 容器中，所以我们只要在 spring 容器中拿到当前这个bean 再去调用 test2() 就可以开启事务了。 解决123456789101112131415161718192021222324252627282930313233343536373839404142import org.springframework.beans.BeansException;import org.springframework.context.ApplicationContext;import org.springframework.context.ApplicationContextAware;import org.springframework.stereotype.Component;/** * Spring的ApplicationContext的持有者,可以用静态方法的方式获取spring容器中的bean * */@Componentpublic class SpringContextHolder implements ApplicationContextAware &#123; private static ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; SpringContextHolder.applicationContext = applicationContext; &#125; public static ApplicationContext getApplicationContext() &#123; assertApplicationContext(); return applicationContext; &#125; @SuppressWarnings("unchecked") public static &lt;T&gt; T getBean(String beanName) &#123; assertApplicationContext(); return (T) applicationContext.getBean(beanName); &#125; public static &lt;T&gt; T getBean(Class&lt;T&gt; requiredType) &#123; assertApplicationContext(); return applicationContext.getBean(requiredType); &#125; private static void assertApplicationContext() &#123; if (SpringContextHolder.applicationContext == null) &#123; throw new RuntimeException("applicaitonContext属性为null,请检查是否注入了SpringContextHolder!"); &#125; &#125;&#125; 123456789101112public boolean test1() &#123; // xxx 业务逻辑 // 在spring容器中 获取当前类的代理类 return SpringContextHolder.getBean(TestS.class).test2();&#125;@Transactionalpublic boolean test2() &#123; testMapper.insertSalary(&quot;test&quot;, UUID.randomUUID().toString()); int a = 10/0; return true;&#125; ok！搞定！ 传送门Spring AOP的实现原理Java 代理模式]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)Java 代理模式]]></title>
    <url>%2Fpost%2F4dd31fbe.html</url>
    <content type="text"><![CDATA[原文：https://www.cnblogs.com/cenyu/p/6289209.html 代理(Proxy)是一种设计模式，通过代理对象访问目标对象.这样做的好处是:可以在目标对象实现的基础上，扩展目标对象的功能。 举个例子来说明：假如说我现在想买一辆二手车，最便捷的方法一定是我去找中介，他们来给我做琐碎的事情，我只是负责选择自己喜欢的车，然后付钱就可以了。 代理对象是对目标对象的扩展,并会调用目标对象 静态代理12345public interface IUserDao &#123; void save(); &#125; 12345678public class UserDao implements IUserDao &#123; @Override public void save() &#123; System.out.println(&quot;----已经保存数据!----&quot;); &#125;&#125; 12345678910111213141516public class UserDaoProxy implements IUserDao &#123; //目标对象 private IUserDao target; public UserDaoProxy(IUserDao target)&#123; this.target=target; &#125; @Override public void save() &#123; System.out.println(&quot;开始事务...&quot;); target.save();//执行目标对象的方法 System.out.println(&quot;提交事务...&quot;); &#125;&#125; 12345678@Testpublic void staticProxy() &#123; //目标对象 UserDao target = new UserDao(); //代理对象，建立代理关系 UserDaoProxy proxy = new UserDaoProxy(target); proxy.save();//执行的是代理的方法&#125; 静态代理的缺点：接口新增方法时，类过多时，需要手动维护，过于繁琐，但是通过动态代理机制可以解决这一问题。 动态代理spring 中 AOP 通过 动态代理 在运行时为我们的代码增强，例如我们在开发的时候只需要做业务逻辑，AOP 通过动态代理可以为我们做事务，日志管理，权限校验等等。 jdk 动态代理我们不需要再去手动创建代理类（但是要求被代理类必须实现一个接口），只需要做一个动态代理工厂即可，jdk通过反射为我们在内存中动态创建代理对象，以及在方法执行的前后添加通知。1234567891011121314151617181920212223242526272829303132333435import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;/** * 创建动态代理对象 * */public class JdkProxyFactory&#123; //维护一个目标对象 private Object target; public JdkProxyFactory(Object target)&#123; this.target=target; &#125; //给目标对象生成代理对象 public Object getProxyInstance()&#123; return Proxy.newProxyInstance( target.getClass().getClassLoader(), target.getClass().getInterfaces(), new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("开始事务2"); //执行目标对象方法 Object returnValue = method.invoke(target, args); System.out.println("提交事务2"); return returnValue; &#125; &#125; ); &#125;&#125; 123456789101112131415@Testpublic void jdkProxy() &#123; // 目标对象 IUserDao target = new UserDao(); // 原始类型 System.out.println(&quot;原始类型&quot; + target.getClass()); // 给目标对象，创建代理对象 IUserDao proxy = (IUserDao) new JdkProxyFactory(target).getProxyInstance(); // class $Proxy0 内存中动态生成的代理对象 System.out.println(&quot;动态代理后对象类型:&quot; + proxy.getClass()); // 代理对象 执行方法 proxy.save();&#125; 执行测试方法12345原始类型class com.example.demo.original_code.UserDao动态代理后对象类型:class com.sun.proxy.$Proxy4开始事务2----已经保存数据!----提交事务2 cglib 动态代理静态代理 和 jdk代理 要求目标对象必须实现接口，而 cglib 动态代理无需实现接口。cglib 会动态为目标对象 创建一个子类对象。使用须知：1.被代理的类不能为 final2.被代理类的方法 不能为 final/static，否则无法被拦截12345678910111213141516171819202122232425262728293031323334353637383940414243import java.lang.reflect.Method;import org.springframework.cglib.proxy.Enhancer;import org.springframework.cglib.proxy.MethodInterceptor;import org.springframework.cglib.proxy.MethodProxy;/** * Cglib子类代理工厂 * 对UserDao在内存中动态构建一个子类对象 */public class CglibProxyFactory implements MethodInterceptor &#123; //维护目标对象 private Object target; public CglibProxyFactory(Object target) &#123; this.target = target; &#125; //给目标对象创建一个代理对象 public Object getProxyInstance()&#123; //1.工具类 Enhancer en = new Enhancer(); //2.设置父类 en.setSuperclass(target.getClass()); //3.设置回调函数 en.setCallback(this); //4.创建子类(代理对象) return en.create(); &#125; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; System.out.println("开始事务..."); //执行目标对象的方法 Object returnValue = method.invoke(target, args); System.out.println("提交事务..."); return returnValue; &#125;&#125; 1234567891011@Testpublic void cglibProxy() &#123; //目标对象 UserDao target = new UserDao(); //代理对象 UserDao proxy = (UserDao) new CglibProxyFactory(target).getProxyInstance(); //执行代理对象的方法 proxy.save();&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-查询树结构]]></title>
    <url>%2Fpost%2Fa998078e.html</url>
    <content type="text"><![CDATA[在 oracle 数据库中，通过 start with connect by prior 递归可以直接查出树结构，但是在 mysql 当中如何解决树查询问题呢？ #####思路：我们可以通过自定义函数，遍历找出某一节点的所有子节点 （或者某一节点的所有父节点）的字符串集合。然后通过 FIND_IN_SET 函数，这就查出了我们想要的树 #####实践：1）建表 以及 测试数据准备123456CREATE TABLE `tree` ( `id` int(11) NOT NULL, `pid` int(11) DEFAULT NULL, `name` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 12345678INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;1&apos;, NULL, &apos;一级&apos;);INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;2&apos;, &apos;1&apos;, &apos;二级1&apos;);INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;3&apos;, &apos;2&apos;, &apos;三级1&apos;);INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;4&apos;, &apos;3&apos;, &apos;四级1&apos;);INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;5&apos;, &apos;4&apos;, &apos;五级&apos;);INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;6&apos;, &apos;1&apos;, &apos;三级2&apos;);INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;7&apos;, &apos;1&apos;, &apos;二级2&apos;);INSERT INTO `wabc`.`tree` (`id`, `pid`, `name`) VALUES (&apos;8&apos;, &apos;6&apos;, &apos;四级2&apos;); 2）查询 某节点下所有子节点12345678910111213CREATE FUNCTION `GET_CHILD_NODE`(rootId varchar(100)) RETURNS varchar(2000) BEGIN DECLARE str varchar(2000); DECLARE cid varchar(100); SET str = &apos;$&apos;; SET cid = rootId; WHILE cid is not null DO SET str = concat(str, &apos;,&apos;, cid); SELECT group_concat(id) INTO cid FROM tree where FIND_IN_SET(pid, cid); END WHILE; RETURN str; END 第一次进入函数 cid 为根节点，开始循环后，cid 为每次查询结果集 （也就是子节点），下一次会查询出所有子节点的子节点 … 以此类推，当没有子节点时退出循环，也就得到了所有的节点。12345678910mysql&gt; SELECT * from tree where FIND_IN_SET(id, GET_CHILD_NODE(2));+----+-----+-------+| id | pid | name |+----+-----+-------+| 2 | 1 | 二级1 || 3 | 2 | 三级1 || 4 | 3 | 四级1 || 5 | 4 | 五级 |+----+-----+-------+4 rows in set 再通过这些节点 筛选，ok ! 3）查询 某节点的所有父节点1234567891011121314151617CREATE FUNCTION `GET_PARENT_NODE`(rootId varchar(100)) RETURNS varchar(1000) BEGIN DECLARE fid varchar(100) default &apos;&apos;; DECLARE str varchar(1000) default rootId; WHILE rootId is not null do SET fid =(SELECT pid FROM tree WHERE id = rootId); IF fid is not null THEN SET str = concat(str, &apos;,&apos;, fid); SET rootId = fid; ELSE SET rootId = fid; END IF; END WHILE; return str; END 和上一个函数类似，不断的遍历去找 父id，然后拼接到字符串中，为空退出循环。1234567891011mysql&gt; select * from tree where FIND_IN_SET(id, GET_PARENT_NODE(5));+----+------+-------+| id | pid | name |+----+------+-------+| 1 | NULL | 一级 || 2 | 1 | 二级1 || 3 | 2 | 三级1 || 4 | 3 | 四级1 || 5 | 4 | 五级 |+----+------+-------+5 rows in set]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Dijkstra 算法实现原理]]></title>
    <url>%2Fpost%2F9186820e.html</url>
    <content type="text"><![CDATA[迪杰斯特拉(Dijkstra)算法是典型最短路径算法，用于计算一个节点到其他节点的最短路径。它的主要特点是以起始点为中心向外层层扩展(广度优先搜索思想)，直到扩展到终点为止。 （嗯，第一段是抄的，由于本人算法的基础比较薄弱，我会尽量用通俗易懂的语言来让大家理解本文） 参考博客:数据结构–Dijkstra算法最清楚的讲解 大概就是这样一个有权图，Dijkstra算法可以计算任意节点到其他节点的最短路径 算法思路 指定一个节点，例如我们要计算 ‘A’ 到其他节点的最短路径 引入两个集合（S、U），S集合包含已求出的最短路径的点（以及相应的最短长度），U集合包含未求出最短路径的点（以及A到该点的路径，注意 如上图所示，A-&gt;C由于没有直接相连 初始时为∞） 初始化两个集合，S集合初始时 只有当前要计算的节点，A-&gt;A = 0，U集合初始时为 A-&gt;B = 4, A-&gt;C = ∞, A-&gt;D = 2, A-&gt;E = ∞，敲黑板！！！接下来要进行核心两步骤了 从U集合中找出路径最短的点，加入S集合，例如 A-&gt;D = 2 更新U集合路径，if ( &#39;D 到 B,C,E 的距离&#39; + &#39;AD 距离&#39; &lt; &#39;A 到 B,C,E 的距离&#39; ) 则更新U 循环执行 4、5 两步骤，直至遍历结束，得到A 到其他节点的最短路径算法图解1.选定A节点并初始化，如上述步骤3所示 2.执行上述 4、5两步骤，找出U集合中路径最短的节点D 加入S集合，并根据条件 if ( &#39;D 到 B,C,E 的距离&#39; + &#39;AD 距离&#39; &lt; &#39;A 到 B,C,E 的距离&#39; ) 来更新U集合 3.这时候 A-&gt;B, A-&gt;C 都为3，没关系。其实这时候他俩都是最短距离，如果从算法逻辑来讲的话，会先取到B点。而这个时候 if 条件变成了 if ( &#39;B 到 C,E 的距离&#39; + &#39;AB 距离&#39; &lt; &#39;A 到 C,E 的距离&#39; ) ，如图所示这时候A-&gt;B距离 其实为 A-&gt;D-&gt;B 思路就是这样，往后就是大同小异了 算法结束 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class Dijkstra &#123; public static final int M = 10000; // 代表正无穷 public static void main(String[] args) &#123; // 二维数组每一行分别是 A、B、C、D、E 各点到其余点的距离, // A -&gt; A 距离为0, 常量M 为正无穷 int[][] weight1 = &#123; &#123;0,4,M,2,M&#125;, &#123;4,0,4,1,M&#125;, &#123;M,4,0,1,3&#125;, &#123;2,1,1,0,7&#125;, &#123;M,M,3,7,0&#125; &#125;; int start = 0; int[] shortPath = dijkstra(weight1, start); for (int i = 0; i &lt; shortPath.length; i++) System.out.println(&quot;从&quot; + start + &quot;出发到&quot; + i + &quot;的最短距离为：&quot; + shortPath[i]); &#125; public static int[] dijkstra(int[][] weight, int start) &#123; // 接受一个有向图的权重矩阵，和一个起点编号start（从0编号，顶点存在数组中） // 返回一个int[] 数组，表示从start到它的最短路径长度 int n = weight.length; // 顶点个数 int[] shortPath = new int[n]; // 保存start到其他各点的最短路径 String[] path = new String[n]; // 保存start到其他各点最短路径的字符串表示 for (int i = 0; i &lt; n; i++) path[i] = new String(start + &quot;--&gt;&quot; + i); int[] visited = new int[n]; // 标记当前该顶点的最短路径是否已经求出,1表示已求出 // 初始化，第一个顶点已经求出 shortPath[start] = 0; visited[start] = 1; for (int count = 1; count &lt; n; count++) &#123; // 要加入n-1个顶点 int k = -1; // 选出一个距离初始顶点start最近的未标记顶点 int dmin = Integer.MAX_VALUE; for (int i = 0; i &lt; n; i++) &#123; if (visited[i] == 0 &amp;&amp; weight[start][i] &lt; dmin) &#123; dmin = weight[start][i]; k = i; &#125; &#125; // 将新选出的顶点标记为已求出最短路径，且到start的最短路径就是dmin shortPath[k] = dmin; visited[k] = 1; // 以k为中间点，修正从start到未访问各点的距离 for (int i = 0; i &lt; n; i++) &#123; //如果 &apos;起始点到当前点距离&apos; + &apos;当前点到某点距离&apos; &lt; &apos;起始点到某点距离&apos;, 则更新 if (visited[i] == 0 &amp;&amp; weight[start][k] + weight[k][i] &lt; weight[start][i]) &#123; weight[start][i] = weight[start][k] + weight[k][i]; path[i] = path[k] + &quot;--&gt;&quot; + i; &#125; &#125; &#125; for (int i = 0; i &lt; n; i++) &#123; System.out.println(&quot;从&quot; + start + &quot;出发到&quot; + i + &quot;的最短路径为：&quot; + path[i]); &#125; System.out.println(&quot;=====================================&quot;); return shortPath; &#125; &#125;]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>Dijkstra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot jackson（Date类型入参、格式化，以及如何处理null）]]></title>
    <url>%2Fpost%2F23d9d33.html</url>
    <content type="text"><![CDATA[首先，我们要知道 springboot 默认使用 jackson 解析 json（当然这里也是可以配置使用其他 json 解析框架）。在不配置其他 json 解析的情况下，我们可以通过 spring boot 提供的注解和配置 来让 jackson 帮助我们提高开发效率 使用 @ResponseBody @RequestBody， Date 类型对象入参，返回json格式化解决方法如下: 1. application.properties中加入如下代码12spring.jackson.date-format=yyyy-MM-dd HH:mm:ssspring.jackson.time-zone=GMT+8 2. 如果个别实体需要使用其他格式的 pattern，在实体上加入注解即可1234567import org.springframework.format.annotation.DateTimeFormat;import com.fasterxml.jackson.annotation.JsonFormat;public class MrType &#123; @JsonFormat(timezone = "GMT+8",pattern = "yyyy-MM-dd") @DateTimeFormat(pattern="yyyy-MM-dd") private Date createdDate;&#125; 关于spring boot 时间类型支持我做了以下测试：1) application.properties 配置注释，不添加注解：spring 无法接收时间参数（400），json 输出 &quot;2018-03-29T09:45:31.513+0000&quot;2) application.properties 配置开启，不添加注解：仅支持 yyyy-MM-dd HH:mm:ss 的格式参数和 json 输出3) application.properties 配置开启，实体添加 @JsonFormat(pattern = &quot;yyyy-MM-dd&quot;)，实体可接受 yyyy-MM-dd HH:mm:ss 和 yyyy-MM-dd 格式的参数，json输出格式为 yyyy-MM-dd，由此可见@JsonFormat是限制Date 类型 json 输出的，但是为什么对接受的类型也造成了影响？有待考证4) application.properties 配置开启，实体添加 @DateTimeFormat(pattern = &quot;yyyy-MM-dd&quot;)，结果与第二条测试一样？貌似@DateTimeFormat 注解并没有生效？有待考证5) application.properties 配置开启，实体添加 @JsonFormat 和 @DateTimeFormat 结果与第三条一样 结论：实际项目中 application.properties设置通用时间格式，个别属性需要特殊处理时，添加@JsonFormat（@JsonFormat 自己好像就把这件事搞定了） 使用 @ResponseBody 时 忽略 json 中值为null的属性1. application.properties中加入如下代码1spring.jackson.default-property-inclusion=non-null 或者在类上声明@JsonInclude(JsonInclude.Include.NON_NULL) 1234567import java.io.Serializable;import com.fasterxml.jackson.annotation.JsonInclude;@JsonInclude(JsonInclude.Include.NON_NULL)//该注解配合jackson，序列化时忽略 null属性public class AjaxResult implements Serializable &#123;&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>jackson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-并发下生成不重复流水号]]></title>
    <url>%2Fpost%2Fef3d0933.html</url>
    <content type="text"><![CDATA[更新于 2018-12-23 22:21:44前言：一年前的写的，当时的做法并不能在并发下保证流水号的唯一性，因为当时并没有写多线程测试过… 思路 sCode sName sQz sValue order 订单 DD 18120100 首先每个业务的流水号对应表中的一条数据 每个要获取流水号的线程调用 一个用来生成流水号的 存储过程 根据sCode 找到 sValue。1234if (sValue == null) 初始化值 else sValue ++ 敲黑板！划重点！ 分析：上述的思路看似没什么问题，但是如果两个线程并发的执行会出现两个事务读取到相同的数据，同时都执行初始化或者自增。 方案：使用 MySQL 写锁（select…for update，也叫X锁，排它锁） 来解决这一问题 例如：事务A和事务B同时进行，事务A 拿到 当前这条数据的写锁，此时如果事务B 想访问这条数据需要等待事务A结束。并发时让两个事务对同一条数据的操作变成了串行化。 123456CREATE TABLE `sys_sno` ( `sCode` varchar(50) DEFAULT NULL COMMENT &apos;编码&apos;, `sName` varchar(100) DEFAULT NULL COMMENT &apos;名称&apos;, `sQz` varchar(50) DEFAULT NULL COMMENT &apos;前缀&apos;, `sValue` varchar(80) DEFAULT NULL COMMENT &apos;值&apos;) ENGINE=InnoDB DEFAULT CHARSET=utf8; 123456789101112131415161718192021222324252627282930313233343536373839CREATE DEFINER=`root`@`localhost` PROCEDURE `GetSerialNo`(IN tsCode VARCHAR(50),OUT result VARCHAR(200) )BEGIN DECLARE tsValue VARCHAR(50); DECLARE tdToday VARCHAR(20); DECLARE nowdate VARCHAR(20); DECLARE tsQZ VARCHAR(50); DECLARE t_error INTEGER DEFAULT 0; DECLARE CONTINUE HANDLER FOR SQLEXCEPTION SET t_error=1; START TRANSACTION; /* UPDATE sys_sno SET sValue=sValue WHERE sCode=tsCode; */ SELECT sValue INTO tsValue FROM sys_sno WHERE sCode=tsCode for UPDATE; SELECT sQz INTO tsQZ FROM sys_sno WHERE sCode=tsCode ; -- 因子表中没有记录，插入初始值 IF tsValue IS NULL THEN SELECT CONCAT(DATE_FORMAT(NOW(),&apos;%y%m&apos;),&apos;0001&apos;) INTO tsValue; UPDATE sys_sno SET sValue=tsValue WHERE sCode=tsCode ; SELECT CONCAT(tsQZ,tsValue) INTO result; ELSE SELECT SUBSTRING(tsValue,1,4) INTO tdToday; SELECT CONVERT(DATE_FORMAT(NOW(),&apos;%y%m&apos;),SIGNED) INTO nowdate; -- 判断年月是否需要更新 IF tdToday = nowdate THEN SET tsValue=CONVERT(tsValue,SIGNED) + 1; ELSE SELECT CONCAT(DATE_FORMAT(NOW(),&apos;%y%m&apos;) ,&apos;0001&apos;) INTO tsValue ; END IF; UPDATE sys_sno SET sValue =tsValue WHERE sCode=tsCode; SELECT CONCAT(tsQZ,tsValue) INTO result; END IF; IF t_error =1 THEN ROLLBACK; SET result = &apos;Error&apos;; ELSE COMMIT; END IF; SELECT result ; END; 测试代码完整测试代码：https://gitee.com/yintianwen7/taven-springboot-learning/tree/master/uni-number123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import com.gitee.taven.uninumber.mapper.OrderMapper;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.HashMap;import java.util.Map;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;@Servicepublic class OrderService &#123; private static OrderMapper orderMapper; @Autowired public void setOrderMapper(OrderMapper orderMapper) &#123; this.orderMapper = orderMapper; &#125; private static final int TOTAL_THREADS = 100; public void multiThread() &#123; // 创建固定长度线程池 ExecutorService fixedThreadPool = Executors.newFixedThreadPool(50); for (int i = 0; i &lt; TOTAL_THREADS; i++) &#123; Thread thread = new OrderThread(); fixedThreadPool.execute(thread); &#125; &#125; public static class OrderThread extends Thread &#123; @Override public void run() &#123; try &#123; Map&lt;String, String&gt; parameterMap = initParameterMap(); orderMapper.createOrderNum(parameterMap); String number = parameterMap.get(&quot;result&quot;); System.out.println(Thread.currentThread().getName() + &quot; : &quot; +number); &#125; catch (Exception e) &#123; System.out.println(e); &#125; &#125; public Map&lt;String, String&gt; initParameterMap() &#123; Map&lt;String, String&gt; parameterMap = new HashMap&lt;&gt;(); parameterMap.put(&quot;tsCode&quot;, &quot;order&quot;); parameterMap.put(&quot;result&quot;, &quot;-1&quot;); return parameterMap; &#125; &#125;&#125; 12345678910111213141516// mapper 接口public interface OrderMapper &#123; int createOrderNum(Map&lt;String, String&gt; parameterMap);&#125;// xml &lt;update id=&quot;createOrderNum&quot; parameterMap=&quot;initMap&quot; statementType=&quot;CALLABLE&quot;&gt; CALL GetSerialNo(?,?) &lt;/update&gt; &lt;parameterMap type=&quot;java.util.Map&quot; id=&quot;initMap&quot;&gt; &lt;parameter property=&quot;tsCode&quot; mode=&quot;IN&quot; jdbcType=&quot;VARCHAR&quot;/&gt; &lt;parameter property=&quot;result&quot; mode=&quot;OUT&quot; jdbcType=&quot;VARCHAR&quot;/&gt; &lt;/parameterMap&gt; 备注 执行了一个百个线程之后 可以看一下数据，自增到了100 说明成功了 删掉 存储过程中的 for update，再执行 可以看出锁的作用]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>并发</tag>
      </tags>
  </entry>
</search>
